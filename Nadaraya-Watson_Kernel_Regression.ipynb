{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdf9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nadaraya_watson_estimator(X_train, Y_train, X_test, h):\n",
    "    \"\"\"\n",
    "    Nadaraya-Watson Kernel Regression Estimator (using Gaussian Kernel)\n",
    "    Estimates E[Y|X=x] for points in X_test given training data (X_train, Y_train).\n",
    "\n",
    "    Args:\n",
    "        X_train (np.array): Explanatory variables (shape (M, d) or (M,))\n",
    "        Y_train (np.array): Dependent variables (shape (M,) or (M, 1))\n",
    "        X_test (np.array): Points where to estimate the conditional expectation (shape (M, d) or (M,))\n",
    "        h (float): Bandwidth parameter (smoothing parameter)\n",
    "\n",
    "    Returns:\n",
    "        np.array: Estimated conditional expectations for X_test (shape (M,) or (M, 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure inputs are 2D arrays for dimension consistency\n",
    "    if X_train.ndim == 1: X_train = X_train[:, None]\n",
    "    if Y_train.ndim == 1: Y_train = Y_train[:, None]\n",
    "    if X_test.ndim == 1: X_test = X_test[:, None]\n",
    "        \n",
    "    M_train, d = X_train.shape\n",
    "    M_test, _ = X_test.shape\n",
    "    \n",
    "    # Calculate squared Euclidean distances between test points and training points\n",
    "    # distances2[i, j] = ||X_test[i] - X_train[j]||^2\n",
    "    # Use broadcasting for efficiency\n",
    "    # Note: this might be memory-intensive for very large M and d\n",
    "    distances2 = np.sum((X_test[:, None, :] - X_train[None, :, :])**2, axis=2)\n",
    "    \n",
    "    # Gaussian Kernel weights calculation\n",
    "    # K_h(u) = (1 / (h*sqrt(2pi)))^d * exp(-0.5 * (u/h)^2)\n",
    "    # Weights for each test point across all training points\n",
    "    weights = np.exp(-0.5 * distances2 / (h**2))\n",
    "    \n",
    "    # Normalize weights so they sum to 1 for each test point (row-wise sum)\n",
    "    # The normalization constant (1 / (h*sqrt(2pi)))^d cancels out in the ratio\n",
    "    weights_normalized = weights / np.sum(weights, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate the weighted average: E[Y|X=x] = sum(W_i(x) * Y_i)\n",
    "    # Resulting shape (M_test, 1)\n",
    "    Y_pred = np.sum(weights_normalized[:, :, None] * Y_train[None, :, :], axis=1)\n",
    "    \n",
    "    # Return 1D array if original Y was 1D\n",
    "    return Y_pred.flatten() if Y_train.shape[1] == 1 else Y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "T = 1.0          # Terminal time\n",
    "N = 10          # Number of time steps\n",
    "dt = T / N       # Time step size\n",
    "d = 1            # Dimension of Brownian motion\n",
    "M = 10000    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee584186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t, x):\n",
    "    return np.ones((M, d))  # Example: constant volatility\n",
    "\n",
    "def sigma_inv(t, x):\n",
    "    return np.ones((M, d))  # Inverse of sigma (identity for constant sigma)\n",
    "\n",
    "def H(t, x, z):\n",
    "    return -0.5 * np.sum(z**2, axis=1)  # Example Hamiltonian\n",
    "\n",
    "def g(x):\n",
    "    return np.sum(x**2-x, axis=1)  # Terminal condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225fa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((N + 1, M, d))\n",
    "Y = np.zeros((N + 1, M))\n",
    "Z = np.zeros((N, M, d))\n",
    "dW = np.random.normal(0, np.sqrt(dt), size=(N, M, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bcdac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(N):\n",
    "    X[n + 1] = X[n] + sigma(n * dt, X[n]) * dW[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "579a9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing backward recursion for BSDE using Kernel Regression...\n",
      "Backward recursion complete.\n",
      "------------------------------\n",
      "Approximate Y_0: 0.5167329919847594\n",
      "Theoretical Y_0 (for this specific problem): 1.0\n"
     ]
    }
   ],
   "source": [
    "# ... (Previous code for setup and forward simulation of X) ...\n",
    "\n",
    "# Backward recursion for BSDE using Kernel Regression\n",
    "Y[N] = g(X[N]).flatten() if X[N].ndim > 1 else g(X[N]) # Ensure Y[N] is 1D\n",
    "\n",
    "# Define a simple constant bandwidth (needs tuning in practice)\n",
    "# A typical rule of thumb might be h = np.std(X[n]) * M**(-1/(d+4))\n",
    "h_bandwidth = 0.5 # Example value, adjust as needed\n",
    "\n",
    "print(\"Performing backward recursion for BSDE using Kernel Regression...\")\n",
    "\n",
    "for n in reversed(range(N)):\n",
    "    t_n = n * dt\n",
    "    \n",
    "    # --- Estimate conditional expectations using Kernel Regression ---\n",
    "    \n",
    "    # E[Y_{n+1} | X_n]: \n",
    "    # X_train = X[n], Y_train = Y[n+1], X_test = X[n]\n",
    "    EY = nadaraya_watson_estimator(X[n], Y[n+1], X[n], h_bandwidth)\n",
    "    \n",
    "    # E[Y_{n+1} * dW_n | X_n]:\n",
    "    # We need to estimate the conditional expectation for each dimension of dW\n",
    "    # YdW is of shape (M, d)\n",
    "    YdW = Y[n+1][:, None] * dW[n] \n",
    "    EYdW = np.zeros((M, d))\n",
    "    for dim in range(d):\n",
    "        EYdW[:, dim] = nadaraya_watson_estimator(X[n], YdW[:, dim], X[n], h_bandwidth)\n",
    "\n",
    "    # --- Compute Z and Y using the BSDE discretization formula ---\n",
    "    \n",
    "    Z[n] = (1 / dt) * sigma_inv(t_n, X[n]) * EYdW\n",
    "    Y[n] = EY + H(t_n, X[n], Z[n]) * dt\n",
    "\n",
    "print(\"Backward recursion complete.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Output and Visualization (as before) ---\n",
    "approx_Y0 = np.mean(Y[0])\n",
    "print(f\"Approximate Y_0: {approx_Y0}\")\n",
    "theoretical_Y0 = T\n",
    "print(f\"Theoretical Y_0 (for this specific problem): {theoretical_Y0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351e8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
