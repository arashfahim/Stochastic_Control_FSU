{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebd46f4",
   "metadata": {},
   "source": [
    "Run this notebook to approximate $\\sin(x)$ with $y = {a} + {b} x + {c} x^2 + {d} x^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1a2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.patches as patches\n",
    "font = font_manager.FontProperties(style='normal', size=20)\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath}')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "from IPython.display import display, Math, Markdown\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "timestamp\n",
    "version = '_0.1.0'\n",
    "import math\n",
    "pi = math.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee2ca7",
   "metadata": {},
   "source": [
    "# Shortest exit time\n",
    "\n",
    "Given a bounded domain $D\\subset\\mathbb{R}^d$, find \n",
    "\n",
    "$\\inf_{u}\\{t\\ge0~:~x_t\\not\\in D\\}$\n",
    "\n",
    "where ${dx}_t=u_t{{dt}}$ with control $|u_t|\\le1$ and $u_t\\in\\mathbb{R}^d$ and initial position $x_0=x\\in D$.\n",
    "\n",
    "\n",
    "For  $D=[a,b]\\subset\\mathbb{R}$, the solution is obvious; go full speed to the nearest exit point $a$ or $b$. In higher dimension, the solution is not as simple. \n",
    "\n",
    "Here, we use a specific cost function to approximately find the solution of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31653e93",
   "metadata": {},
   "source": [
    "# Approximation of velocity with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9fdfd",
   "metadata": {},
   "source": [
    "## Defining a neural network in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4dd6e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 40\n",
    "velocity= torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, num_neurons),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(num_neurons, num_neurons),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(num_neurons,1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780775d",
   "metadata": {},
   "source": [
    "## Generate samples for the initial point $x_0$\n",
    "\n",
    "This helps up solve the problem for the whole interval $[a,b]$ once for all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dd8a6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 # number of samples for x0\n",
    "a = 0 #interval start\n",
    "b = 1 #interval end\n",
    "x0 = a + torch.rand([M,1])*(b-a)\n",
    "T = 1.2 # time horizon (artificial)\n",
    "N = 10 # number of time steps\n",
    "delta = T/N # time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61357a",
   "metadata": {},
   "source": [
    "## Dynamic of the state process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "375ec926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state(x0,model):\n",
    "    t = torch.linspace(0, T, N+1)\n",
    "    x = torch.zeros((M, N+1, 1))\n",
    "    x[:,0,:] = x0\n",
    "    tmp = x[:,0,:].clone().detach()\n",
    "    for n in range (1, N+1):\n",
    "        x[:,n,:] = tmp + torch.minimum(torch.maximum(model(tmp),torch.tensor([-1.]*M).reshape(M,1)),torch.tensor([1.]*M).reshape(M,1)) *delta\n",
    "        tmp = x[:,0,:].clone().detach()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4b595",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Loss function for the problem is the combination of running cost and the terminal function. For this problem:\n",
    "\n",
    "$C(x,u)=1\\!\\!1_{\\{x\\in[a,b]\\}}$ and $g(x)=1\\!\\!1_{\\{x\\in[a,b]\\}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "704e576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    loss_ = (x[:,N,:]>a).float()*(x[:,N,:]<b).float() + torch.nn.ReLU()(x[:,N,:]) - torch.nn.ReLU()(x[:,N,:])#to make auto-differentiation work. require gradient\n",
    "    # print(loss_.requires_grad)\n",
    "    for n in range(0,N):\n",
    "        loss_ = loss_ + (x[:,n,:]>a).float()*(x[:,n,:]<b).float()*delta #+ torch.nn.ReLU()(x[:,n,:]) - torch.nn.ReLU()(x[:,n,:]) \n",
    "        # print(loss_.requires_grad)\n",
    "    return torch.mean(loss_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1649b5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "caff69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(torch.linspace(0, T, N+1),x[2,:,0].clone().detach().numpy())\n",
    "# plt.plot(torch.linspace(0, T, N+1),x[0,:,0].clone().detach().numpy(), marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bd9810dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "101 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "201 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "301 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "401 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "501 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "601 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "701 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "801 tensor(2.2000, grad_fn=<MeanBackward0>)\n",
      "901 tensor(2.2000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-1\n",
    "# optimizer = optim.Adam(velocity.parameters(), learning_rate)\n",
    "for e in range(num_epochs):\n",
    "    x = state(x0,velocity)\n",
    "    # x[:,0,:] = x0\n",
    "    # xo = x[:,0,:]\n",
    "    # for n in range (1, N+1):\n",
    "    #     xo = xo.clone().detach() + velocity(xo.clone().detach())*delta # (+= does not work)\n",
    "    #     x[:,n,:] = xo\n",
    "    # # loss = torch.square(y_pred - y).sum().clone().detach().requires_grad_(True)\n",
    "    cost = loss(x)\n",
    "    # loss_ = (x[:,N,:]>a).float()*(x[:,N,:]<b).float() + torch.nn.ReLU()(x[:,N,:]) - torch.nn.ReLU()(x[:,N,:])#to make auto-differentiation work. require gradient\n",
    "    # for n in range(0,N):\n",
    "    #     loss_ = loss_ + (x[:,n,:]>a).float()*(x[:,n,:]<b).float()*delta + torch.nn.ReLU()(x[:,n,:]) - torch.nn.ReLU()(x[:,n,:]) \n",
    "    # cost = torch.mean(loss_)\n",
    "    # optimizer.zero_grad() # Zero the gradients before running the backward pass.\n",
    "    velocity.zero_grad() # Zero the gradients before running the backward pass.    \n",
    "    cost.backward() # Backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "    if e % 100 == 0:\n",
    "        print(e+1, cost)\n",
    "    with torch.no_grad(): # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "        for param in velocity.parameters(): # We can access its gradients like we did before.\n",
    "            param -= learning_rate * param.grad # Update the weights using gradient descent\n",
    "    # optimizer.step() # Update the weights using gradient descent. Each parameter is a Tensor, so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7adea0",
   "metadata": {},
   "source": [
    "The loss function does not change and the model is not properly trained. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "96fca9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(x):\n",
    "    loss_ = torch.nn.ReLU()(x[:,N,:]-a) - 2*torch.nn.ReLU()(x[:,N,:]-(a+b)/2) + torch.nn.ReLU()(x[:,N,:]-b)#to make auto-differentiation work. require gradient\n",
    "    # print(loss_.requires_grad)\n",
    "    for n in range(0,N):\n",
    "        loss_ = loss_ + (torch.nn.ReLU()(x[:,n,:]-a) - 2*torch.nn.ReLU()(x[:,n,:]-(a+b)/2) + torch.nn.ReLU()(x[:,n,:]-b))*delta #+ torch.nn.ReLU()(x[:,n,:]) - torch.nn.ReLU()(x[:,n,:]) \n",
    "        # print(loss_.requires_grad)\n",
    "    return torch.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f489fe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "101 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "201 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "301 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "401 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "501 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "601 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "701 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "801 tensor(0.4376, grad_fn=<MeanBackward0>)\n",
      "901 tensor(0.4376, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-1\n",
    "# optimizer = optim.Adam(velocity.parameters(), learning_rate)\n",
    "for e in range(num_epochs):\n",
    "    x = state(x0,velocity)\n",
    "    # x[:,0,:] = x0\n",
    "    # xo = x[:,0,:]\n",
    "    # for n in range (1, N+1):\n",
    "    #     xo = xo.clone().detach() + velocity(xo.clone().detach())*delta # (+= does not work)\n",
    "    #     x[:,n,:] = xo\n",
    "    # # loss = torch.square(y_pred - y).sum().clone().detach().requires_grad_(True)\n",
    "    cost = new_loss(x)\n",
    "    # loss_ = (x[:,N,:]>a).float()*(x[:,N,:]<b).float() + torch.nn.ReLU()(x[:,N,:]) - torch.nn.ReLU()(x[:,N,:])#to make auto-differentiation work. require gradient\n",
    "    # for n in range(0,N):\n",
    "    #     loss_ = loss_ + (x[:,n,:]>a).float()*(x[:,n,:]<b).float()*delta + torch.nn.ReLU()(x[:,n,:]) - torch.nn.ReLU()(x[:,n,:]) \n",
    "    # cost = torch.mean(loss_)\n",
    "    # optimizer.zero_grad() # Zero the gradients before running the backward pass.\n",
    "    velocity.zero_grad() # Zero the gradients before running the backward pass.    \n",
    "    cost.backward() # Backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "    if e % 100 == 0:\n",
    "        print(e+1, cost)\n",
    "    with torch.no_grad(): # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "        for param in velocity.parameters(): # We can access its gradients like we did before.\n",
    "            param -= learning_rate * param.grad # Update the weights using gradient descent\n",
    "    # optimizer.step() # Update the weights using gradient descent. Each parameter is a Tensor, so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "65f741aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1534], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "velocity(torch.tensor([0.75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "493e847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4400), tensor([-0.3134], grad_fn=<MinimumBackward0>))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor([0.2]*11).unsqueeze(-1).unsqueeze(0)\n",
    "print(x1.shape)\n",
    "new_loss(x1),torch.minimum(torch.maximum(velocity(torch.tensor([0.2])),torch.tensor([-1.])),torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a8caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
