{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3OWHjhGGGVL7OowvVjQI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arashfahim/Stochastic_Control_FSU/blob/main/Discrete_time_DPP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/arashfahim/Stochastic_Control_FSU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYpsD_vHtIpa",
        "outputId": "116ab044-5379-4d06-a687-b17c1bf0c77c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Stochastic_Control_FSU'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 63 (delta 33), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (63/63), 312.31 KiB | 4.88 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcK8F3qLthk9",
        "outputId": "ed976d7d-8ad6-41eb-dded-0864312a9c7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 1 root root 4096 Oct  2 13:21 sample_data\n",
            "drwxr-xr-x 4 root root 4096 Oct  3 23:05 Stochastic_Control_FSU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = r'/content/Stochastic_Control_FSU/arashfahim/Stochastic_Control_FSU/'"
      ],
      "metadata": {
        "id": "gDKJJU8n5db3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix4wczxVvVD8",
        "outputId": "af3565b2-3836-40c7-88b7-a05045d8a6ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to implement a one-step dynamic programming principle, DPP, for a stochastic control problem.\n",
        "\n",
        "The stochastic control problem is given by\n",
        "\n",
        "$\\inf_{u_0,...u_{T-1}}\\mathbb{E}\\bigg[\\sum_{t=0}^{T-1}C(t,X^u_t,u_t)+g(X^u_T)\\bigg]$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hq2eZ_d-UDNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DPP is given by\n",
        "\n",
        "$\\begin{cases}\n",
        "V(t,x)=\\inf_{u\\in\\mathbb{R}^d}\\mathbb{E}[C(t,x,u)+V(t+1,X^u_{t+1})]\\\\\n",
        "V(T,x)=g(x)\n",
        "\\end{cases}$\n",
        "\n",
        "with $X^u_{t+1}=x+\\mu(t,x,u)+\\sigma(t,x,u)\\xi_{t+1}$"
      ],
      "metadata": {
        "id": "JP7f4T78Uqvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Creation of optimization module\n",
        "\n",
        "We need to solve the above one-step optimization problem.\n",
        "\n",
        "Input: $\\phi(x,u,\\xi)$ and $c(x,u)$\n",
        "\n",
        "Optimize: $\\inf_u\\mathbb{E}[c(x,u)+\\phi(x,u,\\xi)]$\n",
        "\n",
        "Output: $u^*(x)\\in\\textrm{argmin}\\mathbb{E}[c(x,u)+\\phi(x,u,\\xi)]$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "It is important to make sure the input and output are functions and not just data points."
      ],
      "metadata": {
        "id": "nH3hSPxVVKX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.1. Expected value\n",
        "\n",
        "To evaluate $\\mathbb{E}[c(t,x,u)+\\phi(x,\\xi)]$ in Step 1, we need to create a module that evaluate expected value using a set of samples of $\\xi$.\n",
        "Such module must evaluate the expected value a a function of $x$. To do this, we use the following two wellknown theorems about conditional expectation\n",
        "\n",
        "\n",
        "  1) If $X$ and $\\xi$ are independent, then $\\mathbb{E}[\\phi(X,\\xi)|X]= \\Phi(X)$, where $\\Phi(x)=\\mathbb{E}[\\phi(x,\\xi)]$.\n",
        "\n",
        "  2) $\\mathbb{E}[Y|X]=f(X)$, where $f(x)$ is the minimizer of $\\mathbb{E}[(Y-f(X))^2]$.\n",
        "\n"
      ],
      "metadata": {
        "id": "SFst5aYlrH6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network approximation of conditional expectation\n",
        "\n",
        "$\\inf_{\\theta}\\mathbb{E}[(Y-\\phi(X;\\theta))^2]$\n",
        "\n",
        "and $\\phi(x;\\theta)$ is a neural network with parameter $\\theta$."
      ],
      "metadata": {
        "id": "Qws0AvGLMa9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from time import strftime, localtime\n",
        "import torch.optim as optim #import optimizer\n",
        "# import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "M =10000# number of samples\n",
        "X = torch.normal(0., 1., size=(M,1))# samples for X~N(0,1)\n",
        "Y = torch.exp(-X)+torch.exp(X)*torch.normal(0., 1., size=(M,1))# samples for Y=e^{-x}+e^{x}N(0,1)\n",
        "# X.shape\n",
        "T=1# terminal horizon\n",
        "N = 10 # of time steps\n",
        "Dt= torch.Tensor([T/N])# time step size"
      ],
      "metadata": {
        "id": "-Obh3NNKOTMI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fun(y,x,model):# y :samples for $Y$, x: samples for x, model: nn\n",
        "  return torch.mean(torch.pow(model(x)-y,2))\n"
      ],
      "metadata": {
        "id": "GT4QGv1xRECt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create time stamp to save the result\n",
        "stamp = strftime('%Y-%m-%d %H:%M:%S', localtime())\n",
        "print(str(stamp))"
      ],
      "metadata": {
        "id": "5hwEP5QcSA4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c468e71-8776-4376-b41f-2416a3c122d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-03 23:06:05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for conditional expectation"
      ],
      "metadata": {
        "id": "lpZBXu0JjVsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test examples using neural networks\n",
        "\n",
        "Let's write a module that takes samples of $(X,Y)$ and returns $\\mathbb{E}[Y|X=x]$\n",
        "\n",
        "Example: $X$ standard normal and $Y=e^{-X}+e^X\\xi$ where $\\xi$ is standard normal independent of $X$. We know $\\mathbb{E}[Y|X]=e^{-X}$."
      ],
      "metadata": {
        "id": "QYB5z0mjOTxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters for learning\n",
        "lr = 1e-3\n",
        "num_epochs = 10000\n",
        "torch.set_printoptions(precision=10)\n",
        "loss_epoch = torch.zeros(num_epochs)"
      ],
      "metadata": {
        "id": "wzxOkxtppJyK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''conditional expectation of X[1,:] given X[0,:] in for of trained model'''\n",
        "def cond_expect(data, model):\n",
        "  optimizer = optim.Adam(model.parameters(), lr)\n",
        "  L_ = torch.Tensor([-2.0])\n",
        "  loss = torch.Tensor([2.0])\n",
        "  epoch=0\n",
        "  while (torch.abs(L_-loss)>1e-4) & (epoch <= num_epochs):# epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss=loss_fun(data[:,1].unsqueeze(1),data[:,0].unsqueeze(1),model)# Y=data[:,1], X=data[:,0]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_epoch[epoch] = loss\n",
        "    if epoch>0:\n",
        "      L_ = loss_epoch[epoch-1]\n",
        "    if (epoch % 100==0):\n",
        "      print(\"At epoch {} the mean error is {}.\".format(epoch,loss.detach()))\n",
        "    epoch += 1\n"
      ],
      "metadata": {
        "id": "dokXzWTCjXh1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7vG3XfSrXMJV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# increase neurons\n",
        "model3= torch.nn.Sequential(\n",
        "    torch.nn.Linear(1, 16),\n",
        "    torch.nn.Tanh(),\n",
        "    # torch.nn.Sigmoid(),\n",
        "    # torch.nn.ReLU(),\n",
        "    torch.nn.Linear(16,1)\n",
        ")"
      ],
      "metadata": {
        "id": "fX4l4l3_pD45"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.cat((X,Y),dim=1)\n",
        "cond_expect(data,model3)\n"
      ],
      "metadata": {
        "id": "XCrDFmdDrNFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61cd8d47-dd64-4de7-9aa5-104194ee4819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At epoch 0 the mean error is 15.12679386138916.\n",
            "At epoch 100 the mean error is 12.075037956237793.\n",
            "At epoch 200 the mean error is 10.327677726745605.\n",
            "At epoch 300 the mean error is 9.749556541442871.\n",
            "At epoch 400 the mean error is 9.524532318115234.\n",
            "At epoch 500 the mean error is 9.350786209106445.\n",
            "At epoch 600 the mean error is 9.196995735168457.\n",
            "At epoch 700 the mean error is 9.059896469116211.\n",
            "At epoch 800 the mean error is 8.9364595413208.\n",
            "At epoch 900 the mean error is 8.824448585510254.\n",
            "At epoch 1000 the mean error is 8.722101211547852.\n",
            "At epoch 1100 the mean error is 8.628267288208008.\n",
            "At epoch 1200 the mean error is 8.542262077331543.\n",
            "At epoch 1300 the mean error is 8.463520050048828.\n",
            "At epoch 1400 the mean error is 8.391520500183105.\n",
            "At epoch 1500 the mean error is 8.325756072998047.\n",
            "At epoch 1600 the mean error is 8.265735626220703.\n",
            "At epoch 1700 the mean error is 8.21098804473877.\n",
            "At epoch 1800 the mean error is 8.161059379577637.\n",
            "At epoch 1900 the mean error is 8.115516662597656.\n",
            "At epoch 2000 the mean error is 8.073949813842773.\n",
            "At epoch 2100 the mean error is 8.035983085632324.\n",
            "At epoch 2200 the mean error is 8.001264572143555.\n",
            "At epoch 2300 the mean error is 7.9694743156433105.\n",
            "At epoch 2400 the mean error is 7.94032096862793.\n",
            "At epoch 2500 the mean error is 7.913537502288818.\n",
            "At epoch 2600 the mean error is 7.888883590698242.\n",
            "At epoch 2700 the mean error is 7.866140842437744.\n",
            "At epoch 2800 the mean error is 7.845113277435303.\n",
            "At epoch 2900 the mean error is 7.825625896453857.\n",
            "At epoch 3000 the mean error is 7.807521820068359.\n",
            "At epoch 3100 the mean error is 7.790657997131348.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z = torch.normal(0., 1., size=(M,1))\n",
        "W = torch.exp(-Z)+torch.exp(Z)*torch.normal(0., 1., size=(M,1))\n",
        "torch.set_printoptions(precision=10)\n",
        "print(\"Test on new data: MSE = {}\".format(loss_fun(W,Z,model3)))"
      ],
      "metadata": {
        "id": "8ZJDvbgZsUyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx = torch.linspace(-2,2,600)\n",
        "yy = model3(xx.unsqueeze(1))\n",
        "plt.plot(xx.detach().numpy(),yy.detach().numpy(), label='approximation');\n",
        "plt.plot(xx.detach().numpy(),np.exp(-xx.detach().numpy()), label='analytical');\n",
        "plt.title(\"Comparison with analytical solution\");\n",
        "plt.legend();\n",
        "plt.savefig(path+'test for model3'+stamp+'.png');"
      ],
      "metadata": {
        "id": "jckqYPR3sUyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multistep DPP"
      ],
      "metadata": {
        "id": "VHQ24nbeAHCT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu8NzX9QT_Rf"
      },
      "outputs": [],
      "source": [
        "def g(x):\n",
        "  return torch.Tensor([0.5])*torch.pow(x,2)-x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def c(x,u):\n",
        "  return torch.Tensor([0.5])*torch.pow(x,2) + torch.Tensor([0.5])*torch.pow(u,2)"
      ],
      "metadata": {
        "id": "U8wYvFX9DgPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running loss\n",
        "def running_loss(data,model,c):\n",
        "  u = model(data)\n",
        "  return torch.mean(c(data,u))\n",
        "\n"
      ],
      "metadata": {
        "id": "luhNDbmEAGon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# increase neurons\n",
        "modelu= torch.nn.Sequential(\n",
        "    torch.nn.Linear(1, 16),\n",
        "    torch.nn.Tanh(),\n",
        "    # torch.nn.Linear(8, 8),\n",
        "    # torch.nn.ReLU(),\n",
        "    torch.nn.Linear(16,1)\n",
        ")"
      ],
      "metadata": {
        "id": "YP5VUo5dCTj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cost:\n",
        "  def __init__(self,flag):\n",
        "    self.flag = flag\n",
        "  def cost(self,x,*args):\n",
        "    if self.flag == 'T':\n",
        "      return torch.Tensor([0.5])*torch.pow(x,2)-x\n",
        "    else:\n",
        "      u=args[0]\n",
        "      return torch.Tensor([0.5])*torch.pow(x,2) + torch.Tensor([0.5])*torch.pow(u,2)\n",
        "# goal: to add the term E[V(t+1, X^u_{t+1})|X_t=x] inside the cost function for t<T."
      ],
      "metadata": {
        "id": "0hYvYODPFJVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = cost('T')\n",
        "c = cost('')"
      ],
      "metadata": {
        "id": "sfHup0mjGR00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.Tensor([1.0])\n",
        "c.cost(x,modelu(x)),g.cost(x)"
      ],
      "metadata": {
        "id": "Yvr9uc0-GU5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(x,u):\n",
        "  return x + (-x+u)*Dt + torch.sqrt(Dt)*torch.normal(0., 1., size=x.shape)"
      ],
      "metadata": {
        "id": "rPVaRQBdUVOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Last step with $g(X^u_{T})$"
      ],
      "metadata": {
        "id": "TX5YRzbmTsL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.normal(0., 1., size=(M,1))# samples for X~N(0,1)\n",
        "for p in modelu.parameters():\n",
        "  p.requires_grad = False\n",
        "Y = g.cost(update(X,modelu(X)))"
      ],
      "metadata": {
        "id": "FMR6HO4YTrq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# increase neurons\n",
        "modelexcost= torch.nn.Sequential(\n",
        "    torch.nn.Linear(1, 16),\n",
        "    torch.nn.Tanh(),\n",
        "    # torch.nn.Linear(8, 8),\n",
        "    # torch.nn.ReLU(),\n",
        "    torch.nn.Linear(16,1)\n",
        ")"
      ],
      "metadata": {
        "id": "gE3MJNw3WQNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.cat((X,Y),dim=1)\n",
        "cond_expect(data,modelexcost)\n"
      ],
      "metadata": {
        "id": "VR3UUsvQVsAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.cost(X,modelu(X))+modelexcost(X)"
      ],
      "metadata": {
        "id": "1b3-8T-ZiQCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in modelu.parameters():\n",
        "  p.requires_grad = True\n",
        "for p in modelexcost.parameters():\n",
        "  p.requires_grad = False\n",
        "optimizer1 = optim.Adam(modelu.parameters(), lr)\n",
        "L_ = torch.Tensor([-2.0])\n",
        "loss = torch.Tensor([2.0])\n",
        "epoch=0\n",
        "while (torch.abs(L_-loss)>1e-4) & (epoch <= num_epochs):# epoch in range(num_epochs):\n",
        "  optimizer1.zero_grad()\n",
        "  loss=c.cost(X,modelu(X)) + modelexcost(X)#\n",
        "  loss.backward()\n",
        "  optimizer1.step()\n",
        "  loss_epoch[epoch] = loss\n",
        "  if epoch>0:\n",
        "    L_ = loss_epoch[epoch-1]\n",
        "  if (epoch % 100==0):\n",
        "    print(\"At epoch {} the mean error is {}.\".format(epoch,loss.detach()))\n",
        "  epoch += 1"
      ],
      "metadata": {
        "id": "mW-VGLvHXPH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scratch coding"
      ],
      "metadata": {
        "id": "Xy1Vh3uFWk_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########################################"
      ],
      "metadata": {
        "id": "wjwFszuSWlOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters for learning\n",
        "lr = 1e-3\n",
        "num_epochs = 1000\n",
        "torch.set_printoptions(precision=10)\n",
        "loss_epoch = torch.zeros(num_epochs)\n",
        "optimizer = optim.Adam(modelu.parameters(), lr)\n",
        "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1e-1, end_factor=1e-5, total_iters=100)"
      ],
      "metadata": {
        "id": "hWKPMOQX09v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''conditional expectation of X[1,:] given X[0,:] in for of trained model'''\n",
        "def oneStep(data, model):\n",
        "  optimizer = optim.Adam(model.parameters(), lr)\n",
        "  L_ = torch.Tensor([-2.0])\n",
        "  loss = torch.Tensor([2.0])\n",
        "  epoch=0\n",
        "  while (torch.abs(L_-loss)>1e-4) & (epoch <= num_epochs):# epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss=loss_fun(data[:,1].unsqueeze(1),data[:,0].unsqueeze(1),model)# Y=data[:,1], X=data[:,0]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_epoch[epoch] = loss\n",
        "    if epoch>0:\n",
        "      L_ = loss_epoch[epoch-1]\n",
        "    if (epoch % 100==0):\n",
        "      print(\"At epoch {} the mean error is {}.\".format(epoch,loss.detach()))\n",
        "    epoch += 1\n"
      ],
      "metadata": {
        "id": "82-zpr63R7D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the model\n",
        "torch.save(model1, path+\"model1_\"+str(stamp)+\".pt\")"
      ],
      "metadata": {
        "id": "ZPyyBKSL4ucg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write loss into a daraframe\n",
        "df_loss = pd.DataFrame([range(num_epochs), loss_epoch.detach().numpy()]).T\n",
        "df_loss.columns = ['Epoch','Loss']\n",
        "df_loss['Epoch'] = df_loss['Epoch'].astype('int64')\n",
        "df_loss.to_csv(path+\"run_model1_\"+str(stamp)+\".csv\",sep=\",\")"
      ],
      "metadata": {
        "id": "mwGcoKU9xYFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "Z = torch.normal(0., 1., size=(M,1))\n",
        "W = torch.exp(-Z)+torch.exp(Z)*torch.normal(0., 1., size=(M,1))\n",
        "torch.set_printoptions(precision=10)\n",
        "print(\"Test on new data: MSE = {}\".format(loss_fun(W,Z,model1)))"
      ],
      "metadata": {
        "id": "5uaufRfESx4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.linspace(0,num_epochs,num_epochs)\n",
        "m = loss_epoch.detach().numpy()\n",
        "plt.plot(n,m);\n",
        "plt.title(\"Loss vs number of pochs in model1\");\n",
        "plt.savefig(path+'loss_vs_epoch_model1'+stamp+'.png');"
      ],
      "metadata": {
        "id": "DwpbR1fLSrf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx = torch.linspace(-3,3,600)\n",
        "yy = model1(xx.unsqueeze(1))\n",
        "plt.plot(xx.detach().numpy(),yy.detach().numpy(), label='approximation');\n",
        "plt.plot(xx.detach().numpy(),np.exp(-xx.detach().numpy()), label='analytical');\n",
        "plt.title(\"Comparison with analytical solution\");\n",
        "plt.legend();\n",
        "plt.savefig(path+'test for model1'+stamp+'.png');"
      ],
      "metadata": {
        "id": "-Ny17ZLuVeoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deriv(model,x):\n",
        "  x_ = x.clone().detach()\n",
        "  x_.requires_grad_(True)\n",
        "  u=model(x_)\n",
        "  return torch.autograd.grad(outputs=u, inputs=x_,grad_outputs=torch.ones_like(u),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0]"
      ],
      "metadata": {
        "id": "f-aR_atMc4Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx = torch.linspace(-3,3,600)\n",
        "yy = deriv(model1,xx.unsqueeze(1))\n",
        "plt.plot(xx.detach().numpy(),yy.detach().numpy(), label='approximation');\n",
        "plt.plot(xx.detach().numpy(),-np.exp(-xx.detach().numpy()), label='analytical');\n",
        "plt.title(\"Comparison with analytical solution\");\n",
        "plt.legend();\n",
        "plt.savefig(path+'test for derivative  of model1'+stamp+'.png');"
      ],
      "metadata": {
        "id": "6JDRyWL4Y8fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# increase neurons\n",
        "model2= torch.nn.Sequential(\n",
        "    torch.nn.Linear(1, 16),\n",
        "    torch.nn.ReLU(),\n",
        "    # torch.nn.Linear(8, 8),\n",
        "    # torch.nn.ReLU(),\n",
        "    torch.nn.Linear(16,1)\n",
        ")"
      ],
      "metadata": {
        "id": "ckp-RKO29VnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create time stamp to save the result\n",
        "stamp = strftime('%Y-%m-%d %H:%M:%S', localtime())\n",
        "print(str(stamp))"
      ],
      "metadata": {
        "id": "HwczH1P79VnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters for learning\n",
        "optimizer = optim.Adam(model2.parameters(), lr)\n",
        "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1e-1, end_factor=1e-5, total_iters=100)"
      ],
      "metadata": {
        "id": "viscjvNz9VnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  optimizer.zero_grad()\n",
        "  loss=loss_fun(Y,X,model2)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  loss_epoch[epoch] = loss\n",
        "  # scheduler.step()\n",
        "  if (epoch % 1000==-1):\n",
        "    print(\"At epoch {} the mean error is {}.\".format(epoch,loss.detach()))\n"
      ],
      "metadata": {
        "id": "QqNzuZ1L9VnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the model\n",
        "torch.save(model2, path+\"model2_\"+str(stamp)+\".pt\")"
      ],
      "metadata": {
        "id": "mGW5EHoZ9VnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write loss into a daraframe\n",
        "df_loss = pd.DataFrame([range(num_epochs), loss_epoch.detach().numpy()]).T\n",
        "df_loss.columns = ['Epoch','Loss']\n",
        "df_loss['Epoch'] = df_loss['Epoch'].astype('int64')\n",
        "df_loss.to_csv(path+\"run_model2_\"+str(stamp)+\".csv\",sep=\",\")"
      ],
      "metadata": {
        "id": "VQLZeJjR9VnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = torch.normal(0., 1., size=(M,1))\n",
        "W = torch.exp(-Z)+torch.exp(Z)*torch.normal(0., 1., size=(M,1))\n",
        "torch.set_printoptions(precision=10)\n",
        "print(\"Test on new data: MSE = {}\".format(loss_fun(W,Z,model1)))"
      ],
      "metadata": {
        "id": "EAEUUahn9VnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "n = np.linspace(0,num_epochs,num_epochs)\n",
        "m = loss_epoch.detach().numpy()\n",
        "plt.plot(n,m);\n",
        "plt.title(\"Loss vs number of epochs in model2\");\n",
        "plt.savefig(path+'loss_vs_epoch_model2'+stamp+'.png');"
      ],
      "metadata": {
        "id": "LyCJVN2Y9VnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx = torch.linspace(-2,2,600)\n",
        "yy = model2(xx.unsqueeze(1))\n",
        "plt.plot(xx.detach().numpy(),yy.detach().numpy(), label='approximation');\n",
        "plt.plot(xx.detach().numpy(),np.exp(-xx.detach().numpy()), label='analytical');\n",
        "plt.title(\"Comparison with analytical solution\");\n",
        "plt.legend();\n",
        "plt.savefig(path+'test for model2'+stamp+'.png');"
      ],
      "metadata": {
        "id": "PW6OLwmt9VnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx = torch.linspace(-3,3,600)\n",
        "yy = deriv(model2,xx.unsqueeze(1))\n",
        "plt.plot(xx.detach().numpy(),yy.detach().numpy(), label='approximation');\n",
        "plt.plot(xx.detach().numpy(),-np.exp(-xx.detach().numpy()), label='analytical');\n",
        "plt.title(\"Comparison with analytical solution\");\n",
        "plt.legend();\n",
        "plt.savefig(path+'test for derivative  of model2'+stamp+'.png');"
      ],
      "metadata": {
        "id": "3mxJvleweFr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Projectin method\n",
        "\n",
        "\n",
        "One trick to approximate $\\mathbb{E}[Y|X]=f(X)$ is to take the minimum over a smaller class of fuctions, e.g., $f(x)=\\sum_{k=1}^Ka_k\\phi_k(x)$, for a suitable set of functions $\\phi_k$. To evluate approximate $f$, we use a joint (unconditional) sample of $(X,Y)$, $\\{(x_j,y_j):j=i,...,J\\}$ to write\n",
        "\n",
        "$\\inf_{a_1,...,a_K}\\sum_{j=1}^J \\big(y_j-\\sum_{k=1}^Ka_k\\phi_k(x_j)\\big)^2$\n",
        "\n",
        "This is a quadratic minimization problem which has a unique solution. To find the minimizer, we take take derivative wrt $a_{k'}$ and set it to zero to obtain a linear system.\n",
        "\n",
        "$\\sum_{j=1}^J \\phi_{k'}(x_j)\\big(y_j-\\sum_{k=1}^Ka_k\\phi_k(x_j)\\big)=0$\n",
        "\n",
        "$\\sum_{k=1}^Ka_k\\sum_{j=1}^J\\phi_k(x_j)\\phi_{k'}(x_j) = \\sum_{j=1}^J \\phi_{k'}(x_j)y_j$\n",
        "\n",
        "or $Ma=b$ with\n",
        "\n",
        "$a=(a_1,...,a_K)^\\top$, $M=[m_{k'k}]$ with $m_{k'k}=\\sum_{j=1}^J\\phi_{k'}(x_j)\\phi_{k}(x_j)$, and $b=(b_1,...,b_K)^\\top$ with $b_k=\\sum_{j=1}^J\\phi_k(x_j)y_j$."
      ],
      "metadata": {
        "id": "053tEF-z6dyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Choosing basis $\\phi_k$\n",
        "\n",
        "It is useful to choose the basis $\\phi_k$ such that $m_{k'k}=0$ when $k'\\neq k$. Then, the linear system above takes no time to solve: $a_k=b_k/m_{kk}$. Sometimes this is done by using orthogonal polynomials. However, we propose a simpler solution: choose the basis such that $\\phi_k(x)$ and $\\phi_{k'}(x)$ have disjoint support. For instance, they are indicator functions of hypercubes.\n",
        " More detains of this method can be found in the paper [Monte-Carlo valuation of American options: facts and new algorithms to improve existing methods](https://www.ceremade.dauphine.fr/~bouchard/pdf/BW10.pdf)."
      ],
      "metadata": {
        "id": "GvmlYke0-cG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Simulationg samples of $(X_t,\\xi)$\n",
        "Let $\\xi$ be Gaussian, e.g., increment of Brownian motion, we can choose any distribution for $X_t$ independent of $\\xi$."
      ],
      "metadata": {
        "id": "k2kPNML4_kY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 DPP\n",
        "Start the backward scheme by feeding $C(t,x,u)$ and $g(x)$ into the optimization module and obtain $V(T-1,x)$ over the range of simulated values for $X_{T-1}$.\n",
        "\n",
        "Repeat the process by feeding $C(t,x,u)$ and $V(t+1,x)$ into the optimization module to obtain $V(t,x)$.\n",
        "\n",
        "It is important to make sure the simulated samples of $X^u_{t+1}$ are well inside the range in which estimation of $V(t+1,x)$ is rather accurate/"
      ],
      "metadata": {
        "id": "HlNWVIImAJGE"
      }
    }
  ]
}