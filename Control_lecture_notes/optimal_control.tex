\documentclass[11pt]{book}
\usepackage{notes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{libertine}
\usepackage{enumitem}
\usepackage{float}
\usepackage{epsfig}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{pgfplots}
 \usepackage{caption}
\usepackage{tikz}
\usepackage{subcaption} 
\usepackage{svg}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{natbib}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}
\SetKwInOut{Parameter}{Parameter}
% Uncomment these for a different family of fonts
% \usepackage{cmbright}
% \renewcommand{\sfdefault}{cmss}
% \renewcommand{\familydefault}{\sfdefault}
\newcommand{\dd}{\text{\normalfont d}}
\newcommand{\dt}{\text{\normalfont d}t}
\newcommand{\ds}{\text{\normalfont d}s}
\newcommand{\du}{\text{\normalfont d}u}
\newcommand{\dr}{\text{\normalfont d}r}
\newcommand{\dx}{\text{\normalfont d}x}
\newcommand{\dX}{\text{\normalfont d}X}
\newcommand{\dW}{\text{\normalfont d}W}
\newcommand{\thiscoursecode}{Methods Of Optimal Control}
\newcommand{\thiscoursename}{}
\newcommand{\thisprof}{Arash Fahim}
%\newcommand{\me}{Liam Horne}
\newcommand{\thisterm}{}
\newcommand{\website}{}

% Headers
\chead{\thiscoursename}
\lhead{\thisterm}


%%%%%% TITLE %%%%%%
\newcommand{\notefront} {
\pagenumbering{roman}
\begin{center}

{\ttfamily \url{\website}} {\small}

\textbf{\Huge{\noun{\thiscoursecode}}}{\Huge \par}

{\large{\noun{\thiscoursename}}}\\ \vspace{0.1in}

  {\noun \thisprof} \ $\bullet$ \ {\noun \thisterm} \ $\bullet$ \ {\noun {}} \\

  \end{center}
  }
\pgfplotsset{compat=1.18}
% Begin Document
\begin{document}

  % Notes front
  \notefront
  % Table of Contents and List of Figures
  \tocandfigures
  
    \begin{center}
 \noindent{\bf Notations} 
 \medskip
 
 \begin{tabularx}{.9\textwidth}{|c|X|}
  \hline
$(\Omega,\mathcal{F},\mathbb{P})$ & a probability space with a $\sigma$-field $\mathcal{F}$ and probability $\mathcal{P}$\\
 \hline
 $\mathbb{F}=\{\mathcal{F}_t\}_{t\ge0}$ & a filtration in a probability space\\
 \hline
 
$\mathbb{F}^{X}$ & filtration generated by process $X$\\
 \hline
 $\mathbb{F}^{X+}$ & $\cup_{s>t}\mathcal{F}^X_s$\\
 \hline
 $\bar{\mathbb{F}}$ & filtration augmented by adding all the $\mathbb{P}$-null events\\
 \hline
 $(\Omega,\mathbb{F},\mathbb{P})$& a filtered probability space with right-continuous augmented filtration (usual conditions) that hosts a Brownian motion adapted to the filtration $\mathbb{F}$\\
 \hline
 $O\subseteq\R^d$& open set\\
  \hline
 $\USC(O)$ ($\LSC(O)$) &  lower semicontinuous (upper semicontinuous) functions on  $O$\\
  \hline
 $\Cn{k}(O)$ &  $k$ times continuously differentiable functions on $O$  \\
  \hline
  $\Cn{k}(\bar O)$ & functions on $\bar O$  $k$ times continuously differentiable  on all variables over on  $O$ with derivatives continuously extendable to $\bar O$\\
   \hline
 $\Cn{k,l,...}(O)$ & functions on $O$ $k$ times continuously differentiable on first variable, $l$ times on second variable, ... \\
  \hline
 $\Cn{k,l,...}(\bar O)$ & functions on $\bar O$ times continuously differentiable on first variable, $l$ times on second variable, ... on $O$ with derivatives continuously extendable to $\bar O$ \\
  \hline
 $M(n,m)$&the set of all real $n$ by $m$ matrices\\
  \hline
 $A\cdot B:={\rm Tr}[AB^T]$& the inner product of two matrices $A$ and $B$ in $M(n,m)$\\
  \hline
 $\|A\|:=A\cdot A=\sum_{i=1}^n\sum_{j=1}^ma_{ij}^2$& $l^2$-norm of matrices in $M(n,m)$\\
  \hline
 $\Omega$ & the sample space of a random event\\
  \hline
 $\omega$& is reserved for the members of $\Omega$\\
  \hline
 ${V}_{t}$, $ V_{x}$, $\partial_{xx} V$& Partial derivatives of a function $V:[0,T]\times\R$ once wrt $t$, once wrt $x$ and twice wrt $x$\\
  \hline
 $\partial_tV$, $\nabla V$, $D^2 V$& Partial derivative of a function $V:[0,T]\times\R^2$ once wrt $t$, gradient of $V$ wrt $x$ and Hessian of $V$ wrt $x$\\
  \hline
 \end{tabularx}
\end{center}
\newpage
\setcounter{secnumdepth}{2}
\setcounter{page}{1}
\pagenumbering{arabic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Deterministic control}
\label{chap:deterministic}
\section{Optimization Versus Control}
\label{sec:opt_control}
In this chapter, we provide a brief overview of the aspect in which optimization and control are different.  We start the chapter with some examples.
\begin{eg}\label{simple_optimization}
We start by a quadratic problem. Let $\alpha:[0,T]\to\in\mathbb{R}$ be given.
    \begin{equation}
        \inf\bigg\{\int_0^T\Big(x_t^2 - \alpha_t x_t\Big) {{\dt}}\bigg\}
    \end{equation}
where the infimum is over all functions $x:[0,T]\to\in\mathbb{R}$. As $x$ can be any mapping, one can solve the following problem for each $x_t$ separately to obtain $x^*_t=\alpha_t/2$.
    \begin{equation}
        \inf_{x\in\mathbb{R}^d}\bigg\{x^2 - \alpha_t x\bigg\}
    \end{equation}
The above problem is a dynamic optimization problem. 
\end{eg}
\begin{eg}
We make the above example more complicated by specifying a dynamics for $x_t$:
    \begin{equation}\label{simple_control}
        \inf\bigg\{\int_0^T\Big(x_t^2 - \alpha_t x_t\Big) {{\dt}}\bigg\}
    \end{equation}
where the infimum is over all functions $x:[0,T]\to\in\mathbb{R}$ such that for some (Borel measurable) function ${\color{blue} u:[0,T]\to\mathbb{R}}$
\begin{equation}\label{dynamic_x}
    {\dx}_t = (-\beta x_t  {\color{blue}+ u_t }) {{\dt}}, ~~ x_0=x
\end{equation}
Note that the existence of the term ${\color{blue} u_t {{\dt}}}$ is necessary for the meaning of infimum. This problem is not a simple dynamic optimization problem because one can only choose $x$ such that the dynamic equation \eqref{dynamic_x} holds for some function ${\color{blue} u}$. \textbf{Therefore,} we can only choose ${\color{blue} u}$ and indirectly modify $x_t$ to minimize \eqref{simple_control}.
\end{eg}
\begin{ex}
Solve \eqref{dynamic_x} for $x_t$ in terms of $u$. Hint: $\dd(e^{\beta t}x_t)=e^{\beta t}u_t{{\dt}}$.
\end{ex}
\begin{ex}
Show that if there exists a function $u$ such that $\alpha_t=2\int_0^te^{\beta (s-t)}u_s {{\ds}}$, then $u$ minimizes \eqref{simple_control}.
\end{ex}
\begin{ex}
    If we modify an \emph{optimal} control $u$ in a countable number of points described in the exercise above, does it remain an optimal control? Does the initial value $x_0=x$ play a role in the problem?
\end{ex}
\begin{ex}
Assume that there exists no function $u$ such that $\alpha_t=2\int_0^te^{-\beta (s-t)}u_s {{\ds}}$. What is the minimum value of \eqref{simple_control}?
\end{ex}

The problem \eqref{simple_control} is a simple control problem. However, after doing the above exercises, you note that it can simply be reduced to an optimization problem. Such a solution for control problems are called myopic solutions and do not necessarily exist for more interesting control problems. Here is an example which does not allow for a \emph{myopic} solution.
\begin{eg}
    Consider the control problem: \begin{equation}\label{cost_u}
        \inf \int_0^T\Big(x_t^2 - \alpha_t x_t + u_t^2 \Big) {{\dt}} 
    \end{equation}
where the infimum is over all (Riemann integrable) functions $u:[0,T]\to\mathbb{R}$ and $x:[0,T]\to\mathbb{R}$ and ${u:[0,T]\to\mathbb{R}}$ satisfy  \eqref{dynamic_x}. Unlike the previous example, the choice of $u$ induces a new cost, the term $\int_0^T u_t^2 dt$. Due to this new cost, we cannot freely choose $u_t$ to make $x_t=\alpha_t/2$ optimal. It is possible that $x_t=\alpha_t/2$ is not even optimal. 
\end{eg}
In the above example, there is a tradeoff between the choice of $u_t$ to minimize the  dependence of cost on $x_t$ and to minimize  the dependence of cost on $u_t$ itself. In such cases, the tradeoff prevents us from finding a myopic solution. Here is another example.
\begin{eg}
    Consider the control problem: \begin{equation}
        \inf \int_0^T \big(x_t^2+{\color{blue}u_t^2}\big)  {{\dt}} 
    \end{equation}
    where the infimum is over all (Riemann integrable) functions $u:[0,T]\to\mathbb{R}$ and $x:[0,T]\to\mathbb{R}$ and ${u:[0,T]\to\mathbb{R}}$ satisfy  ${\dx}_t = (-\beta x_t  {\color{blue}+ u_t}) {{\dt}}, ~~ x_0=x$. 
    On one hand, we like $x_t$ to be zero to minimize the cost, but pushing $x_t$ to zero requires application of $u_t$, which introduces another cost term.
\end{eg}

A general control problem is described as 
\begin{equation}
    \label{prob:deterministic_control}
    \inf_{u\in\mathcal{U}}\int_0^TC(t,x_t,u_t){{\dt}}+g(x_T)
\end{equation}
where 
${\dx}_t=f(x_t,u_t){{\dt}}$. The function $C:\mathbb{R}_+\times\mathbb{R}^d\times\mathbb{R}^n\to \mathbb{R}$ is called the \emph{running cost} and  $g:\mathbb{R}^d\to\mathbb{R}$ is called the \emph{terminal cost}. 

Set $\mathcal{U}$ is \emph{a} set of functions $u:[0,T]\to\mathbb{R}^n$ called \emph{control variable}, which is determined by the application or by the wellposedness of the problem. It is crucial to choose \emph{a} set $\mathcal{U}$ of control variables to fit the proper application. More over, it is also important to choose a set that makes the control problem wellposed. For instance, for the control problem 
    \begin{equation}
        \inf_{u\in\mathcal{U}} \int_0^T(x_t-u_t^2) {{\dt}},~~~~{\dx}_t=(x_t-u_t){{\dt}}
    \end{equation}
if we choose $\mathcal{U}$ to be the set of all functions $u:[0,T]\to\mathbb{R}$, 
by simply choosing $u_t$ very large, the value of the infimum is $-\infty$. However, if we restrict  $\mathcal{U}$ to the set of functions $u:[0,T]\to[-1,\infty)$ (some lower bound on the value), then 
    \begin{equation}
        \inf_{u\in\mathcal{U}} \int_0^T(x_t-u_t^2) {{\dt}}>-\infty,~~~~{\dx}_t=(x_t-u_t){{\dt}}.
    \end{equation}
A suitable set of controls chosen for a specific problem is referred to as the set of all \emph{admissible} control. We denote this set by $\mathcal{U}$.


An infinite horizon control problem is accommodated by setting $T=\infty$. For example, 
\begin{equation}
    \label{prob:deterministic_control_infinite_horizon}
    \inf_{u\in\mathcal{U}}\int_0^\infty e^{-t}(x^2_t+u^2_t){{\dt}}
\end{equation}
The following exercise is an example of infinite horizon control problem.

\begin{ex}\label{ex:eikonal}
Write the following problem as a generic control problems by associating the horizon $T$, the running cost $C(t,x,u)$ and terminal cost $g(x)$ in \eqref{prob:deterministic_control}: 

\centering{\fbox{%
    \parbox{0.95\textwidth}{%
    (Shortest time to exit a bounded domain) Given a bounded domain $D\subset\mathbb{R}^d$, find 
\begin{equation}
    \inf_{u}\{t\ge0~:~x_t\not\in D\}
\end{equation}
where ${\dx}_t=u_t{{\dt}}$ with control $|u_t|\le1$ and $u_t\in\mathbb{R}^d$ and initial position $x_0=x\in D$.
    }%
    }
}
\end{ex}

\begin{ex}
    In your area of study, find an optimal control problem. Then, write the cost functions and the control variable and determine a set of admissible controls. 
\end{ex}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solution methods for deterministic optimal control}
\label{sec:sol_deter}
In this section, we propose methods for the optimal control problems. There are two groups of numerical methods, first group are based on the dynamic programming principle (DPP). DPP provides a backward recursive method to solve an optimal control problem.  The second group completely avoids DPP and formulates problem as an optimization. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dynamic programming principle (DPP)}
\label{sec:dpp_deter}
Consider the optimal control problem \eqref{prob:deterministic_control}:
\begin{equation}
    \inf_{u\in\mathcal{U}}\int_0^TC(t,x_t,u_t){{\dt}}+g(x_T),~~~~{\dx}_t=f(x_t,u_t){{\dt}}
\end{equation}
A key concept in DPP is the value function:
\begin{defn}
Let $x_t=x$. Then, the \emph{value function} of \eqref{prob:deterministic_control} is defined by
\begin{equation}\label{eqn:value_deterministic}
    V(t,x):= \inf_{u\in\mathcal{U}_{t}}\int_t^TC(s,x_s,u_s){{\ds}}+g(x_T),~~~~{\dx}_s=f(x_s,u_s){{\ds}}
\end{equation}
where $\mathcal{U}_{t}$ is the set of admissible controls restricted to time interval $[t,T]$.
\end{defn}
The value function encodes the outcome of optimal control at time $t$ at point $x$. The function $V(t,\cdot):\mathbb{R}^d\to\mathbb{R}$ returns the optimal value of the control problem for any point $x$ at time $t$.

Given the value function, the DPP is explained in the following result.
\begin{thm}\label{thm:stoch_dpp_no_stopping_deterministic}
    [Dynamic Programming Principle]
    Let $0\le t<s\le T$ and $x_t=x$. Then, the DPP for \eqref{prob:deterministic_control} is given by 
    \begin{equation} \label{eqn:dpp_deterministic}
    V(t,x) =\inf_{u\in\mathcal{U}_{t,s}}\int_t^sC(r,x_r,u_r){{\dr}}+V(s,x_s),~~~~{\dx}_r=f(r,x_r,u_r){{\dr}}
\end{equation}
where $\mathcal{U}_{t,s}$ is the set of admissible controls restricted to time interval $[t,s]$.
\end{thm}
DPP can be interpreted as the tradeoff between two intervals $[t,s]$ and $[s,T]$. More precisely, to obtain the optimal value of the control problem at time $t$, DPP finds a balance between the running cost on $[t,s]$, $\int_t^sC(r,x_r,u_r){{\dr}}$, and the continuation cost on $[s,T]$, $V(t,x_s)$. Note that choosing a control $u$ on $[t,s]$, determines the position of $x_s$.
This interpretation reveals the proof of DPP.
\begin{proof}
    [Proof of DPP, Theorem~\ref{thm:stoch_dpp_no_stopping_deterministic}]
    To show that DPP holds, we need to decompose a control $u$ on $[t,T]$ into two pieces, $u_1$ on $[t,s]$ and $u_2$ on $[s,T]$. More formally,
    \[
    u(r) =\begin{cases}
        u_1(r)& r\in[t,s]\\
        u_2(r)& r\in[s,T]
    \end{cases} 
    \]
    We denote this decomposition by $u=u_1\oplus u_2$. Note that $u_1\in\mathcal{U}_{t,s}$ and $u_2\in\mathcal{U}_{s}$.
    Therefore, by \eqref{eqn:value_deterministic},
    \[
    \begin{split}
        V(t,x)&= \inf_{u\in\mathcal{U}_{t}}\int_t^TC(r,x_r,u_r){{\dr}}+g(x_T)\\
        &= \inf_{u_1\in\mathcal{U}_{t,s}}\inf_{u_2\in\mathcal{U}_{s}}\int_t^sC(r,x_r,u_1(r)){{\dr}}+\int_s^TC(r,x_r,u_2(r)){{\dr}}+g(x_T)\\
        &=
        \inf_{u_1\in\mathcal{U}_{t,s}} \int_t^sC(r,x_r,u_1(r)){{\dr}} + \inf_{u_2\in\mathcal{U}_{s}}\int_s^TC(r,x_r,u_2(r)){{\dr}}+g(x_T)\\
    \end{split}
    \]
    Note that $\inf_{u_2\in\mathcal{U}_{s}}\int_s^TC(r,x_r,u_2(r)){{\dr}}+g(x_T)=V(s,x_s)$. Therefore,
     \[
        V(t,x)= 
        \inf_{u_1\in\mathcal{U}_{t,s}} \int_t^sC(r,x_r,u_1(r)){{\dr}} + V(s,x_s)
    \]
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Numerical solution based on DPP}
Let $\Delta t = \frac{T}{N}$ and set $t_i=i\Delta t$ with $i=0,...,N$. The DPP \eqref{eqn:dpp_deterministic} for $t=t_i$ and $s=t_{i+1}$ is written by
\begin{equation}
    V(t_{i},x) =\inf_{u\in\mathcal{U}_{t_i,t_{i+1}}}\int_{t_i}^{t_{i+1}}C(r,x_r,u_r){{\dr}}+V(t_{i+1},x_{t_{i+1}})
\end{equation}
We can approximate $\int_{t_i}^{t_{i+1}}C(r,x_r,u_r){{\dr}}\approx C(t_i,x_{t_i},u_{t_i})\Delta t$ to write
\begin{equation}
    V(t_{i},x) \approx\inf_{u}C(t_{i},x_{t_{i}},u){{\Delta t}}+V(t_{i+1},x_{t_{i+1}})
\end{equation}
This suggest to approximate the value function and optimal control for the control problem \eqref{prob:deterministic_control} in Algorithm~\ref{alg:dpp}. Throughout this notes, $\hat{f}$ represents an approximation of function $f$. 
Therefore, if $V(t_{i+1},\cdot)$ is approximated by  $\hat{V}(t_{i+1},\cdot)$, then 
\begin{equation}\label{eqn:approximate_deterministic_dpp}
    \hat{V}(t_{i},x):=\inf_{u}C(t_{i},x_{t_{i}},u){{\Delta t}}+\hat{V}(t_{i+1},x_{t_{i+1}}),~~x_{t_{i+1}} = x + f(t_i,x,u)\Delta t 
\end{equation}
suggests the approximation for $V(t_{i},\cdot)$. Note that unlike DPP,  the infimum  in the approximate DPP \eqref{eqn:approximate_deterministic_dpp} is over a $u\in\mathbb{R}^m$ and not the functions $u:[t_i,t_{i+1}]\to\mathbb{R}^m$. Therefore, $\hat{u}^*$ given by 
\[
\hat{u}^*(t_i,x)\in\mathop{\text{\normalfont argmin}}_{u}C(t_{i},x_{t_{i}},u){{\Delta t}}+\hat{V}\big(t_{i+1},x + f(t_i,x,u)\Delta t\big)  
\]
provides a constant control over the interval $[t_i,t_{i+1}$.

We provide an algorithm derived from discrete DPP in Algorithm~\ref{alg:dpp}.
\begin{algorithm}
        % Algorithm content goes here
        \Parameter{$T$, $N$, $f(t,x,u)$, $C(t,x,u)$, and $g(x)$\;
        $\Delta t = \frac{T}{N}$}
        \KwData{$\hat{V}(t_{N},x)=g(x)$\; $x^j_i$ for $j=1,...,J$ and $i=0,...,N-1$\;
        ($x^j_i$ means the  $j$th discrete point at time $t_i$.)}
        \For{$i \leftarrow N-1$ to $0$}
        {$\hat{x}^j_{i+1}=x_i^j+f(t_{i},x_i^j,u)\Delta t$\;
        $\tilde{V}(t_{i},x^j_i)\leftarrow \inf_{u}C(t_{i},{x}^j_{i},u){{\Delta t}}+\hat{V}(t_{i+1},\hat{x}^j_{t_{i+1}})$\;
        $\hat{V}(t_{i},x)$ obtained from interpolation on $\tilde{V}(t_{i},x^j_i)$ for $j=1,...,J$\;
        $\hat{u}^*(t_i,x_i^j)\in\mathop{\text{\normalfont argmin}}\limits_{u}C(t_{i},{x}^j_{i},u){{\Delta t}}+\hat{V}(t_{i+1},\hat{x}^j_{i+1})$;
        }
        \Return{$\hat{V}(t_i,\cdot)$ and $\hat{u}^*(t_i,\cdot)$ for $i=0,...,N-1$.}
        \caption{Numerical DPP}
         \label{alg:dpp}
\end{algorithm}
\begin{ex}
    Why interpolation is required in Algorithm~\ref{alg:dpp}? Can we perform the algorithm by only knowing  $\hat{V}(t_{i+1},{x}^j_{i+1})$ for all $j=1,...,J$? 
    Note the difference between $\hat{V}(t_{i+1},{x}^j_{i+1})$ and $\hat{V}(t_{i+1},\hat{x}^j_{i+1})$ with $\hat{x}^j_{i+1} = {x}^j_{i} + f(t_{i},x_i^j,u)\Delta t$.
\end{ex}
Next example provides a special case where we can evaluate the approximate value function from Algorithm~\ref{alg:dpp} without using interpolation.
\begin{eg}
    Consider 
    \begin{equation}
    \inf_{u\in\mathcal{U}} \int_0^T\left(x_t^2+u_t^2\right){{\dt}}+\frac12x_T^2-x_T,~~~~{\dx}_t=(x_t-u_t){{\dt}}
\end{equation}
We write the value function by \eqref{eqn:value_deterministic}. 
\begin{equation}
    V(t,x):= \inf_{u\in\mathcal{U}_{t}}\int_t^T\left(x_s^2+u_s^2\right){{\ds}}+\frac12x_T^2-x_T,~~~~{\dx}_s=(x_s-u_s){{\ds}}.
\end{equation}
We cannot find value functions using a myopic argument. We will later find a way to find the value function $V$. However, In the next exercise, we apply Algorithm!\ref{alg:dpp} to find the approximate value function.  
\end{eg}
\begin{ex}\label{ex:discrete_lqc}
    In example above, write the approximate DPP from time $t_{i}$ to $t_{i+1}$. Then, assume that $\hat{V}(t_{i+1},x)=a_{i+1}x^2+b_{i+1}x+c_{i+1}$ for some known values $a_{i+1}$, $b_{i+1}$, and $c_{i+1}$. Use  optimization of a quadratic function to find $\hat{V}(t_{i},x)$. Note that you need to use $\hat{x}_{t_{i+1}}=x+(x^2+u^2)\Delta t$. Does $\hat{V}(t_{i},x)$ is of the form $a_{i}x^2+b_{i}x+c_{i}$? What is the relation between $(a_{i},b_{i},c_{i})$ and $(a_{i+1},b_{i+1},c_{i+1})$?
\end{ex}
Example~\eqref{ex:discrete_lqc} is a \emph{linear-quadratic} optimal control problem, LQC. In LQC, the running cost is quadratic in $x$ and $u$, the terminal cost is quadratic in $x$, and the ODE for state variable is linear in $x$ and $u$.

\begin{rem}
Consider the discretized version of the control problem \eqref{prob:deterministic_control}:
\begin{equation}
    \label{prob:discrete_deterministic_control}
    \inf\Big\{\sum_{i=0}^{N-1}C(t_i,\hat{x}_{t_i},u_{t_i}){{\Delta t}}+g(\hat{x}_{T})~|~{u_{t_i}:i=0,...,N-1}\Big\}
\end{equation}
with $\hat{x}_{t_{i+1}} = \hat{x}_{t_i} + f(t_i,\hat{x}_{t_i},u_{t_i}){\Delta t}$. The DPP for this discrete-time optimal control problem is the same as the discretized DPP \eqref{eqn:approximate_deterministic_dpp}.
\end{rem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hamilton-Jacobi equation}
\label{sec:HJ}
One of the side-products of DPP is a partial differential equation, \emph{Hamilton-Jacobi equation} or HJ for short, that can be solved via different methods to provide us with the solution of the control problem. 
To derive the HJ, we first assume that the value function is sufficiently continuously differentiable ,first order derivatives exist are are continuous. Then, we use the first Taylor polynomial with the remainder term for the value function $V(s,x_s)$ about $(t,x)$ as follows:
\[
V(s,x_s) = V(t,x) + V_t(t,x)(s-t) + V_x(t,x)(x_s-x) + R_2
\]
Note that $x_s=x+\int_t^s f(x_r,u_r){{\dr}}$. Let's insert the above Taylor polynomial into the DPP:
\[
\begin{split}
    V(t,x) & =\inf_{u\in\mathcal{U}_{t,s}}\int_t^sC(r,x_r,u_r){{\dr}}+V(s,x_s)\\
    & = \inf_{u\in\mathcal{U}_{t,s}}\int_t^sC(r,x_r,u_r){{\dr}}+V(t,x) + V_t(t,x)(s-t) + V_x(t,x)\int_t^s f(x_r,u_r){{\dr}} + R_2\\
    &= V(t,x) + V_t(t,x)(s-t) +\inf_{u\in\mathcal{U}_{t,s}}\int_t^sC(r,x_r,u_r){{\dr}} + V_x(t,x)\int_t^s f(x_r,u_r){{\dr}} + R_2\\
\end{split}
\]
$V(t,x)$ on both sides can be canceled. Note that $R_2=o(s-t)$, which means $\lim_{s\to t} \frac{R_2}{s-t}=0$.
Dividing both sides by $s-t$ and sending $s\to t$, we obtain
\begin{equation}\label{eqn:HJ_deterministic}
        0 = V_t(t,x) +\inf_{u} \{C(t,x,u) + V_x(t,x)f(t,x,u)\}
\end{equation}
In the above, we used the fundamental theorem of calculus to write $\lim_{s\to t} \frac{\int_t^s f(x_r,u_r){{\dr}}}{s-t}=f(t,x,u)$ and $\lim_{s\to t} \frac{\int_t^sC(r,x_r,u_r){{\dr}}}{s-t}=C(t,x,u)$.
A PDE requires proper boundary condition. For HJ, the boundary condition is the terminal cost at $T$:
\begin{equation}
    \label{eqn:HJ_terminal}
    V(T,x)=g(x)
\end{equation}
The nonlinear function in the HJ equation
\begin{equation}
    H(t,x,p):=\inf_{u} \{C(t,x,u) + pf(t,x,u)\}
\end{equation}
is called Hamiltonian.
We conclude that the value function of the control problem satisfies the HJ equation: 
\begin{equation}
    \label{eqn:HJ_w_Hamiltonian}
    \begin{cases}
        0=V_t(t,x)+H(t,x,V_x(t,x))\\
        V(T,x)=g(x)
    \end{cases}
\end{equation}
The optimal control $u^*(t,x)$ must satisfy
\begin{equation}\label{cond:optimality_deterministic}
C(t,x,u^*(t,x)) + V_x(t,x)f(t,x,u^*(t,x)) = H(t,x,V_x(t,x))\le H(t,x,u),~~\forall u.
\end{equation}

Let's explore the HJ inside an example.
\begin{eg}
    Consider the control problem in Section~\ref{sec:opt_control}:
\begin{equation}
\inf_{u}\bigg\{\int_0^T\Big(x_t^2 +u_t^2\Big) {{\dt}}\bigg\}
\end{equation}
where ${\dx}_t = (-\beta x_t + u_t ){{\dt}}$.
Here, $C(t,x,u)=x^2 + u^2$ and $f(t,x,u)=-\beta x + u$.
The HJ is given by:
\[
0 = V_t + x^2 -\beta x V_x + \inf_{u} \{u^2 + V_x u\}=V_t + x^2 -\beta x V_x -\frac14 V_x^2 
\]
Note that the infimum above is attained at $u^*=-\frac12V_x$ and, therefore, we can write the HJ without using infimum:
\[
0 = V_t + H(x,p) ~~\text{ with }~~ H(x,p)=x^2 -\beta x p -\frac14 p^2 
\]
Since there is no terminal cost, the boundary condition for the HJ is $V(T,x)=0$. 
\end{eg}
An important question is that how can the HJ equation help us find the optimal control. For the above example, if we solve the HJ and find the value function, then a candidate for the optimal control is $u_t^*=-\frac12V_x(t,x_t)$ where ${\dx}_t = (-\beta x_t + u_t^* ){{\dt}}$. In general, we cannot guarantee a simple way to solve the HJ. However, for certain problems there is a simple way and for other problems numerical solutions to PDEs can be used. Next, exercise shows how to solve the HJ derived for the previous example.
\begin{ex}\label{ex:lq_solution}
    We guess that the solution for the HJ
    \[
    \begin{cases}
    0 = V_t + x^2 -\beta x V_x -\frac14 V_x^2 \\
    V(T,x)=0
    \end{cases}
    \]
    takes the form $V(t,x)=a(t)x^2+b(t)x+c(t)$. Insert this guess into the HJ and find ODEs for $a(t)$, $b(t)$, and $c(t)$. Use terminal conditions to prescribe terminal conditions for $a$, $b$, and $c$. Can you solve the system of ODEs?
\end{ex}

After solving the HJ equation and obtaining a solution, we often require a verification step. The reason for the necessity of verification is that in certain (rare) cases, there can be multiple solutions for HJ equation with the terminal condition. We want to make sure the solution matches the value function
\begin{thm}
\label{thm:verification_deterministic}
    Assume that function $v(t,x)$ is once continuously differentiable with respect to $t$ and twice  continuously differentiable with respect to $x$ and satisfies the HJ equation \eqref{eqn:HJ_w_Hamiltonian}. Further more, assume that there exists a function $u^*(t,x)$ such that \eqref{cond:optimality_deterministic} holds and $u^*(t,x^*_t)$ with $x_t^*$ satisfying $\dx^*_t=f(t,x^*_t,u^*(t,x^*_t))$ constitutes an admissible control; $u^*(t,x^*_t)\in\mathcal{U}$.Then, $v=V$.
\end{thm}
\begin{ex}
    Does $v(t,x)=a(t)x^2+b(t)x+c(t)$ in Exercise~\ref{ex:lq_solution} satisfies the condition of verification theorem, Theorem~\ref{thm:verification_deterministic}?
\end{ex}
\begin{ex}\label{ex:SIR}
        In epidemiology, modeling a disease in a population is done by splitting the population into three groups: susceptible, infected, and recovered. Then, then number of individuals in each group at time $t$ is denoted by $S_t$, $I_t$, and $R_t$ which follow the following ODEs.
        \[
        \begin{cases}
            \dd{S}_t=-\beta_t S_t I_t \dt\\
            \dd{I}_t = (\beta_t I_t S_t - \gamma_t I_t)\dt\\
            \dd{R}_t= \gamma_t I_t \dt
        \end{cases}
        \]
        Here $\beta_t\in[b_0,b_1]$ is the rate at which a susceptible individual gets infected and $\gamma_t\in[c_0,c_1]$ is the rate at which an infected individual recovers from the disease.  We assume that investing in some public health  measures decreases $\beta_t$ and and investing in a cure increases $\gamma_t$. Both both investments come with a cost function, $C(\beta,\gamma)=(b_1-\beta_t)^2+\gamma_t^2$. Finally, at a finite time $T$, we like to have the number of infected individuals as low as possible, $g(I)=I^2$.
    \[
    \inf_{\substack{\beta_t\in[b_0,b_1] \\\gamma_t\in[c_0,c_1]}} \int_0^T \big((b_1-\beta_t)^2+\gamma_t^2\big)\dt + I^2_T
    \]
    Write the HJ equation for this control problem.
\end{ex}

\begin{ex}
    In Exercise~\ref{ex:SIR}, notice that $\dd(S_t+I_t+R_t)=0$. This should allow us to reduce the number of state variables $x_t=(S_t,I_t,R_t)$ to two, in place of three. Assume that the population size is given by $N$, $S_t+I_t+R_t=N$. Remove the variable $R_t$ and write the HJ equation in terms of $(S_t,I_t)$.
\end{ex}

\begin{ex}\label{ex:consumption}
    The following problem studies the optimal consumption from a savings account. Let the balance of the savings account be given by: 
        \[
        \dd{X}_t = (rX_t - c_t)\dt
        \]
        $c_t\ge0$ is the rate of consumption.
        \[
        {\color{orange}\sup_{c_t\ge0}}\int_0^T U(c_t)\dt + U(X_T)
        \]
        Function $U$, called utility function, is a concave function that represent our enjoyment from consumption or wealth. The concavity signifies the fact that if our consumption or wealth level is low, increasing one more unit grants more joy compared to when our consumption or wealth level is higher and we obtain one more unit. In general, a utility function is an increasing concave function. Example of utility function is given in Figure~\ref{fig:utilities}.
        
        In the above problem, we are maximizing a combination running \emph{gain} on consumption and terminal gain on wealth. Therefore, the Hamiltonian in the HJ equation has a {\color{orange}supremum}. 

        \begin{figure}
            \centering
            \includegraphics[width=0.35\linewidth]{Control_lecture_notes/Figs/utility1.pdf}\includegraphics[width=0.35\linewidth]{Control_lecture_notes/Figs/utility2.pdf}
            \caption{Utility functions: left $U(x)=1-e^{-\gamma x}$, right $U(x)=\frac{x^{\gamma}}{\gamma}$}
            \label{fig:utilities}
        \end{figure}
        
        Derive the HJ equation for this problem.
\end{ex}

\begin{ex}\label{ex:consumption_no_terminal}
    In the above exercise, if we ignore the terminal gain, we obtain 
    \[
        {\color{orange}\sup_{c_t\ge0}}\int_0^T U(c_t)\dt,~~\dd{X}_t = (rX_t - c_t)\dt
    \]
    Is there something wrong with this problem? What stops us from consuming infinite amount and go negative on the wealth?

    If we restrict the consumption to maintain a positive balance, $X_t\ge0$ for all $t$, does the issue go away?
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Exponential decay in control problems}
We consider the case where, $C(t,x,u)={\color{blue}e^{-r t}}\bar{C}(u,x)$ and $f(t,x,u)=f(x,u)$
\begin{equation}
\label{prob:deterministic_decay}
    \inf_{u}\int_0^T e^{-k t}\bar{C}(u_t,x_t)\dt + {\color{blue}e^{-k T}}g(x_T),~~\dx_t=f(x_t,u_t)\dt
\end{equation}
The \emph{exponential decay} term, ${\color{blue}e^{-kt}}$ emphasized on the importance of cost functions relative to the starting time. As time grow, the contribution of a unit of cost decreases. This is a common practice in applications from economics. While we can write the value function and the dynamic programming principle the same way as \eqref{eqn:value_deterministic} and \eqref{eqn:dpp_deterministic}, we can slightly modify the definition of value function and simplify the DPP and the HJ equation.
\begin{defn}
The value function of \eqref{prob:deterministic_decay} for $x_t=x$ is defined by
\begin{equation}\label{eqn:value_deterministic_decay}
    V(t,x):= \inf_{u\in\mathcal{U}_{t}}\int_t^Te^{-k(s-t)}\bar{C}(x_s,u_s){{\ds}}+e^{-k(T-t)}g(x_T),~~~~{\dx}_s=f(x_s,u_s){{\ds}}
\end{equation}
where $\mathcal{U}_{t}$ is the set of admissible controls restricted to time interval $[t,T]$.
\end{defn}
The dynamic programming principle is modified by:
\begin{thm}\label{thm:stoch_dpp_no_stopping_deterministic_decay}
    [Dynamic Programming Principle]
    Let $0\le t<s\le T$ and $x_t=x$. Then, the DPP for \eqref{prob:deterministic_control} is given by 
    \begin{equation} \label{eqn:dpp_deterministic_decay}
    V(t,x) =\inf_{u\in\mathcal{U}_{t,s}}\int_t^s e^{-k(r-t)}\bar{C}(x_r,u_r){{\dr}}+e^{-k(s-t)}V(s,x_s),~~~~{\dx}_r=f(x_r,u_r){{\dr}}
\end{equation}
where $\mathcal{U}_{t,s}$ is the set of admissible controls restricted to time interval $[t,s]$.
\end{thm}
\begin{proof}
    The proof is quite similar to Theorem~\ref{thm:stoch_dpp_no_stopping_deterministic}.
\end{proof}
    We apply Taylor series on $e^{-k(s-t)}V(s,x_s)$ around $(t,x)$.
    \[
        \begin{split}
             {e^{-k(s-t)}}V(s,x_s) = &V(t,x) + {e^{-k(s-t)}}\Big(\big(V_t(t,x) {- k V(t,x)}\big)(s-t)\\
             &~~~~+ V_x(t,x)\cdot\int_t^s f(x_r,u_r)\dr\Big) + o((s-t)^2)
        \end{split}
        \]
       We use Taylor series inside \eqref{eqn:dpp_deterministic_decay} to write
        \[
        \begin{split}
            0=\inf_{u}\int_t^s &{e^{-k(r-t)}}\bar{C}(x_r,u_r)\dr \\
            &+  {e^{-k(s-t)}}\Big(\big(V_t(t,x) {- k V(t,x)}\big)(s-t)\\
             &~~~~+ V_x(t,x)\cdot\int_t^s f(x_r,u_r)\dr\Big) + o((s-t)^2)
        \end{split}
        \]
        Dividing both sides by $s-t$ and sending $s\to t$ yields the HJ equation.
        \begin{equation}
            \label{eqn:HJ_deterministic_decay}
            0=V_t(t,x) - k V(t,x)+\inf_{u}\Big\{\bar{C}(x,u) + V_x(t,x)\cdot f(x_r,u_r)\Big\}
        \end{equation}
        The term ${\color{blue}-kV}$ in the HJ equation is coming from the discounting. 
        The boundary (terminal) condition remains the same, $V(T,x)=g(x)$. 
        \begin{equation}
        \label{eqn:HJ_decay_w_Hamiltonian}
            \begin{cases}
                0=V_t(t,x) - k V(t,x)+H(x,V_x)\\
                V(T,x)=g(x)
            \end{cases}
        \end{equation}
        with
        \[
        H(x,p)=\inf_{u}\Big\{\bar{C}(x,u) + p\cdot f(x_r,u_r)\Big\}
        \]
        The relation between the HJ equation with decay term in \eqref{eqn:HJ_deterministic_decay} and the HJ equation in \eqref{eqn:HJ_deterministic} can easily be verifies by the change of variable $\bar{V}(t,x)=e^{k(T-t)}V(t,x)$. $V(t,x)$ satisfies \eqref{eqn:HJ_decay_w_Hamiltonian} if and only if $\bar{V}(t,x)$ satisfies \eqref{eqn:HJ_w_Hamiltonian}. 
        \begin{ex}
            Show that $V(t,x)$ satisfies \eqref{eqn:HJ_decay_w_Hamiltonian} if and only if $\bar{V}(t,x)$ satisfies \eqref{eqn:HJ_w_Hamiltonian}. 
        \end{ex}
    
    A good question to ask ourselves is that for Problem \eqref{prob:deterministic_decay} if the the HJ equation with or without decay term can be obtained via a simple change of variable, why do we introduce a different value function and DPP for exponential decay?
    The answer lies in the case when $T=\infty$:
    \begin{equation}
    \label{prob:deterministic_decay_infinite}
        \inf_{u}\int_0^\infty e^{-k t}\bar{C}(u_t,x_t)\dt ,~~\dx_t=f(x_t,u_t)\dt
    \end{equation}
    Since $f(x_t,u_t)$ does not depend on $t$, the solution $x_s$ of ${\dx}_s=f(x_s,u_s){\ds}$ with $x_t=x$ is the same as a time shift of $\bar{x}_r$ the solution of ${\dx}_r=f(x_r,u_r){\ds}$ with $x_0=x$ by $t$.
    \[
        V(t,x)= \inf_{u\in\mathcal{U}_{t}}\int_t^\infty e^{-k(s-t)}\bar{C}(x_s,u_s){{\ds}}{{\ds}}=\inf_{u\in\mathcal{U}}\int_0^\infty e^{-kr}\bar{C}(\bar{x}_{r},u_{r}){{\dr}}
    \]
    Therefore, value function is time homogeneous
    and is given by
    \begin{equation} \label{eqn:value_decay_infinite}
    V(x)=\inf_{u\in\mathcal{U}}\int_0^\infty e^{-kr}\bar{C}(\bar{x}_{r},u_{r}){{\dr}}
    \end{equation}
    and the HJ equation is given by 
    \begin{equation} \label{eqn:HJ_decay_w_Hamiltonian_homogeneous}
                0=- k V(t,x)+H(x,V_x)            
    \end{equation}
    \begin{ex}
    Consider the consumption problems, Exercise~\ref{ex:consumption} and Exercise~\ref{ex:consumption_no_terminal}, in infinite horizon with decay term:
    \[
        {\sup_{c_t\ge0}}\int_0^\infty e^{-kt}U(c_t)\dt,~~\dd{X}_t = (rX_t - c_t)\dt
    \]
    Here, set of admissible consumption are those such that we maintain $X_t\ge0$ for all $t\ge0$. Write the value function by using \eqref{eqn:value_decay_infinite} and the HJ equation by \eqref{eqn:HJ_decay_w_Hamiltonian_homogeneous}. Find proper boundary condition and solve the HJ equation with the boundary condition.
    \end{ex}
        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{An introduction to viscosity solution}
To motivate viscosity solutions, we introduce the following example.
\begin{eg}
\label{eg:eikonal_2}
    Recall from Exercise~\ref{ex:eikonal} that
    \[
        \inf_u\int_0^\infty \mathds{1}_{\{x_t\in D\}}{\dt},~{\dx}_t=u_t{{\dt}}~\text{ with }~|u_t|\le1
    \]
    The value function for initial state $x_0=x\in D$ is given by
    \[
        V(x)=\inf_u\int_0^\infty \mathds{1}_{\{x_t\in D\}}{\dt},~{\dx}_t=u_t{{\dt}}~\text{ with }~|u_t|\le1
    \]
    Here $C(x,u)=C(x)=\mathds{1}_{\{x\in D\}}$ and $f(x,u)=f(u)=u$.
    Notice that the value function \emph{does not} depend on $t$, because the total cost is a function of duration of time until exit and not the function of current time.

    The HJ equation is given by
    \[
    H(V_x)=0,~~~H(x,p):=\inf_{|u|\le1}\{\mathds{1}_{\{x\in D\}}+p\cdot u\}
    \]
    We can restrict the Hamiltonian to points $x\in D$ and write 
    \[
    H(p):=\inf_{|u|\le1}\{1+p\cdot u\}=1+\inf_{|u|\le1}\{p\cdot u\}
    \]
    The infimum above is attained for $u=-\frac{u}{|u|}$. Therefore,
    \[
    H(p)=|p|
    \]
    Outside (or on the boundary of) $D$, the exit time is zero. Therefore, we restrict the Hamiltonian to inside of the domain and impose the boundary condition $V(x)=0$ for $x\in\partial D$. We conclude that the HJ equation with the boundary condition is given by
    \[
    \begin{cases}
        1-|V_x|=0&x\in D\\
        V(x)=0&x\in \partial D
    \end{cases}
    \]
\end{eg}
Example~\ref{eg:eikonal_2} can shed a light on the notion of solution to the HJ equations. Take $D=[-1,1]\subset \mathbb{R}$ and consider the following functions,  shown in Figure~\ref{fig:v1v2}:
        \[
        v_1(x) = 1-|x|, v_2(x)= \begin{cases}
            \frac12-|x-\frac12|&0\le x\le1\\
            \frac12-|x+\frac12|&-1\le x<0
        \end{cases}
        \]
The minimal exit time of $[-1,1]$ is given by $v_1$. Additionally, except at point $x=0$, $v_1$ is differentiable, satisfies the HJ equation, and the optimal direction and speed is given by $V_x=\text{\normalfont sgn}(x)$. $v_2$ is not the value function  , but it also satisfies the HJ equation except at three points, $x=0,\pm\frac12$. 
 In fact, with a little bit of effort, one can write infinitely many solutions for the Eikonal equations. All this solutions have points of non-differentiability. However, the right and the left derivatives at those points still satisfy the HJ equation. 
\begin{figure}
    \centering
    \includegraphics[width=0.25\linewidth]{Control_lecture_notes/Figs/v1.pdf}\includegraphics[width=0.25\linewidth]{Control_lecture_notes/Figs/v2.pdf}
    \caption{Functions $v_1$ and $v_2$ solve Eikonal equation, the HJ equation for \ref{eg:eikonal_2}. But, only  $v_1$ is the same as the value function.}
    \label{fig:v1v2}
\end{figure}
That raises an issue; \emph{if we solve an HJ equation, how do we know if we obtained the correct solution, hence, the value function?} The answer to this problem is quite complicated. The theory that solves the issue is the theory of \emph{viscosity solutions}, which we discuss in more details in a later chapter. Here, we explain a simpler version of this theory by focusing on the Eikonal equation, $0=1-|v^{\prime}(x)|$ with boundary condition $v(-1)=v(1)=0$. 

We introduce a new equation by adding a second order term. 
\[
0=1-|v^{\epsilon\prime}(x)|+\epsilon V^{\epsilon\prime\prime}, ~~ v^{\epsilon}(-1)=v^{\epsilon}(1)=0
\]
The above equation is a second order equation with two boundary conditions, which guarantees existence of a unique differentiable solution which can be found in closed form by solving
\[
\begin{cases}
    0=1+v^{\epsilon\prime}(x)+\epsilon V^{\epsilon\prime\prime}& x\in(0,1)\\
    0=1-v^{\epsilon\prime}(x)+\epsilon V^{\epsilon\prime\prime}& x\in(-1,0)\\
    v^{\epsilon\prime}(0)=0&\\
    v^{\epsilon}(0-)=v^{\epsilon}(0+)&\\
    v^{\epsilon}(-1)=v^{\epsilon}(1)=0
\end{cases}
\]
\begin{ex}
    Find a closed form solution. above system of equations.
\end{ex}
Then, if we evaluate $\lim_{\epsilon\to0}v^{\epsilon}$, we obtain $v_1=V$.

\citet{CIL92}, codified the above construction into the theory of viscosity solutions, which perfectly fits the need of HJ equations in optimal control. For the points of non-differentiability, they introduces candidates for  derivative from above and from below. More precisely, they defined the set of \emph{superjets} as the slope of all lines that touch the function from above and the set of \emph{subjets} as the slope of all lines that touch the function from below, as shown in Figure~\ref{fig:jets}.

\begin{figure}
    \centering
    \includegraphics[width=0.35\linewidth]{Control_lecture_notes/Figs/superjets.pdf}\includegraphics[width=0.35\linewidth]{Control_lecture_notes/Figs/jets.pdf}
    \caption{Superjets and subjets for functions $v_1$ and $v_2$ at the points of non-differentiability.}
    \label{fig:jets}
\end{figure}
The set of superjets, $\overline{J}$, and subjets, $\underline{J}$, for all the points of non-differentiability are the closed interval $[-1,1]$. According to the theory of viscosity solutions, the members of superjet must satisfy the equation with inequality:
\[
\forall a\in\overline{J}=[-1,1], 1-|a| \ge 0
\]
which is trivially correct for for $v_1$ at $x=0$ and for $v_2$ at $x=\frac12=-\frac12$.
Similarly,  the members of subjet must satisfy the equation with inequality in the opposite direction of superjets:
\[
\forall a\in\underline{J}=[-1,1], 1-|a| \le 0.
\]
However, for $v_2$ at $x=0$, $a\in\underline{J}=(-1,1)$ $1-|a| \not\le 0$. Therefore, $v_2$ is not a viscosity solution of the eikonal equation.

Interesting results from the theory of viscosity solution are as follows.
\begin{enumerate}[label=\arabic*)]
    \item The value function of a control problem is a viscosity solution of the HJ equation.
    \item HJ equation has a unique viscosity solution.
    \item Numerical solutions to HJ equations with certain standard properties converge to the viscosity solution.
\end{enumerate}
Even though there are more details into these results, we continue to use these results in this note until we require more clarifications.

\subsection{Summary}
The steps of solving deterministic control problems via HJ problem are described in Chart~\ref{fig:chart1}:
\begin{figure}[ht!]
    \centering
    \includegraphics[height=0.5\textheight]{Control_lecture_notes/Figs/HJ_chart.pdf}
    \caption{Summary of HJ solution method for control problems}
    \label{fig:chart1}
\end{figure}
It is important to note that when the HJ equation is derived, solving HJ equation is an standalone problem. After solving the HJ equation, the solution and its derivatives are used to find an optimal control and the corresponding optimal trajectory.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Pontryagin principle}
\label{sec:det_pontryagin}
Consider the simple control problem in Section~\ref{sec:opt_control}:
\begin{equation}
\inf_{u}\bigg\{\int_0^T\Big(x_t^2 - \alpha_t x_t\Big) {{\dt}}\bigg\}
\end{equation}
where ${\dx}_t = (-\beta x_t + u_t ){{\dt}}$, $x_0=x$.
One can consider the dynamics of $x$ as a constraint. Therefore, we can formally write Lagrangian by 
\begin{equation}\label{ex:sup_inf}
    \sup_{\lambda}\inf_{u,x}\bigg\{\int_0^T\big(x_t^2 - \alpha_t x_t\big) {{\dt}} - \int_0^T \lambda_t \big({\dx}_t + (\beta x_t  - u_t ){{\dt}}\big)\bigg\}
\end{equation}
Here, there are two important remarks. First,
 since the constraint is given by a differential equation for each $t$, the dual variable $\lambda$ is a function of $t$. Second, the problem is now unconstrained, that is, the state variable $x$ and the control variable $u$ are now both  variables in an optimization problem.
 
The main trick to solve this optimization problem is integration by part formula
\begin{equation}\label{eqn:pontryagin_ibp}
    \int_0^T \lambda_t {\dx}_t = \lambda_T x_T - \lambda_0 x_0 - \int_0^T x_t{\dd}\lambda_t
\end{equation}
to write 
\eqref{ex:sup_inf} as
\begin{equation}\label{ex:pontryagin}
    \sup_{\lambda}\inf_{u,x}\bigg\{\int_0^T\Big(x_t {\dd}\lambda_t-\lambda_t( \beta x_t - u_t )+ x_t^2 - \alpha_t x_t \Big) {{\dt}} - \lambda_T x_T + \lambda_0 x_0\bigg\}
\end{equation}
Note that optimization on $x_T$ is independent of  $x_t$ for $t<T$. The KKT conditions for the strong duality in the saddle point problem \eqref{ex:pontryagin} were discovered by Lev Pontryagin in 1952. Define the \emph{Hamiltonian} by
\begin{equation}
    H(t,x,\lambda,u):=-\lambda( \beta x - u )+ x^2 - \alpha_t x
\end{equation}
Thus, \eqref{ex:pontryagin} is written as a saddle point problem with free variables $x_t$, $u_t$, and $\lambda_t$:
\begin{equation}
    \sup_{\lambda}\inf_{u,x}\bigg\{\int_0^T\Big(x_t {\dd}\lambda_t+ H(t,x_t,\lambda_t,u_t)\Big) {{\dt}} - \lambda_T x_T + \lambda_0 x_0\bigg\}
\end{equation}
\begin{equation}
    \begin{cases}
        {\dd}\lambda^*_t+ \partial_x H(t,x^*_t,\lambda^*_t,u^*_t){{\dt}}={\dd}\lambda^*_t+ (-\lambda^*_t \beta  +2x^*_t -\alpha_t){{\dt}}=0~ (\textrm{minimize integrand wrt $x$})\\
        \lambda^*_T=0 ~(\textrm{minimize terminal wrt $x_T$})\\
        H(t,x^*_t,\lambda^*_t,u^*_t)\le H(t,x^*_t,\lambda^*_t,u)~\textrm{ for all } u ~(\textrm{minimize integrand wrt $u$})\\
        {\dx}^*_t = (-\beta x^*_t + u^*_t) {{\dt}} ~(\textrm{constraint})
        \end{cases}
\end{equation}
In the above, the first equation  is obtained from taking derivative with respect to $x_t$, second equality corresponds to derivative with respect to $x_T$ in $\lambda_T x_T$, the third line guarantees the optimality of $u^*$, and the last equation is the  constraint of the problem which is the dynamic of the state variable in the control. 
\begin{ex}
    Show that $\lambda^*_t=0$, $x^*_t=\alpha_t/2$, and $u^*$ with $\alpha_t=2\int_0^te^{-\beta (s-t)}u^*_s {{\ds}}$ satisfy Pontryagin principle for the above problem. 
\end{ex}

Following the steps of in the above example, the Pontryagin principle for a generic deterministic control problem, \ref{prob:deterministic_control} is described as the following saddle point problem.
\begin{equation}\label{eqn:sup_inf}
    \sup_{\lambda}\inf_{u,x}\bigg\{\int_0^TC(t,x_t,u_t) {{\dt}} +g(x_T) - \int_0^T \lambda_t \big({\dx}_t + (\beta x_t  - u_t ){{\dt}}\big)\bigg\}
\end{equation}
By applying \eqref{eqn:pontryagin_ibp}, we obtain 
\begin{equation}
    \sup_{\lambda}\inf_{u,x}\bigg\{\int_0^T\Big(x_t {\dd}\lambda_t+ H(t,x_t,\lambda_t,u_t)\Big) {{\dt}} +g(x_T) - \lambda_T x_T + \lambda_0 x_0\bigg\}
\end{equation}
where the Hamiltonian is given by 
\begin{equation}
    H(t,x,\lambda,u):=\lambda f(t,x,u)+ C(t,x,u)
\end{equation} 
\begin{thm}\label{thm:pontryagin_deterministic}
    [Pontryagin principle]
    Assume that there exists $(x^*,u^*,\lambda^*):[0,T]\to\mathbb{R}^d\times\mathbb{R}^n\times\mathbb{R}^d$ such that
    \begin{equation}\label{eqn:pontryagin_determinimstic}
    \begin{cases}
        {\dd}\lambda^*_t+ \partial_x H(t,x^*_t,\lambda^*_t,u^*_t){{\dt}}=0~ (\textrm{minimize integrand wrt $x$})\\
        \lambda^*_T=\nabla g(x^*_T) ~(\textrm{minimize terminal wrt $x_T$})\\
        H(t,x^*_t,\lambda^*_t,u^*_t)\le H(t,x^*_t,\lambda^*_t,u)~\textrm{ for all } u ~(\textrm{minimize integrand wrt $u$})\\
        {\dx}^*_t = f(t,x^*_t,u^*_t) {{\dt}} ~(\textrm{constraint})
        \end{cases}
\end{equation}
Then, $u^*$ is an optimal control for \eqref{prob:deterministic_control}.
\end{thm}
% Assume that $u^*$ is \emph{an} \emph{optimal control} for \ref{prob:deterministic_control} and  $x^*$ is \emph{the} optimal trajectory for $u^*$, i.e., ${\dx}^*_t = f(t,x^*_t,u^*_t) {{\dt}}$.
% Then, there exists $\lambda^*$ such that 
% \begin{equation}
%     \begin{cases}
%         {\dd}\lambda^*_t+ \partial_x H(t,x^*_t,\lambda^*_t,u^*_t){{\dt}}=0\\
%         \lambda^*_T=\triangledown g(x^*_T)\\
%         H(t,x^*_t,\lambda^*_t,u^*_t)\le H(t,x^*_t,\lambda^*_t,u)~\textrm{ for all } u
%         \end{cases}
% \end{equation}
The function $\lambda^*_t$, described by \ref{thm:pontryagin_deterministic} is called the \emph{adjoint} process. It is important to distinguish between the two ODEs in \eqref{eqn:pontryagin_determinimstic}, namely, 
\begin{equation}
    {\dx}^*_t = f(t,x^*_t,u^*_t) {{\dt}} \text{ and }{\dd}\lambda^*_t+ \partial_x H(t,x^*_t,\lambda^*_t,u^*_t){{\dt}}=0
\end{equation}
The first one has an initial condition $x^*_0=x_0$, the initial position of the state process. However, the adjoint equation comes with a terminal condition, $\lambda^*_T=\nabla g(x^*_T)$. This makes solving Theorem~\ref{thm:pontryagin_deterministic} challenging. We shall see later how Pontryagin principle is applied to some specific examples such as linear quadratic linear control problem, where solving \eqref{eqn:pontryagin_determinimstic} is simpler to solve. 
\begin{rem}
If $u^*_t$ is an interior minimizer of $H(t,x^*_t,\lambda^*_t,u)$, then we can write $\partial_u H(t,x^*_t,\lambda^*_t,u^*_t)=0$. However, if there are constraint on $u$, e.g., $u\ge0$, we shall stick to the inequality above. 
\end{rem}

We start with an example of a LQC problem. 
\begin{eg}\label{eg:lqc}
    Consider the control problem with $C(x,u)= x^2 +  u^2$, $g(x) = x^2 +  x$, and $f(x,u)=x + u$:
\begin{equation}
    \inf_{u}\int_0^T(x_t^2 + u_t^2 ){{\dt}} + x^2_T + x_T,~\text{ subject to  }~{\dx}_t=( x_t +  u_t){{\dt}}
\end{equation}
\end{eg}
LQC problems can be solved via Pontryagin principle. More precisely,  for the above example, the Hamiltonian is given by
\[
H(x,\lambda,u) = x^2 + u^2 + \lambda ( x + u)
\]
and \eqref{eqn:pontryagin_determinimstic} is 
\begin{equation}\label{eg:pontryagin_lqc}
    \begin{cases}
        {\dd}\lambda^*_t+ (2x^*_t + \lambda^*_t ){{\dt}}=0\\
        \lambda^*_T= 2x^*_T + 1\\
        H(t,x^*_t,\lambda^*_t,u^*_t)\le H(t,x^*_t,\lambda^*_t,u)\\
        {\dx}^*_t=( x^*_t + u^*_t){{\dt}}
        \end{cases}
\end{equation}
Minimizing $H(x,\lambda,u)$ in $u$ suggests that $H_u(x,\lambda^*_t,u^*_t)=2u^*_t+  \lambda^*_t =0$, equivalently, $u^*_t=-\frac{1}{2}\lambda^*_t$.
\begin{ex}
    (1) Use $u^*_t=-\frac{1}{2}\lambda^*_t$ to write the system of ODEs \eqref{eg:pontryagin_lqc} for $\lambda^*$ and $x^*$ by
    \begin{equation}
        \begin{cases}
        {\dd}\lambda^*_t= (- \lambda^*_t - 2x^*_t){{\dt}}\\
        {\dx}^*_t=( -\frac12\lambda^*_t+x^*_t  ){{\dt}}
        \end{cases}
    \end{equation}
    (2) Find the general solution the system of ODEs in terms of $x^*_0$ and $\lambda^*_0$. Hint: Use denationalization of the matrix 
    \begin{equation}
        \left[
        \begin{matrix}
            -1&-2\\
            -\frac12&1
        \end{matrix}
        \right] =\left[\begin{matrix}2(\sqrt2+1)&1\\1&2(\sqrt2-1)\end{matrix}\right] \left[\begin{matrix}-\sqrt{2}&0\\0&\sqrt{2}\end{matrix}\right] \left[\begin{matrix}\frac23(\sqrt2-1)&-1\\-1&\frac23(\sqrt2+1)\end{matrix}\right]
    \end{equation}
    and write the ODEs as 
    \begin{equation}
        \left[\begin{matrix}{\dd}\lambda^*\\{\dx}^*\end{matrix}\right] = \left[
        \begin{matrix}
            1&1\\
            1&-\frac12
        \end{matrix}
        \right] 
        \left[\begin{matrix}\lambda^*\\x^*\end{matrix}\right] = \left[\begin{matrix}2(\sqrt2+1)&1\\1&2(\sqrt2-1)\end{matrix}\right] \left[\begin{matrix}-\sqrt{2}&0\\0&\sqrt{2}\end{matrix}\right] \left[\begin{matrix}\frac23(\sqrt2-1)&-1\\-1&\frac23(\sqrt2+1)\end{matrix}\right]\left[\begin{matrix}\lambda^*\\x^*\end{matrix}\right]
    \end{equation}
    (3) Consider $x^*_0$ given. Use $\lambda^*_T= 2x^*_T + 1$ to find $\lambda^*_0$, hence a special solution for the system of ODEs from Pontryagin principle as a function $x^*_0$ and $x^*_T$.  
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear quadratic control problem}
\label{sec:LQC}
A class of control problem with closed-form solution consists of \emph{linear-quadratic} optimal control problems, LQC henceforth. 
In LQC, the running cost is quadratic in $x$ and $u$, the terminal cost is quadratic in $x$, and the ODE for state variable is linear in $x$ and $u$. We assume that $x$ is a $d$-dimensional column vector and $u$ is an $m$-dimensional column vector.
\begin{equation}
    \begin{split}
        &C(t,x,u) = \left[\begin{matrix}
        x^\top&
        u^\top
        \end{matrix}\right]Q_t\left[\begin{matrix}
        x\\
        u
        \end{matrix}\right]+a_t\cdot x + b_t\cdot u,~\text{ and }~Q_t=\left[\begin{matrix}
        A_t&C_t\\
        C_t^\top&B_t
        \end{matrix}\right]\\
        &g(x) = x^\top A_T x+a_T \cdot x\\
        &f(t,x,u) = M_t x + N_t u
    \end{split}
\end{equation}
Here, $A:[0,T]\to \mathbb{M}(d,d)$, $B:[0,T]\to \mathbb{M}(m,m)$, $C:[0,T]\to \mathbb{M}(d,m)$, $a:[0,T]\to \mathbb{R}^d$, $b:[0,T]\to \mathbb{R}^m$, $M:[0,T]\to \mathbb{M}(d,d)$, and $N:[0,T]\to \mathbb{M}(d,m)$. 


For LQC problem to be well-posed, the value is not $-\infty$, it is sufficient to assume that $Q_t$ and $A_T$ are symmetric and positive definite. In this case $C$ and $g$ are convex functions of $(x,u)$ and $x$, respectively. See exercise below.
\begin{ex}
    In Example~\ref{eg:lqc}, change the running cost function to $C(x,u)= x^2 -  u^2$. Make some effort to find a solution to this modified problem. Of course, since the cost function is not convex, your effort fails. Explain why the optimal control problem is ill-defined.
\end{ex}

\begin{ex}
    Write the HJB equation for the LQC problem.
\end{ex}

\begin{ex}
Assume that the cost functions of the general LQC problem are convex.
    Verify that there exists $G:[0,T]\to \mathbb{M}(d,d)$, $H:[0,T]\to \mathbb{R}^d$, and $K:[0,T]\to \mathbb{R}$ such that 
    $V(t,x) = x^\top G_t x + H_t\cdot x + K_t$ satisfied the HJ equation.
\end{ex}


\begin{ex}
Assume that the cost functions of the general LQC problem are convex.
    Write Pontryagin principle for the general LQC and suggest a solution method. 
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Stochastic Control}
In this chapter, we start generalizing the material in the previous section to include stochastic control. The ingredients of a stochastic control problem is similar to the deterministic control: running cost function $C(t,x,u)$, terminal cost function $g(x)$, and dynamics of the state variable. However, the dynamics of the state variable comes with uncertainty modeled with white noise. 
In the next section, we describe the dynamics of the state variable with a noise component. Such dynamics is called stochastic differential equation, SDE henceforth.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Noise and Brownian motion}
Recall from Section~\ref{chap:deterministic} that the dynamics of state variable is given by
\[
\dX_t=b(t,X_t,u_t)\dt
\]
When the dynamics is noisy, we simply need to write
\[
\dX_t=b(t,X_t,u_t)\dt+\text{noise}
\]
But, what is the noise. Simplest way to model noise is through \emph{white noise}. White noise, denoted by $\dW_t$ is a stochastic process that evolves by time $t$ and for two different times $t_1$ and $t_2$, $\dW_{t_1}$ and $\dW_{t_2}$ are independent. To many's disappointment, it turns out such a stochastic process does not exist in a conventional sense and we have to introduce something called \emph{Brownian motion}, a.k.a. \emph{Wiener process} that describes the noise.
A Wiener process is defined by certain properties:
\begin{defn}
A stochastic process $\{X_t\}_{t}$ is a set of random variables indexed by time $t\in[0,\infty)$. It represents evolution of an uncertain quantity over time. At each time $t$, $X_t$ is a random variable with a certain distribution that depends on $t$. Furthermore, the joint distribution of $X_{t_1},...,X_{t_n}$ for different times $t_1,...,t_n$ is important in shaping a stochastic process.

An observation, denoted by $\omega$, is one \emph{realization} of a stochastic process. The function $X(\omega):[0,\infty)\to\mathbb{R}^d$ is called a sample path of $X$.
\end{defn}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{Control_lecture_notes/Figs/bm_1d_100.png}
    \caption{Eight different sample paths of a stochastic process. The randomness causes each sample paths to be different.}
    \label{fig:random_var}
\end{figure}
\begin{defn}\label{defn:bm}
    A Wiener process or Brownian motion is a stochastic process with the following property.
    \begin{enumerate}[label = \bfseries \arabic*)]
        \item $W_0$ is distributed according to a known distribution, $\mu$. $\mu$ can be a Dirac delta at a point, $W_0=0$.
        \item For any $t_{1}<\cdots<t_{n}$, $W_{t_{n}}-W_{t_{n-1}},...,W_{t_{2}}-W_{t_{1}}$ are independent and have Gaussian distribution with mean zero and variance $t_{n}-t_{n-1},...,t_{2}-t_{1}$.
        \item Sample paths of $W$ are continuous functions of $t$.
    \end{enumerate}
\end{defn}
For practical purposes, we do not need the above definition. A Wiener process can be understood through the lens of random walk. Let's in dimension $d$, consider a sequence of independent random variables which take $2d$ values $S:=\{\pm e_1,...,\pm e_d\}$, where $\{e_i=(0,...,0,\underbrace{1}_{i^\text{th}},0,...,0):i=1,...,d\}$ with equal probabilities is the standard basis with for $\mathbb{R}^d$. In dimension $1$, there are two directions. In dimension $d=2$, there are four directions. The set $S$ represents the steps of the walk. At each time, the random walk randomly chooses a step from set $S$ and moves accordingly. A sample paths of a 2-$d$ random walk is shown in Figure~\ref{fig:rw}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Control_lecture_notes/Figs/rw_2d_1.png}
    \caption{A sample paths of a 2-$d$ random walk starting at $(0,0)$ with 400 steps.}
    \label{fig:rw}
\end{figure}
To obtain a Wiener process from a random walk, we modify the size of the steps and the timing between two steps. For $N\in\mathbb{N}$, we modify the step size to $\sqrt{1/N}$ and modify the time between two steps by $1/N$. As $N\to\infty$, we obtain a process that possesses the properties described in Definition~\ref{defn:bm}. 
\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{Control_lecture_notes/Figs/rw_2d_1.png}\includegraphics[width=0.45\linewidth]{Control_lecture_notes/Figs/rw_2d_10.png}
    \includegraphics[width=0.45\linewidth]{Control_lecture_notes/Figs/rw_2d_100.png}
    \includegraphics[width=0.45\linewidth]{Control_lecture_notes/Figs/rw_2d_1000.png}
    \caption{Top left: a random walk with step size $1$ and pacing $1$. Top right: a random walk with step size $\sqrt{0.1}$ and pacing $0.1$. Bottom left: a random walk with step size $0.1$ and pacing $0.01$. Bottom right: a random walk with step size $\sqrt{0.001}$ and pacing $0.001$. As $N\to\infty$, a random walk with step size $\sqrt{1/N}$ and pacing $1/N$ converges to a Wiener process given in Definition~\ref{defn:bm}. Specifically, the continuous sample paths are preserved under the limit.}
    \label{fig:bm_construction}
\end{figure}
At a point in the random walk, the slope of the path is $\frac{\sqrt{1/N}}{1/N}=\sqrt{1/N}\to\infty$. Therefore, it is expected that the derivative of the Wiener process, $\dW_t$, does not exist. This derivative is what we heuristically know as the white noise. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stochastic differential equation (SDE)}
\label{sec:sde}
To include noise in $\dX_t=b(t,X_t,u_t)\dt+\text{noise}$, we can still write  $\dX_t=f(t,X_t,u_t)\dt+\dW_t$. However, we should interpret the equation as in integral equation:
\[
X_t=X_0+\int_0^t b(s,X_s,u_s)\ds+W_t
\]

The magnitude of noise $W_t$ is governed by a multiplicative constant, $\sigma W_t$. In this case, the variance of the noise is $\sigma^2 t$.   
\[
X_t=X_0+\int_0^t b(s,X_s,u_s)\ds+\sigma W_t
\]
While $\sigma$ can be a constant, we can think about a generalization where $\sigma$ a function of time $t$ and state $X_t$, i.e. $\sigma(t,X_t) \dW_t$. This means for different $(t,x)$ the magnitude of noise evolves in time and bases on the current value of state variable. For example, $X_t\dW_t$ means when $X_t$ is close to zero, the noise is small. If $X_t$ is large, the noise is significant. 

Since we are in the control context, we can talk about controlling the noise magnitude, $\sigma(t,X_t,u_t) \dW_t$. For instance, for $\sigma(t,x,u)=ux$, $u_tX_t\dW_t$, when we set $u=0$, the noise vanishes. 

To summarize, the general noise term added to an ODE is written by 
\[
\dX_t= b(t,X_t,u_t)\dt+\sigma(t,X_t,u_t)\dW_t
\]
or in integral form
\[
X_t=X_0+\int_0^t b(s,X_s,u_s)\ds+{\color{orange}\int_0^t \sigma(s,X_s,u_s)\dW_s}
\]
This is what we refer to by \emph{stochastic differential equation} or SDE.

The integral $\int_0^t \sigma(s,X_s,u_s)\dW_s$ is called Itô integral and is defines by
\begin{equation}
\label{eqn:ito_sum}
\int_0^t \sigma(s,X_s,u_s)\dW_s = \lim_{|t_n-t_{n-1}|\to0}\sum_{n=1}^{N} \sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})    
\end{equation}

We can call the above summation an Itô sum, which is different from a Riemann sum. The time chosen in the interval $[t_{n-1},t_{n}]$ in the Itô sum is always the left point of the interval $t_{n-1}$. Any other choice of point leads to an integral different from Itô's.  This is important because in the Itô sum \eqref{eqn:ito_sum} $\sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})$ and $W_{t_{n}}-W_{t_{n-1}}$ are independent. Recall property (2) from the definition of Wiener process; Definition~\ref{defn:bm}.
\subsection{What do we need to know about Wiener process and Itô integral?}
\label{sec:toknow_Wiener}
For starters, we need to know that \emph{$W_{t_{n}}-W_{t_{n-1}}$ has mean zero}. 
We also need to recall that by property (2) in Definition~\ref{defn:bm}, \emph{$W_{t_{n}}-W_{t_{n-1}}$ is independent of the past including any term that has index $t_{n-1}$ or smaller index}.
As a result of this, 
\[
\begin{split}
    \mathbb{E}\left[\sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\right]=\mathbb{E}\left[\sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})\right]\underbrace{\mathbb{E}\left[W_{t_{n}}-W_{t_{n-1}}\right]}_{=0}=0
\end{split}
\]
and therefore, 
\[
\mathbb{E}\left[\int_0^t \sigma(s,X_s,u_s)\dW_s\right]=0
\]
On the other hand, we need to know 
\[
\mathbb{E}\left[(W_{t_{n}}-W_{t_{n-1}})^2]\right]=t_n-t_{n-1}
\]
We also need to remember that \emph{$\mathbb{E}\left[(W_{t_{n}}-W_{t_{n-1}})^2\right]=t_{n}-t_{n-1}$}.
Therefore, 
\[
\begin{split}
    \mathbb{E}&\left[\left(\sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\right)^2\right]\\
    &\hspace{1cm}=\mathbb{E}\left[\sigma^2(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})\right]\underbrace{\mathbb{E}\left[(W_{t_{n}}-W_{t_{n-1}})^2\right]}_{=t_{n}-t_{n-1}}\\
    &\hspace{1cm}=\mathbb{E}\left[(\sigma^2(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(t_{n}-t_{n-1})\right]
\end{split}
\]
Further calculations:
\[
\begin{split}
    &\left(\sum_{n=1}^{N} \sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\right)^2=  \sum_{n=1}^{N} \sigma^2(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})^2\\
    &\hspace{2.5cm}+2\sum_{1\le n<m\le N} \sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\sigma(t_{m-1},X_{t_{m-1}},u_{t_{m-1}})(W_{t_{m}}-W_{t_{m-1}})
\end{split}
\]
\[
\begin{split}
    \mathbb{E}\left[\sum_{n=1}^{N} \sigma^2(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})^2\right] 
    \\
    = &\sum_{n=1}^{N}\mathbb{E}\left[\sigma^2(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})^2\right] \\
    = &\sum_{n=1}^{N}\mathbb{E}\left[\sigma^2(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})({t_{n}}-{t_{n-1}})\right]
    \\
    \to &
    \mathbb{E} \left[\int_0^t\sigma^2(t_{s},X_{t_{s}},u_{t_{s}})\ds\right]
\end{split}
\]
On the other hand, since $m>n$, $W_{t_{m}}-W_{t_{m-1}}$ is independent of 
\[
\sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\sigma(t_{m-1},X_{t_{m-1}},u_{t_{m-1}})
\]
and 
\[
\begin{split}
    \mathbb{E}& \left[\sum_{1\le n<m\le N} \sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\sigma(t_{m-1},X_{t_{m-1}},u_{t_{m-1}})(W_{t_{m}}-W_{t_{m-1}})\right]\\
    &= 
    \sum_{1\le n<m\le N} \mathbb{E}\left[\sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\sigma(t_{m-1},X_{t_{m-1}},u_{t_{m-1}})\right]\underbrace{\mathbb{E}\left[W_{t_{m}}-W_{t_{m-1}}\right]}_{=0}=0
\end{split}
\]
Therefore, 
\[
\mathbb{E}\left[\left(\int_0^t\sigma(s,X_s,u_s)\dW_s\right)^2\right]=\mathbb{E}\left[\int_0^t\sigma^2(s,X_s,u_s)\ds\right]
\]
In other words;

{\centering{\fbox{%
    \parbox{0.95\textwidth}{%
    The noise term $\int_0^t\sigma(s,X_s,u_s)\dW_s$ has mean zero and variance (magnitude)
    \[
    \mathbb{E} \left[\int_0^t\sigma^2(s,X_s,u_s)\ds\right]
    \]
    provided that the above expectation is finite.
    }%
    }
}
}

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Itô's formula}
We heuristically deduced that 
{$W_{t_{n}}-W_{t_{n-1}}\sim O(\sqrt{t_{n}-t_{n-1}})$}. This has implications in the use of Taylor series similar to what we did in derivation of HJ equation from DPP in Section~\ref{sec:HJ}.
To clarify, Consider a function $f$ at its Taylor series about a point $x$.
\[
f(y) = f(x) + f^{\prime}(x) (y-x) + \frac12f^{\prime\prime}(x) (y-x)^2 + \cdots
\]
If we replace $y=x+\int_t^s b(r,X_r,u_r)\ds$, we obtain 
\[
\begin{split}
f(y)& = f(x) + f^{\prime}(x) \int_t^s b(r,X_r,u_r)\dr + \frac12f^{\prime\prime}(x) \left(\int_t^s b(r,X_r,u_r)\dr\right)^2 + \cdots\\
& \approx f(x) + f^{\prime}(x) b(t,X_t,u_t)(s-t) + \frac12f^{\prime\prime}(x) b^2(t,X_t,u_t)(s - t)^2 + \cdots\\
& \approx f(x) + f^{\prime}(x) b(t,X_t,u_t)(s-t) +O((\Delta t)^2)
\end{split}
\]
This is the simple chair rule:
\[
f(X_s)=f(X_t)+\int_t^s f^\prime(X_r)b(r,X_r,u_r)\dr
\]

When $y=x+\int_t^s \sigma(r,X_r,u_r)\dW_r$, 
\[
\begin{split}
f(y)& = f(x) + f^{\prime}(x)\int_t^s \sigma(r,X_r,u_r)\dW_r + \frac12f^{\prime\prime}(x) \left(\int_t^s \sigma(r,X_r,u_r)\dr\right)^2 + \cdots\\
& \approx f(x) + f^{\prime}(x) \sigma(r,X_r,u_r)(W_s-W_r) + \frac12f^{\prime\prime}(x) \sigma^2(t,X_t,u_t)(W_s-W_r)^2 + \cdots\\
& \approx f(x) + f^{\prime}(x) \sigma(t,X_t,u_t)(W_s-W_r) + \frac12\sigma^{\prime\prime}(x) f^2(t,X_t,u_t)(s-r)+o(\Delta t)
\end{split}
\]
Now with the stochastic term, this is a more complicated chain rule:
\[
f(X_s)=f(X_t)+\int_t^s f^\prime(X_r)\sigma(r,X_r,u_r)\dW_r + {\color{orange}\frac12\int_t^s f^{\prime\prime}(X_r)\sigma^2(r,X_r,u_r)\dr}
\]

Having a complete form for stochastic differential equation $y=x+\int_t^s b(r,X_r,u_r)\dr+\int_t^s \sigma(r,X_r,u_r)\dW_r$, we obtain the \emph{Itô's formula};
\[
f(X_s)=f(X_t)+\int_t^s f^\prime(X_r)\sigma(r,X_r,u_r)\dW_r + {\color{orange}\int_t^s \left(f^\prime(X_r)b(r,X_r,u_r)+\frac12f^{\prime\prime}(X_r)\sigma^2(r,X_r,u_r)\right)\dr}
\]
When function $f$ depends on $(t,x)$, 
Itô formula is given by
\[
\begin{split}
    f(s,X_s)=f(t,X_t)+&\int_t^s f_x(r,X_r)\sigma(r,X_r,u_r)\dW_r\\
    +& {\int_t^s \left(f_t(r,X_r)+f_x(r,X_r)b(r,X_r,u_r)+\frac12f_{xx}(r,X_r)\sigma^2(r,X_r,u_r)\right)\dr}
\end{split}
\]
The above Itô's formula is for 1-dimensional state variable $x$. For higher dimensions, $f_x$ is the gradient vector and $f_{xx}$ is the Hessian matrix. Then, the Itô formula is written by using the dot product of vectors and Frobenius product of matrices.
\[
\begin{split}
    f(s,X_s)=f(t,X_t)+&\int_t^s f_x(r,X_r)\cdot \sigma(r,X_r,u_r)\dW_r\\
    +& {\int_t^s \left(f_t(r,X_r)+f_x(r,X_r)\cdot b(r,X_r,u_r)+\frac12f_{xx}(r,X_r)\cdot\sigma^{\top}\sigma(r,X_r,u_r)\right)\dr}
\end{split}
\]
In the above, $X$ is a $d$-dimensional stochastic process, $b$ is a $d$-dimensional vector field, $\sigma$ is a $d\times d$ matrix field, and $X$ is given by
\[
dX_t = b(t,X_t,u_t)\dt +\sigma(t,X_t,u_t)dW_t
\]
where $W$ is a $d$-dimensional Brownian motion. 
In the Itô's formula, $f_x(r,X_r)\cdot b(r,X_r,u_r)$ is the dot product of gradient and vector field $b$ and $f_{xx}\cdot\sigma^{\top}\sigma$ is the Frobenius product of Hessian  and matrix $\sigma^{\top}\sigma$ defined by 
\[
A\cdot B = \text{\normalfont Tr}[AB^\top]
\]
Frobenius product is simply dot product if you write matrices as $d^2$ vectors. If $A=[A_{ij}]_{i,j=1,...,d}$ and $B=[B_{ij}]_{i,j=1,...,d}$, then 
$A\cdot B = \sum_{i=1}^d\sum_{j=1}^d A_{i,j}B_{i,j}$, the sum of product of elements with the same indices.
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stochastic control}
In this section, we consider the basic definitions of  stochastic control.
\subsection{Controlled SDE}
The state process is a controlled SDE given by
\begin{equation}\label{eqn:state_process}
	\begin{cases}
	    {\dX}_t =b(t,X_t,u_t){{\dt}}+\sigma(t,X_t,u_t)\dW_t\\
     X_0=x\in\mathbb{R}^d
	\end{cases}
\end{equation}
The objective function is similar to deterministic case; there is a running cost and a terminal cost. However, it takes the expected value of the randomness.
\begin{equation}
J(x;u)=\mathbb{E}\left[\int_0^T C(s,X_s^u,u_s){{\ds}}+g(X_T^u)\right].
\end{equation}
We also assume there is a set of admissible controls, $\mathcal{U}$. The goal is
\[
\inf_{u\in\mathcal{U}}J(x;u)
\]
Since the problem setting is stochastic, control variables are also stochastic processes. We can choose the control process to behaves randomly following the randomness in the state variable and/or the Wiener process.

Similar to deterministic case, we have myopic solutions. 
\begin{eg}
Let $\alpha_t$ be a given stochastic process, e.g. a Wiener process. Consider the stochastic control problem
    \begin{equation}\label{simple_stoch_control}
        \inf_{u\in\mathcal{U}}\mathbb{E}\bigg[\int_0^T\Big(x_t^2 - \alpha_t x_t\Big) {\dt}\bigg]
    \end{equation}
with controlled state process is given by
\begin{equation}\label{dynamic_SDE_x}
    {\dX}_t =  u_t {{\dt}}+\sigma dW_t, ~~ x_0=x
\end{equation}
If we can choose $u$ such that $X_t=\alpha_t/2$ or $x+\int_0^tu_s\ds+\sigma W_t=\alpha_t/2$. Under this condition, we have a myopic solution. However, this is very restrictive, even more restrictive to the deterministic case. For example, let assume $\alpha_t$ is deterministic and differentiable. Then, for $x+\int_0^tu_s\ds+\sigma W_t=\alpha_t/2$ to hold, $\int_0^tu_s\ds$ and $\sigma W_t$ should cancel each other. But, Brownian motion is never differentiable. This is a twist added by the stochastic term.
\end{eg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Admissibility}
It is easy to model stochastic control problems from real applications.  However, if not carefully done, the stochastic control problem becomes degenerate.

\begin{ex}[St. Petersburg game and doubling strategy]\label{eg:admissibility_paradox}
In this example, we show that if the collection of admissible controls are not determined properly, the control problem becomes trivially useless. Consider a game with consecutive rounds. At each round the player makes a bet and a coin is flipped. Upon receiving favorable outcome, the player gains as much as his bet. Otherwise, he loses as much as his bet. 
The doubling strategy suggests to double the bet for the next round if you lose this round. As a result, if the player's bet in the first round is $x$ and the first win happens at round $n$, he has lost $x(1+\cdots+2^{n-1})=x(2^{n}-1)$. In the $n$th round, he wins $x2^n$, which makes his total wealth $x$ more. This is an obvious arbitrage strategy. However, it requires the play to tolerate large amount of debt, i.e., $x(2^{n}-1)$. Note that even if the coin is largely biased against the player, the doubling strategy works. 

% 	Consider the wealth process as described by \eqref{eg:wealth} and assume that asset price $S$ is a martingale by setting $\mu=0$.
%  In this case, $X_t=X_0+\sigma\int_0^tu_s\dW_s$. Further, assume $X_0=0$.
%  Let's try to maximize expected value of wealth by applying an investment strategy $u$:
%  \begin{equation}\label{st.petersburg}
%      V = \sup_{u}\mathbb{E}[X_T],~~~\dX_t =  u_t\dW_t
%  \end{equation}
%  Is the stochastic integral above zero? Why?
% $M_t:=\int_0^t u_sdW_s$ is a local martingale.
%  Now, consider the strategy that chooses 
%  \[
%  u_t=
%  \begin{cases}
%      u & \max_{0\le t} X_t < M\\
%      0 & \max_{0\le t} X_t \ge M
%  \end{cases}
%  \]
%  where $u>0$ is an arbitrary positive number. If $X_t$ never hits $M$, the investment yields $X_T$.
 
% Note that for $X^u_t=\sigma u W_t$, probability of hitting $M$ before $T$ is
% \begin{equation}
%     \mathbb{P}\big(\max_{t\le T} X_t^u\ge M\big) =  \mathbb{P}\bigg(\max_{t\le T} W_t\ge \frac{M}{\sigma u}\bigg) =  2\mathbb{P}\bigg(W_T\ge \frac{M}{\sigma u}\bigg)
% \end{equation}
% The last equality is coming from the Schwartz reflection principle for Brownian motion. If we send $u\to\infty$ and $M\to\infty$  such that $\frac{M}{u}\to0$, we obtain $\mathbb{P}(B_T\ge \frac{M}{\sigma u})\to \frac12$. 

% On the other hand, from the strategy described above, we have 
% \begin{equation}
%     \mathbb{E}[X_T] = \mathbb{E}[M\mathds{1}_{\{\max_{t\le T} X_t\ge M\}}]+  \mathbb{E}[X_T\mathds{1}_{\{\max_{t\le T} X_t< M\}}]= 2M\mathbb{P}\bigg(W_T\ge \frac{M}{\sigma u}\bigg)+\mathbb{E}[X_T\mathds{1}_{\{\max_{t\le T} X_t< M\}}]
% \end{equation}
% % where $\tau=\inf\{t:X_t\ge M\}\wedge T$
% Now, answer the following questions:
% \begin{enumerate}
%     \item Is it true that $\mathbb{E}[X_t^u1_{\{\max_{t\le T} X_t^u< M\}}]\to 0 $ as $u\to\infty$ and $M\to\infty$  such that $\frac{M}{u}\to0$?
%     \item What is the limit of $2M\mathbb{P}(W_T\ge \frac{M}{\sigma u})$? $V=\infty$?
%     \item If the stochastic integral in \eqref{st.petersburg} is zero, then $V=0$, why did we also get $V=\infty$?
% \end{enumerate}
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dynamic programming principle and Hamilton-Jacobi-Bellman equations}
\subsection{Value function}
Fo a stochastic control problem
\begin{equation}\label{problem:_stoch_generic}
\inf_{u\in\mathcal{U}}J(u),~~~~J(u):=\mathbb{E}\left[\int_0^T C(t,X_t,u_t){{\dt}}+g(X_T)\right].
\end{equation}
where 
\[
{\dX}_t =b(t,X_t,u_t){{\dt}}+\sigma(t,X_t,u_t)\dW_t
\]
We define the value function as below

\begin{defn}[Value function]
    The value function of the control problem is defined by
\begin{equation}
V(t,\omega)=\inf_{u\in\mathcal{U}_{t,T}}\mathbb{E}\left[\int_{t}^{T}C(s,X_s^u,u_s){{\ds}}+g(X_T^u)\Big|\mathcal{F}_t\right]
\end{equation}
$\mathcal{U}_{t,T}$ is the set of admissible controls on $[t,T]$ and the condition, $\mathcal{F}_t$, inside the conditional expectation represents the path of $X$ before or at time $t$.
\end{defn}
There is a well-celebrated result, independently published by \cite{haussmann1986existence} and \cite{el1987compactification}, that shows we can restrict the set of controls to a simpler one. 
% \cite{Krylov1973selection}.
\begin{thm}
    For any well-defined (proper admissibility condition and finite value function) stochastic control problem, the value function does not change if we reduce the set of controls to \textbf{Markovian controls}, i.e., $u_t:=\phi(t,X_t)$ for some function $\phi(t,X_t)$. 
\end{thm}
Markovian controls are also called feedback controls, In deterministic control, the existence of optimal feedback control controls is a straightforward fact. 
\begin{coro}\label{coro:Markov_value_function}
   The value function takes the form $V(t,\omega)= V(t,X_t)$. Additionally, 
\begin{equation}\label{eqn:value_function}
\begin{split}
    V(t,x) &=\inf_{\phi(t,x)\in\mathcal{U}^{m}_{t,T}}\mathbb{E}\left[\int_{t}^{T}C(s,X_s^u,u_s){{\ds}}+g(X_T^u)\Big|{\color{blue}X_t=x}\right]
\end{split}
\end{equation}
where $\mathcal{U}^{m}_{t,T}$ is he set of all admissible Markovian controls $u_t=\phi(t,X_t)$.
\end{coro}
In other words, from the path of $X$ until time $t$, $\mathcal{F}_t$, all that matters is the end point $X_t$ and all the previous values that $X_s$ took for $s<t$ are irrelevant. 

It is worth mentioning there are three type of controls:
\begin{enumerate}
    \item Open-loop controls which do not care about the current state of the system. In our case, an open-loop control is a deterministic control. In Example~\ref{simple_control}, if $\alpha_t$ is differentiable, then $u_t=\frac{\alpha_t^{\prime}}{2}$ is a closed-loop control. 
    \item A closed-loop or feedback control is a control that depends on the path of the state process, i.e., $u_t = \phi(t,\mathcal{F}_{t})$.
    \item A Markovian control is a special case of feedback control that only depends on the latest value of the state variable, $u_t=\phi(t,X_t)$. Non-Markovian controls are only relevant if the cost function or the terminal cost depend on the path of $X$ or $X$ is not a controlled Markov process.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dynamic programming principle (DPP) for stochastic control}
Dynamic programming principle is not as simple as the deterministic case, Theorem~\ref{thm:stoch_dpp_no_stopping_deterministic}. In deterministic case, we can break a control on $[t,T]$ into one on $[t,s]$ and another on $[s,T]$. Conversely, we can patch two controls on $[t,s]$ and $[s,T]$ to create one from $[t,T]$. In stochastic control, the splitting and patching does not work. Specifically, patching can create non-measurable control processes, which makes it mathematically inaccurate. However, one can use a measurable selection theorem to patch properly and avoid the issue. Other methods used quantization  to solve the issue by avoid measurable selection theorems. For more information, see \cite{Touzi12} and references therein.


\begin{thm}\label{thm:stoch_dpp_no_stopping}
    If the value function is continuous and have linear growth, then for $t<s<T$, we have 
    \begin{equation}\label{eqn:stoch_dpp_no_stopping}
        V(t,x)= \inf_{u\in\mathcal{U}_{t,s}}\mathbb{E}\Big[\int_t^s C(r,X_r,u_r){\dr} + V(s,X_s)\Big],~~X_t=x
    \end{equation}
\end{thm}
To obtain an Hamilton-Jacobi-Bellman (HJB) equation from DPP, we require to take $s-t$ very small and expand $V(s,X_s)$ about $(t,x)$.
\[
\begin{split}
    V(s,X_s)  = & V(t,x) + V_t(t,x)(s-t) +V_x(t,x)(X_s-x) + \frac12 V_{xx}(t,x)(X_s-x)^2 + \cdots\\
     =&V(t,x) + V_t(t,x)(s-t) \\
     &+V_x(t,x)\left(\int_t^s b(r,X_r,u_r)\dr +\int_t^s \sigma(r,X_r,u_r)\dW_r\right) \\
     &+ \frac12 V_{xx}(t,x)\left(\int_t^s b(r,X_r,u_r)\dr +\int_t^s\sigma(r,X_r,u_r)\dW_r\right)^2 + \cdots
\end{split}
\]
In the expansion
\[
\begin{split}
    \left(\int_t^s  b(r,X_r,u_r)\dr +\int_t^s  \sigma(r,X_r,u_r)\dW_r\right)^2 = &\left(\int_t^s  b(r,X_r,u_r)\dr\right)^2 \\
    &+2\int_t^s  b(r,X_r,u_r)\dr\int_t^s  \sigma(r,X_r,u_r)\dW_r\\
    &+\left(\int_t^s  \sigma(r,X_r,u_r)\dW_r\right)^2
\end{split}
\]
$(\int_t^s  b(r,X_r,u_r)\dr)^2=O((\tau-t)^2)$, $\int_t^s  b(r,X_r,u_r)\dr\int_t^s  \sigma(r,X_r,u_r)\dW_r=O((\tau-t)^{3/2}$, and $(\int_t^s  \sigma(r,X_r,u_r)\dW_r)^2=O(tau-t)$. Therefore, the first two terms are high order terms and we can ignore them. 
\[
\begin{split}
    V(s,X_s)  = & V(t,x) + V_t(t,x)(s-t) +V_x(t,x)(X_s-x) + \frac12 V_{xx}(t,x)(X_s-x)^2 + \cdots\\
     =&V(t,x) + V_t(t,x)(s-t) \\
     &+V_x(t,x)\left(\int_t^s  b(r,X_r,u_r)\dr +\int_t^s  \sigma(r,X_r,u_r)\dW_r\right) \\
     &+ \frac12 V_{xx}(t,x)\left(\int_t^s  \sigma(r,X_r,u_r)\dW_r\right)^2 + \cdots
\end{split}
\]
In addition, 
\begin{align}
    \mathbb{E}\left[\int_t^s  \sigma(r,X_r,u_r)\dW_r\right]&=0\label{dpp:mean_zero}\\
    \mathbb{E}\left[\left(\int_t^s  \sigma(r,X_r,u_r)\dW_r\right)^2\right]&=\mathbb{E}\left[\int_t^s  \sigma^2(r,X_r,u_r)\dr\right]\label{dpp:variance}
\end{align}
Therefore,
\[
\begin{split}
    \mathbb{E}[V(s,X_s)]  = & V(t,x) + V_t(t,x)\mathbb{E}[s-t]+V_x(t,x)\mathbb{E}\left[\int_t^s  b(r,X_r,u_r)\dr\right] \\
     &+ \frac12 V_{xx}(t,x)\mathbb{E}\left[\int_t^s  \sigma^2(r,X_r,u_r)\dr\right] + \cdots
\end{split}
\]
Insert the Taylor expansion into DPP:
\[
\begin{split}
    0= \inf_{u\in\mathcal{U}_{t,s}}\mathbb{E}\Big[\int_t^s  C(r,X_r,u_r){\dr} + V_t(t,x)(s-t) +\int_t^s  b(r,X_r,u_r)\dr+\frac12 V_{xx}(t,x)\int_t^s  \sigma^2(r,X_r,u_r)\dr\Big]
\end{split}
\]
Take $\Delta t:=s-t$, divide by $\Delta t$, and send $\Delta t\to 0$, we get the HJB equation:
\[
    \begin{cases}
    0= V_t(t,x) + \inf_{u\in U}\left\{ C(t,x,u)  + V_x(t,x) b(t,x,u)+\frac12 V_{xx}(t,x) \sigma^2(t,x,u)\right\}\\
    V(T,x)=g(x)
\end{cases}
\]
Using the notation of Hamiltonian, we write
\begin{align}
     &\begin{cases}
    0= V_t(t,x) + H(t,x,V_x,V_{xx})\\
    V(T,x)=g(x)
\end{cases}\label{eqn:HJB}\\
&\hspace{0cm}H(t,x,p,\Gamma):=\inf_{u\in U}\left\{ C(t,x,u)  + p b(t,x,u)+\frac12 \Gamma \sigma^2(t,x,u)\right\}\label{eqn:Hamilton_1d}
\end{align}
where we added terminal cost as a boundary condition.
\begin{ex}\label{ex:merton}
    The wealth of in individual who splits his investment in the stock market and money market is given by
    \[
    \dd X_t = (\theta_t \mu + r(X_t-\theta_t))\dt + \theta_t \sigma \dW_t
    \]
    Here, $\mu\in\mathbb{R}$ is the rate of return on the stock, $r\ge0$ is the interest rate of money market,  and $\theta_t$ is the control variable that represents the amount invested in risky asset. 
    For a given utility function, $U$, write the HJB for 
    \[
    \sup_{\theta_t}\mathbb{E}[U(X_T)]
    \]
    To avoid doubling strategy, we define the set of all admissible investment strategies as all $\theta_t$ such that $X_t\ge0$ for all $t\ge0$; $\mathcal{U}=\{\theta_t:X_t\ge0~~\text{ for all }~~t\ge0~~\text{a.s.}\}$.
\end{ex}
\begin{ex}\label{ex:merton_w_consumption}
    In Exercise~\ref{ex:merton}, we allow the consumptions to be withdrawn from the investment at any time.
    \[
    \dd X_t = (\theta_t \mu + r(X_t-\theta_t)-c_t)\dt + \theta_t \sigma \dW_t
    \]
    Here, $\mu\in\mathbb{R}$ is the rate of return on the stock, $r\ge0$ is the interest rate of money market,  and $\theta_t$ is the control variable that represents the amount invested in risky asset. 
    For a given utility function, $U$, write the HJB for 
    \[
    \sup_{\theta_t,c_t\ge0}\mathbb{E}\left[\int_0^TU(c_t)\dt + U(X_T)\right]
    \]
    To avoid doubling strategy, we define the set of all admissible investment as in Exercise~\ref{ex:merton}.
\end{ex}
\begin{ex}\label{ex:merton_w_consumption_solution}
    In Exercises~\ref{ex:merton} and \ref{ex:merton_w_consumption}, assume that $U(x)=\frac{x^\gamma}{\gamma}$ for $gamma\in(0,1)$. Find a solution of the form $V(t,x)=f(t)U(x)$ for the HJB equation.
\end{ex}

If we consider higher dimensions, we should write Hamiltonian as  
\begin{equation}
H(t,x,p,\Gamma):=\inf_{u\in U}\left\{ C(t,x,u)  + p\cdot b(t,x,u)+\frac12 \Gamma\cdot(\sigma^{\top}\sigma)(t,x,u)\right\}
\end{equation}
\begin{ex}
The dynamics of portion of susceptible individuals and infected individuals in a population follows.
\begin{align*}
\dd{S}_t &= \left( \mu(1 - S_t) - \beta_t S_t I_t \right) \dt + G_1(S_t) \dW_t^{1}- G_2(S_t,I_t) \dW_t^{2} \\
\dd{I}_t &= \left( \beta_t S_t I_t - (\gamma_t + \mu)I_t \right) \dt + G_2(S_t,I_t)  \dW_t^{2} - G_3(I_t)  \dW_t^{3}
\end{align*}
where $\dW^1$, $\dW^2$, and $\dW^3$,  are independent Wiener processes, $\beta_t>0$ and $\gamma_t>0$ are control variables, and the diffusion coefficients are defined as:
\begin{align*}
G_1(s) &= \sqrt{\mu(1 + s)} \\
G_2(s,i) &= \sqrt{\beta s i} \\
G_3(i) &= \sqrt{(\gamma + \mu)i}
\end{align*}
Write the HJB equation for the stochastic control problem
\[
\inf_{\beta_t,\gamma_t}\mathbb{E}\left[\int_0^T (b-\beta_t)^2+\gamma_t^2 )\dt + I_T^2\right]
\]
\end{ex}
\begin{eg}\label{eg:LQ_drift}
    Consider the stochastic linear quadratic problem below:
    \begin{equation}
        \inf_u\mathbb{E}\Big[\int_0^T \Big(aX_t^2+bX_t +Au_t^2 + Bu_t\Big){\dt}+\alpha X_T^2+\beta X_T\Big]
    \end{equation}
    with
    \begin{equation}
        {\dX}_t=(\gamma X_t+\kappa u_t){{\dt}}+\sigma \dW_t
    \end{equation}
where $a,A, \alpha>0$ and $b,B,\beta,\gamma$ and $\kappa$ are constants. The HJB is given by 
\begin{equation}
    \begin{cases}
        0={V}_{t}(t,x) +\frac{\sigma^2}{2}V_{xx}(t,x)+ \gamma x V_{x}(t,x) + a x^2 + bx +\inf_{u}\Big\{Au^2 +\big(B+\kappa V_{x}(t,x)\big)u \Big\}\\
        V(T,x)=\alpha x^2 + \beta x
    \end{cases}
\end{equation}
We can evaluate
$\inf_u\{Au^2 +\big(B+\kappa V_{x}(t,x)\big)u\} = -\frac{(B+\kappa V_{x}(t,x))^2}{4A}$ with infimum attained at $u =-\frac{B+\kappa V_{x}(t,x)}{2A}$.
\begin{equation}
    \begin{cases}
        0={V}_{t}(t,x) +\frac{\sigma^2}{2}V_{xx}(t,x)+ \gamma x V_{x}(t,x) + a x^2 + bx -\frac{(B+\kappa V_{x}(t,x))^2}{4A}\\
        V(T,x)=\alpha x^2 + \beta x
    \end{cases}
\end{equation}
We anticipate that $V(t,x)=f(t)x^2+h(t)x+\ell(t)$, which is a separation of variable technique. If we plug in $V(t,x)$  into the HJB, we obtain
\begin{equation}
    \begin{cases}
       f^\prime(t) = -2\gamma f(t) + \frac{\kappa^2}{A}f^{2}(t) -a& f(T)=\alpha\\
       h^{\prime}(t)= (\frac{\kappa^2}{A}f(t)-\gamma)h(t)+\frac{B\kappa}{A}f(t)-b& h(T)=\beta\\
       \ell^{\prime}(t)=\frac{B^2}{4A}+\frac{B\kappa}{2A}h(t)+\frac{\kappa^2}{4A}h^2(t)-\sigma^2f(t)&\ell(T)=0
    \end{cases}
\end{equation}
Note that the above system of ODEs is fully solvable. The first ODE is Riccati equation.
We shall show in Section~\ref{sec:verification} that the solution of the PDE is indeed the value function of the linear quadratic optimal control problem.
\end{eg}
\subsection{DPP with stopping time}
The DPP in Theorem~\ref{thm:stoch_dpp_no_stopping} is a special case of the general DPP for stochastic control problems. To explain the general DPP, we need the notion of \emph{stopping time} in a probabilistic setting. When we deal with stochastic processes, the occurrence time of an event is random. For example, let $\tau$ be the first time a 2-$d$ Wiener process exits a given circle.  For two different sample paths, the exit time is different.
A stopping time is a random time that can be determined whether it has happen or not based on the observed events. See Figure~\ref{fig:exit_time}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\linewidth]{Control_lecture_notes/Figs/bm_2d_hitting_time.png}
    \caption{Two paths of random walks started at the center of the disk. One exits after $42188$ steps  and the other after $77374$ steps.}
    \label{fig:exit_time}
\end{figure}
\begin{defn}
Let $\mathcal{F}_t$ represents all events that are revealed at or before time $t$. The a random variable $\tau:\Omega\to[0,\infty)$ is called a stopping time if for all $t\ge0$, the event $\{\tau\le t\}$ belong to $\mathcal{F}_t$.
    In simple words, a stopping time cannot depend on some secret information that are not observed. 
\end{defn}
The DPP with stopping time is presented next.
\begin{thm}\label{thm:stoch_dpp_stopping}
    If the value function is continuous and have linear growth, then for any stopping time $\tau$ such that $t<\tau<T$, we have 
    \begin{equation}\label{eqn:stoch_dpp_stopping}
        V(t,x)= \inf_{u\in\mathcal{U}_{t,\tau}}\mathbb{E}\Big[\int_t^\tau C(r,X_r,u_r){\dr} + V(\tau,X_\tau)\Big],~~X_t=x
    \end{equation}
\end{thm}
\subsection{Why is stopping time in DPP important?}
In Section~\ref{sec:toknow_Wiener}, we show that for the Itô sum
\[
\int_0^t \sigma(s,X_s,u_s)\dW_s = \sum_{n=1}^{N} \sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}}) 
\]
we have
\[
\mathbb{E}\left[\int_0^t \sigma(s,X_s,u_s)\dW_s\right]=0
\]
However, to show this is true for a general stochastic integral, defined by the limit of Itô sums, i.e., 
\begin{equation*}
\int_0^t \sigma(s,X_s,u_s)\dW_s = \lim_{|t_n-t_{n-1}|\to0}\sum_{n=1}^{N} \sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})    
\end{equation*}
we require to be able to commutate the limit and expected value: 
\[
\begin{split}
    \mathbb{E}\bigg[&\lim_{|t_n-t_{n-1}|\to0}\sum_{n=1}^{N} \sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}}) \bigg]\\
    &=\lim_{|t_n-t_{n-1}|\to0}\sum_{n=1}^{N} \mathbb{E}\Big[\sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\Big]=0
\end{split}
\]
This commutation of limit and expected value is not a straightforward operation. In fact, there are limit theorems, e.g., Legesgue dominated convergence, monotone convergence, and Fatou's lemma, that come with restrictive conditions for such commutations.
For example, if 
\begin{equation}\label{cond:square_integrability}
    \mathbb{E}\left[\int_0^t \sigma^2(s,X_s,u_s)\ds\right]<\infty
\end{equation}
then the commutation is possible and 
\[
\begin{split}
\mathbb{E}\bigg[&\int_0^t \sigma(s,X_s,u_s)\dW_s\bigg]\\
    &=\mathbb{E}\bigg[\lim_{|t_n-t_{n-1}|\to0}\sum_{n=1}^{N} \sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}}) \bigg]\\
    &=\lim_{|t_n-t_{n-1}|\to0}\sum_{n=1}^{N} \mathbb{E}\Big[\sigma(t_{n-1},X_{t_{n-1}},u_{t_{n-1}})(W_{t_{n}}-W_{t_{n-1}})\Big]=0
\end{split}
\]
Otherwise, one cannot show that the stochastic integral is mean-zero. In such cases, the mean of stochastic integral does not even exist. For example, in the derivation of HJB equation for stochastic control problems, specifically in \eqref{dpp:mean_zero}, there is no guarantee that the expectation of the stochastic integral is zero. $\sigma(r,X_r,u_r)$ is a general function and $u_r$ is arbitrary. Therefore, we can not guaranty the square integrability  in \eqref{cond:square_integrability}. However, if we write the DPP with a specific stopping time, then the issue is resolved. In \eqref{eqn:stoch_dpp_stopping}. we can choose the stopping time to be
\begin{equation}\label{eqn:tau_s}
    \tau=\min\Big\{s,\inf\{r>t: |\sigma(r,X_r,u_r)|\ge M \}\Big\}
\end{equation}
for some $M>0$. Therefore, we are sure that  $\sigma(r,X_r,u_r)$ is bounded inside
\[
\int_0^\tau \sigma(r,X_r,u_r)\dW_r
\]
As a result, 
\[
\mathbb{E}\bigg[\int_0^\tau \sigma^2(r,X_r,u_r)\ds\big]<\infty
\]
and the stochastic integral vanishes. 
\begin{rem}
    The above discussion is not completely rigorous. In \eqref{eqn:tau_s}, we cannot assume that $u_r$ behaves continuously. The control may simply jump to a high value and invalidate our argument. Usually, in many cases, we either assume $u\in U$ and $U$ is bounded, or we assume $|\sigma(r,X_r,u_r)|\le C(1+|X_r|)$ and define 
    \[
    \tau=\min\Big\{s,\inf\{r>t: |X_r|\ge M \}\Big\}
    \]
\end{rem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Verification step}
In Exercise~\ref{ex:merton_w_consumption_solution} and Example~\ref{eg:LQ_drift}, we found closed-form solutions for the HJB. It is important to remember from Example~\ref{eg:eikonal_2} that the value function is only a viscosity solution of the HJB and finding classical solutions for the HJB is a delicacy. Nevertheless, when we have a classical solution for the HJB, there is still one more step before we can declare a victory; verification results show that the classical solution for the HJB is indeed the value function, under relatively straightforward assumptions, and it provides a way to derive an optimal control from the value function. Usually, for each control problem, the verification is slightly different from another. However, we can provide a generic verification theorem with sacrificing a little bit of rigor.
\begin{thm}\label{thm:verification}
    Let $v\in\text{\normalfont C}^{1,2}$ be a classical solution of the HJB 
    \[
    \begin{cases}
    0= v_t(t,x) + \inf_{u\in U}\left\{ C(t,x,u)  + v_x(t,x) b(t,x,u)\cdot \nabla v(t,x)+\frac12 (\sigma^\top\sigma)(t,x,u)\cdot D^2v(t,x) \right\}\\
    v(T,x)=g(x)
\end{cases}
\]
Then, we have $v\ge V$, where $V$ is the value function of the control problem \eqref{problem:_stoch_generic} given by 
\begin{equation}
\begin{split}
    V(t,x) &=\inf_{u\in\mathcal{U}_{t,T}}\mathbb{E}\left[\int_{t}^{T}C(s,X_s^u,u_s){{\ds}}+g(X_T^u)\Big|{X_t=x}\right]
\end{split}
\end{equation}
Furthermore, if there exists a control $u^*\in\mathcal{U}$ such that
\[
u^*(t,x)\in\text{\normalfont argmin}_{u\in U}\left\{  C(t,x,u)  + v_x(t,x) b(t,x,u)\cdot \nabla v(t,x)+\frac12 (\sigma^\top\sigma)(t,x,u)\cdot D^2v(t,x)\right\}
\]
\end{thm}
Some of the technical assumption that we skip are there to make sure the expectations and the stochastic integrals in the proof are defined. That is what needs to be modified for each specific problem.
\begin{proof}
    For a fixed arbitrary control, $u\in\mathcal{U}$, we use Itô's formula to write
\[
\begin{split}
    \dd v(t,X_t)  = &\Big(v_t(t,X_t)+\frac12(\sigma^{\intercal}\sigma)(t,X_t,u_t)\cdot D^2 v(t,X_t) + b(t,X_t,u_t)\cdot \nabla v(t,X_t)\Big)\dt \\
    &\hspace{2cm} +  \nabla v(t,X_t)\cdot \sigma(t,X_t,u_t)\dW_t
\end{split}
\]
Or equivalently,
\[
\begin{split}
    v(t,X_t)  = & g(X_T) -\int_0^T\!\!\!\!\!\Big(\big(v_t(s,X_s)+\frac12(\sigma^{\intercal}\sigma)(s,X_s,u_s)\cdot D^2 v(s,X_s) + b(s,X_s,u_s)\cdot \nabla v(s,X_s)\big)\ds \\
    &\hspace{2cm} +  \nabla v(s,X_s)\cdot \sigma(s,X_s,u_s)\dW_s\Big)
\end{split}
\]
By the HJB, we have 
\[
\begin{split}
    v_t(s,X_s)+&C(s,X_s,u)   +\frac12(\sigma^{\intercal}\sigma)(s,X_s,u_s)\cdot D^2 v(s,X_s) + b(s,X_s,u_s)\cdot \nabla v(s,X_s) \\
    &\ge v_t(s,X_s) + \inf_{u\in U}\left\{ C(s,X_s,u)  + b(s,X_s,u)\cdot v_x(s,X_s) +\frac12 v_{xx}(t,x) \sigma^2(t,x,u)\right\}=0
\end{split}
\]
Therefore,
\[
\begin{split}
    v_t(s,X_s)+&\frac12(\sigma^{\intercal}\sigma)(s,X_s,u_s)\cdot D^2 v(s,X_s) + b(s,X_s,u_s)\cdot \nabla v(s,X_s)\ge -C(s,X_s,u)   
\end{split}
\]
\[
\begin{split}
    v(t,X_t) \le~  & ~g(X_T) +\int_t^T\Big(C(s,X_s,u_s)\ds  +  \nabla v(s,X_s)\cdot \sigma(s,X_s,u_s)\dW_s\Big)
\end{split}
\]
Taking expectation, we have 
\[
\begin{split}
    v(t,x) \le ~ & ~\mathbb{E}_t\left[ g(X_T) +\int_t^T C(s,X_s,u_s)\ds \Big|X_t=x\right]
\end{split}
\]
Taking infimum over all $u\in\mathcal{U}$, we obtain
\[
v(t,x) \le  \inf_{u\in\mathcal{U}}\mathbb{E}_t\left[ g(X_T) +\int_t^T C(s,X_s,u_s)\ds\Big| X_t=x\right]=:V(t,x)
\]
By repeating the above argument for $u^*(t,x)$ given in the assertion of the theorem, all the inequalities turn to equalities and,  therefore, $v(t,x)=V(t,x)$.
\end{proof}
\begin{eg}
    In Exercise~\ref{ex:merton}, assume that $U(x)=\frac{x^\gamma}{\gamma}$ for $\gamma\in(0,1)$. Find a solution of the form $v(t,x):=f(t)U(x)$ for the HJB equation. We now perform a verification step on $v(t,x)$. The HJB for Exercises~\ref{ex:merton} is given by
        \begin{equation}
        \begin{cases}
            0={V}_{t} + rx V_{x} + \sup_{\theta}\Big\{
            \frac{\sigma^2}{2}V_{xx}\theta^2 +(\mu-r)  V_{x}\theta\Big\}\\
            V(T,x)=U(x)
        \end{cases}
    \end{equation}
    which is written in a simplified form by
    $0={V}_{t} + rx V_{x}-\frac{(\mu-r)^2 V_{x}^2}{2\sigma^2V_{xx}}$. % Note that $f(t)=\exp\left(A(T-t)\right)$ where $A=r-\tfrac{(\mu-r)^2\gamma}{2\sigma^2(1-\gamma)}$.
    Note that 
    \[
    \theta^*(t,x)=\text{\normalfont argmax}\Big\{
            \frac{\sigma^2}{2}V_{xx}\theta^2 +(\mu-r)  V_{x}\theta\Big\}=-\frac{(\mu-r)V_{x}}{\sigma^2 V_{xx}}=-\frac{(\mu-r)f(t) x^{\gamma-1}}{\sigma^2f(t)(\gamma-1)x^{\gamma-2}}=\frac{(\mu-r)}{\sigma^2(1-\gamma)}x
    \]
    According to Theorem~\ref{thm:verification}, we need to show that $\theta^*\in\mathcal{U}$, or, $\theta^*$ is an admissible control. Recall that $\mathcal{U}=\{\theta_t:X_t\ge0~~\text{ for all }~~t~~\text{a.s.}\}$. Therefore, we need to show that $X^*_t\ge0$ for all $t\ge0$ a.s., where
    \[
    \dd X^*_t = (\theta^*(t,X^*_t) \mu + r(X^*_t-\theta^*(t,X^*_t))\dt + \theta^*(t,X^*_t) \sigma \dW_t
    \]
    For simplicity, we set $A=\frac{(\mu-r)}{\sigma^2(1-\gamma)}$. Then, 
    \[
    \dd X^*_t = (A(\mu-r)   + r)X^*_t\dt + A X^*_t\sigma \dW_t
    \]
    Therefore, $X^*_t$ is a geometric Brownian motion and is given by
    \[
    X^*_t=X^*_0\exp\left(\left(A(\mu-r)   + r - \tfrac{A^2}{2}\right) t + A W_t \right)\ge0
    \]
    which provides the desired result.
\end{eg}
\begin{ex}
    Establish verification result for Exercise~\ref{ex:merton_w_consumption}.
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Martingale approach}\label{sec:martingale}
The martingale principle for optimal control (\cite{DV73}) is another verification result which does not require the differentiability of the value function.
\begin{thm}\label{thm:martingale_verification}
	Assume that there exists a function $v(t,x)$ such that for all $u\in\mathcal{A}$ the process $\{Y^u_t\}_{t\ge0}$
	\begin{equation}
		Y^u_t:=v(t,X_t)+\int_0^tC(s,X_s,u_s){{\ds}}
	\end{equation}
	is a super martingale and that for some $u^*\in\mathcal{A}$, $\{Y^{u^*}_t\}_{t\ge0}$ is a martingale. Then, $u^*$ is an optimal control and the value function is equal to $v(t,x)$.
\end{thm}
\begin{proof}
By the supermartingale property, we have
\begin{equation}
Y^u_t\le\mathbb{E}[Y^u_T|\mathcal{F}_t]=\mathbb{E}\Big[g(X_T)+\int_t^TC(s,X_s,u_s){{\ds}}\Big|\mathcal{F}_t]\Big]+\int_0^tC(s,X_s,u_s){{\ds}}
\end{equation}
Thus, 
\begin{equation}
v(t,x)=Y^u_t-\int_0^tC(s,X_s,u_s){{\ds}}\le\mathbb{E}[Y^u_T|\mathcal{F}_t]=\mathbb{E}\Big[g(X_T)+\int_t^TC(s,X_s,u_s){{\ds}}\Big|\mathcal{F}_t]\Big],
\end{equation}
for all $u\in\mathcal{A}_{t,T}$. Given $X_t=x$, we obtain
\begin{equation}
v(t,x)\le\sup_{u\in\mathcal{A}_{t,T}}\mathbb{E}\Big[g(X_T)+\int_t^TC(s,X_s,u_s){{\ds}}\Big|X_t=x]\Big]=V(t,x).
\end{equation}
If for some $u^*$, $Y^{u^*}$ is a martingale, then in all the above, the inequality turns into equality and we have $v(t,x)=V(t,x)$.
\end{proof}
The above theorem can also be regarded as a verification without regularity condition. The function $v(t,x)$ in the martingale approach is the value function of the optimal control problem. The following example shows the use of this theorem.
\begin{eg}\label{eg:martingle_consumption}
In Example~\ref{ex:merton_w_consumption} with $U(x)=\frac{x^\gamma}{\gamma}$ for $\gamma\in(0,1)$, %e^{-\gamma (T-t)}(T-t+e^{A/\alpha t})^{\alpha}e^{-AT}
recall that a solution to the HJB is given by $v(t,x)=f(t)U(x)$. Then, 
	\begin{equation}
		Y^{\theta,c}_t=v(t,X_t)+\int_0^te^{-\gamma s}U(c_s){{\ds}}.
	\end{equation}
	By It\^o formula, we have 
	\begin{equation}
	\begin{split}
				\dd Y_t^{\theta,c}=&\Big(v_t + \big(\theta_t(\mu-r)+rX_t-c_t\big) V_{x}+ 
\frac12\sigma^2\theta_t^2 V_{xx}+e^{-\gamma t}U(c_t)\Big){{\dt}}\\
		&+\sigma\theta_t V_{x} \dW_t. 
	\end{split}
	\end{equation}
	By the supremum in the HJB, one can see that 
	\begin{equation}
		{v}_{t}+ \big(\theta_t(\mu-r)+rX_t-c_t\big) v_{x}
		+\frac12\sigma^2\theta_t^2v_{xx}+e^{-\gamma t}U(c_t)\le0,~~\mathbb{P}\textrm{-a.s.}
	\end{equation}
	for all values of  $\theta$ and $c$. In addition, for $c^*_t(x)=f^{-\frac{1}{(1-\gamma)}}(t)x$ and $\theta^*_t(x)=\frac{(\mu-r)\gamma}{\sigma^2(1-\gamma)}x$, 
	\begin{equation}
		{V}_{t}+ \big(\theta_t(\mu-r)+rX_t-c_t\big) V_{x}
		+\frac12\sigma^2\theta_t^2\partial_{xx} V+e^{-\gamma t}U(c_t)=0,~~\mathbb{P}\textrm{-a.s.}
	\end{equation}
It remains to show that $c^*$ and $\theta^*$ are admissible Markov controls, i.e., to show that 
	\begin{equation}
		dX^*_t=\Big(\theta_t(X^*_t)\big((\mu-r) {{\dt}}+\sigma \dW_t\big)-rX^*_t{{\dt}}\Big)-c^*_t(X^*_t){{\dt}},
	\end{equation}
	has a non-negative solution $X^*$. We leave the details as an exercise. 
\end{eg}
\begin{ex}
   In the above example,  show that $X^*_t\ge0$ a.s..
\end{ex}



\section{Viscosity solutions to the HJB equations}

In the previous section, we showed that if the value function is in $C^{1,2}$, then it satisfies \eqref{eqn:HJB}. Therefore, HJB can be used to find the value function. If we know that the HJB has a unique $C^{1,2}$ solution in a suitable class of function which includes the value function, we are done. However, it is not always easy to obtain existence and uniqueness results for nonlinear PDEs, which HJB equations are. 
\begin{eg}
Recall from Exercise~\ref{ex:eikonal} the fastest exit problem:
\begin{equation}
    \inf_{|u_t|\le 1}\int_0^\infty 1_{\{X^u_t\in O\}} dt
\end{equation}
with ${\dX}_t^u=u_t dt$.
   The HJB for this problem is $0=\inf_{|u|\le 1}\{u\cdot \nabla V(x)\}+1_{\{x\in O\}}$, which is equivalent to the following boundary value problem
   \begin{equation}
   \begin{cases}
       0=-|\nabla V(x)|+1& x\in O\\
       0=V(x)& x\in\partial O
   \end{cases}
   \end{equation}
The optimal feedback (closed-loop) exit strategy is give by $u^*_t(x)=-\nabla V(x)$, which is equivalent to gradient descent.
   
When $O=[-1,1]\subset\mathbb{R}$, the equation is $|V^\prime(x)|=1$. On the other hand, this one-dimensional simplification has an obvious answer: run as fast as you can to the nearest exit point. This yields $V(x)=1-|x|$. The value function satisfies the HJB except at $x=0$, which is the sole point with two optimal exit strategies.

Assume that a robot only knows how to solve HJB equations and it ignores finite number of point of irregularities of the solution. For the robot the function $v(x)$ given below is as a good of a solution as $V(x)=1=|x|$. 
\begin{equation}
    v(x)=
    \begin{cases}
        1-|x| & \frac12\le |x| \le 1\\
        |x| & |x|\le \frac12
    \end{cases}
\end{equation}
However, if the robot comes of with $v$ instead of $V$, it suggests to move towards $0$ for $|x|\le \frac12$, which is obviously incorrect. 
\end{eg}
The above example questions the validity of HJB approach. There are two ways to address the validity issue for the solution of HJB equations. One is the notion of viscosity solution, which singles out $V(x)=1-|x|$, as the unique viscosity solution of the HJB. For more information of the viscosity solution approach, see \cite{CIL92}. There are simple criteria that the robot can check to see whether his approximate solution to the HJB equation converges to the unique viscosity solution. See for example, \cite{Barles-Souganidis91}. However, here we closed this discussion by giving a glimpse of what it means to be a viscosity solution in the following example.
\begin{eg}
Consider the exit time problem with noise:
    \begin{equation}
        \inf_{|u_t|\le 1}\mathbb{E}\Big[\int_0^\infty 1_{\{X^u_t\in O\}} dt\Big]
    \end{equation}
with    ${\dX}_t^u=u_t{\dt}+\epsilon\dW_t$.
The HJB is given by 
$0=\frac{\epsilon^2}{2}V^{\prime\prime}(x)+\inf_{|u|\le 1}\{u\cdot \nabla V(x)\}+1_{\{x\in O\}}$\footnote{My best guess is that the term viscosity comes a fluid mechanics term for the second derivative $\frac{\epsilon^2}{2}V^{\prime\prime}$.}, which is equivalent to the following boundary value problem
   \begin{equation}
   \begin{cases}
       0=\frac{\epsilon^2}{2}V^{\prime\prime}(x)-|\nabla V(x)|+1& x\in O\\
       0=V(x)& x\in\partial O
   \end{cases}
   \end{equation}
Back to the simple case where $O=[-1,1]$, there exists a unique bounded solution, $V^{\epsilon}(x)$, in $C^2$. One can find this solution in closed form and verify that as $\epsilon\to 0$, $V^{\epsilon}(x)\to 1-|x|$. This clarifies why $v(x)$ is not a solution.
\end{eg}
\begin{ex}
     In the previous exercise, show that as $\epsilon\to 0$, $V^{\epsilon}(x)\to 1-|x|$.  
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%
Before moving to the notion of viscosity solutions, we point out an important remark about partial differential equations (PDE) such as the HJBs. The solution to the simplest equations may be not exists. For instance, the backward heat equation given by
\begin{equation}
\begin{cases}
0={V}_{t}+\frac{\sigma^2}{2}\Delta V-rV+f(t,x)\\
V(0,x)=g(x)
\end{cases}
\end{equation}
does not have any solution unless $f$ and $g$ have sufficient regularity. However, the forward (regular) heat equation
\begin{equation}
\begin{cases}
0={V}_{t}+\frac{\sigma^2}{2}\Delta V-rV+f(t,x)\\
V(T,x)=g(x)
\end{cases}
\end{equation}
has a classical solution regardless of the choice of $g$ and $f$. The forward heat equation is a parabolic equation while the backward equation is not. Roughly speaking, in a parabolic equation, ${V}_{t}$ and $\frac{\sigma^2}{2}\Delta V$ show up with the same sign. In the HJB equation \eqref{eqn:HJB}, the coefficient of ${V}_{t}$ and $\frac{\sigma^2}{2}\Delta V$, $1$ and $\frac{\sigma^2}{2}$ respectively, have the same sign. More rigorously, a nonlinear equation of the form 
\begin{equation}\label{eqn:nonlinear_parabolic}
\begin{cases}
		0=\partial_{t}V(t,x)+F\big(t,x,V(t,x),\nabla V(t,x),D^2V(t,x)\big)\\
		V(T,x)=g(x)
\end{cases}
\end{equation}
is parabolic if the nonlinearity, $F(t,x,\varrho,\Pi,\Gamma)$, is nondecreasing in the component $\Gamma$, which represents the Hessian matrix in the order induced by nonnegative-definite matrices. I.e., if for two matrices $\Gamma_1$ and $\Gamma_2$, $\Gamma_2-\Gamma_1$ is a nonnegative-definite matrix, then $F(t,x,\varrho,\Pi,\Gamma_2)\ge F(t,x,\varrho,\Pi,\Gamma_1)$.
One can check that 
\begin{equation}
F(t,x,\varrho,\Pi,\Gamma):=\sup_{u\in \mathbf{U}}\left\{C(t,x,u)+\mu(t,x,u)\Pi+\frac12{\sigma^2(t,x,u)}\Gamma -r(t,x,u)\varrho\right\}
\end{equation}
is nondecreasing in $\Gamma$.

\begin{eg}
Show that the HJB equation \eqref{eqn:HJB_consumption} is a parabolic equation.
\end{eg}

On the other hand, even an equation as simple as the heat equation may have multiple solution if we do not restrict the solution to a proper class of functions. For instance, if $g(x)$ and $f(t,x)$ have linear growth in $x$, then the heat equation has a unique solution in the class of function with linear growth, but not in the class of functions  with super exponential growth.

A linear partial differential equation such as the heat equation has classical solution. Nonlinear equations, however, do not necessary have a classical solution. A classical solution is a solution that is continuously differentiable as much as the equation requires. For instance, if in the heat equation above, $f(t,x)$ is a continuous function, then a solution $V(t,x)$ is a classical solution if  $V\in\mathrm{C}^{1,2}$. But, as seen in Exercise~\ref{ex:eikonal}, $V(x)=1-|x|$ is the value function that we expect to satisfy the HJB equation. However, it is not differentiable as $x=0$.

A suitable notion of weak solution for the HJB equations, which are nonlinear equations, is viscosity solutions. To understand the viscosity solutions, we first need to provide candidates for the weak derivatives of a function at the points of nondifferntiability. The first derivative can always be represented by the slope of a tangent plane.  Locally speaking, the tangent plane is a plane that hits the graph of the function at at least one point locally and keeps the graph on one side. For example, for the function$V(x)=1-|x|$ at point $x=0$, the line $y=1+mx$ is a tangent line by $x=0$ for $m\in[-1,1]$ (red line) but $y=1+mx$ for $m\notin[-1,1]$ is not (green line). See Figure~\ref{fig:tangent}. 
\begin{figure}[ht!]
\centering
\includegraphics{Figs/tangent}
\caption{Red line is an acceptable tangent to the graph, while the green line is not. The green line does not keep the graph on one side locally.}
\label{fig:tangent}
\end{figure}
Therefore, we have $\varphi(t,x)=a\cdot (x-x_0)+b (t-t_0)+V(t_0,x_0)$ such that $\varphi(t,x)-f(t,x)$ has a local extrema at $(t_0,x_0)$.

For the second derivative tangent planes are not sufficient, because the second derivatives of the tangent planes are always zero. Therefore, we need to appeal to the quadratic functions of the form $\varphi(t,x)=(x-x_0)\cdot  A(x-x_0)+ a\cdot (x-x_0)+b (t-t_0)+ V(t_0,x_0)$. Note that since we have first derivative on $t$, we only use first order term on $t$. If a function $\varphi(t,x)$ touches $V(t,x)$ at point $(t_0,x_0)$ from above (resp. below), i.e., $(t_0,x_0)$ is a local minimum (maximum) point for $\varphi-V$, then we call $(b,a,A)=(\partial_t\varphi(t_0,x_0),\nabla\varphi(t_0,x_0),D^2\varphi(t_0,x_0))$ a superderivative (resp. sub) of $V$ at point $(t_0,x_0)$. Such functions $\varphi(t,x)$ are called test functions. See Figure~\ref{fig:viscosity}.

A function $\underline{V}$ (resp. $\overline{V}$) is called a viscosity subsolution (resp. super) of \eqref{eqn:nonlinear_parabolic} if for any superderivative (resp. sub) $(b,a,A)$ at point $(t_0,x_0)$, we have 
\begin{equation}
\begin{cases}		0\ge b+F\big(t_0,x_0,\varphi(t_0,x_0),a,A\big), ~~~~~\textrm{(resp. $0\le $)}\\
		V(T,x)\le g(x) ~~~~~\textrm{(resp. $0\ge $)}
\end{cases}
\end{equation}


\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\textwidth]{Figs/viscosity}
\caption{The test functions that represent the sub and superderivatives of a function at the points of nondifferentiability as well as other points that the function is differentiable.}
\label{fig:viscosity}
\end{figure}
A function is called a viscosity solution if it is a sub and a super solution. 

\begin{rem}
The subsolution (resp. super) is named in accordance with submartingale (resp. super). If $U(t,x)\in\mathrm{C}^{1,2}$ is a subsolution (resp. super) to the equation 
\begin{equation}
		0=\partial_{t}V(t,x)+C(t,x)+\mu(t,x)\partial_{x}V(t,x)+\frac12{\sigma^2(t,x)}\partial_{xx}V(t,x),
\end{equation}
then $Y_t:=U(t,X_t)$ is a submartingale (resp. super) martingale, where
\begin{equation}
{\dX}_t=\mu(t,X_t){{\dt}}+\sigma(t,X_t)\dW_t.
\end{equation}
\end{rem}

For the first order HJB equation, finding the direction of the inequality for the subsolution or super solutions is not particularly obvious. It can be determined while we add a singular perturbation term $\eps B_t$ to the deterministic state process and send $\eps\to0$. For instance, in Exercise~\ref{ex:eikonal}, if we set the state process ${\dX}_t^u=u_t{{\dt}}+\eps \dW_t$, we obtain the HJB 
\begin{equation}\label{eqn:Eikonal_perturbed}
\begin{cases}
		0=1-|V^{\prime}(x)|+\frac{\eps^2}{2}V^{\prime\prime}(x)\\
		V(\pm1)=0
\end{cases}
\end{equation}
In this case, the subsolution (resp. super) can satisfies
\begin{equation}
\begin{cases}
		0\ge1-|V^{\prime}(x)|+\frac{\eps^2}{2}V^{\prime\prime}(x)~~~~\textrm{(resp. $0\le$)}\\
		V(\pm1)\le0~~~~\textrm{(resp. $0\ge$)}
\end{cases}
\end{equation}
By sending $\eps\to0$, we obtain the following inequalities for the eikonal equation, \eqref{eqn:Eikonal_perturbed}.
\begin{equation}\label{eqn:Eikonal_sub_super}
\begin{cases}
		0\ge1-|V^{\prime}(x)|~~~~\textrm{(resp. $0\le$)}\\
		V(\pm1)\le0~~~~\textrm{(resp. $0\ge$)}
\end{cases}
\end{equation}
We can easily check that the function $V(t,x)=1-|x|$ is a viscosity solution of the eikonal equation, \eqref{eqn:Eikonal_perturbed}. For example, at all points $x\neq0$, the sub and superderivatives are equal to $-\textrm{sgn}(x)$, which obviously satisfy the equation with equality. At $x=0$, we only have super derivatives and the set of subderivatives is empty set. This makes the supersolution property to hold obviously by the false antecedent. The set of superderivatives contains all $m\in[-1,1]$. Since $0\le1-|m|$, then $V(t,x)=1-|x|$ is also a subsolution. All the above arguments are valid since the boundary condition $V(t,\pm1)=0$ validates the sub and supersolution properties. 

On the other hand, the function $\tilde{V}=\min\{|x|,1-|x|\}$ only satisfies the subsolution property. 
\begin{ex}
Show that $\tilde{V}$ is a viscosity subsolution to the eikonal equation \eqref{eqn:Eikonal_sub_super}, but not a super solution. Explore the points $x=\pm\frac12$.
\end{ex}

The existence and uniqueness of the solution to the nonlinear parabolic equations in the class of functions with linear growth is studies in several papers. For instance, see \cite{CIL92}. For the HJB equations derived from the optimal control problems, the value function of the control problem is usually the viscosity solution of the HJB. The uniqueness is due to a technical lemma, Ishii's lemma. However, in most cases, if we manage to show that any viscosity solution is indeed a classical solution, $\mathrm{C}^{1,2}$, then the verification theorem, Theorem~\ref{thm:verification} shows that any classical solution is equal to the value function, and therefore, uniqueness is obtained.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Backward stochastic differential equations}
In this section, we introduce a tool related to stochastic optimal control, i.e., backward stochastic differential equations, BSDE henceforth. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Itô representation theorem and BSDE for linear PDEs}
To understand where a BSDE comes from, we present a result  from stochastic calculus. 
\begin{thm}[Itô's martingale representation theorem]\label{thm:mg_rep}
    Let $Y$ be a random variable dependent on the paths of a Wiener process $\{W_t\}$ and $M_t:=\mathbb{E}[Y|\mathcal{F}_t]$ be the conditional expectation of $Y$ given the path of $W$ until time $t$. Then, there exist a stochastic process $Z_t$ such that
    \[
    M_s=M_t+\int_t^sZ_r\dW_r
    \]
    In particular,
    \[
    M_t=M_0+\int_0^tZ_r\dW_r=\mathbb{E}[Y]+\int_0^tZ_r\dW_r
    \]
\end{thm}
For the moment, we forget about stochastic control and focus on the random variable given by
\begin{equation}
Y:=\int_0^T C(s,X_s){{\ds}}+g(X_T)
\end{equation}
where
\[
{\dX}_t =b(t,X_t){{\dt}}+\sigma(t,X_t)\dW_t
\]
Note that the control is absent in the running cost function $C(t,x)$, drift $b(t,x)$, and diffusion $\sigma(t,x)$.
By Itô representation theorem for $Y$, we obtain
\[
Y=\mathbb{E}[Y|\mathcal{F}_t]+\int_t^T Z_r\dW_r
\]
In other words,
\[
\int_0^T C(r,X_r){{\dr}}+g(X_T) =  \mathbb{E}\left[\int_0^T C(r,X_r){{\dr}}+g(X_T)\Big|\mathcal{F}_t\right] + \int_t^TZ_r\dW_r
\]
We can use the properties of conditional expectation to write 
\[
\mathbb{E}\left[\int_0^T C(r,X_r){{\dr}}+g(X_T)\Big|\mathcal{F}_t\right] = \int_0^t C(r,X_r){{\dr}}+\mathbb{E}\left[\int_t^T C(r,X_r){{\dr}}+g(X_T)\Big|\mathcal{F}_t\right]
\]
This implies 
\[
\int_0^T C(r,X_r){{\dr}}+g(X_T) = \int_0^t C(r,X_r){{\dr}} + \mathbb{E}\left[\int_t^T C(r,X_r){{\dr}})\Big|\mathcal{F}_t\right] + \int_t^TZ_r\dW_r
\]
or
\[
\int_t^T C(r,X_r){{\dr}}+g(X_T) =  \mathbb{E}\left[\int_t^T C(r,X_r){{\dr}})\Big|\mathcal{F}_t\right] + \int_t^TZ_r\dW_r
\]
We set
\[
Y_t:=\mathbb{E}\left[\int_t^T C(r,X_r){{\dr}}+g(X_T)\Big|\mathcal{F}_t\right]
\]
Therefore, we derive the simple \emph{BSDE}
\[
Y_t = g(X_T) + \int_t^T C(r,X_r){{\dr}}  - \int_t^T Z_r\dW_r
\]
Or in differential form, we write
\begin{equation}
\label{eqn:BSDE_F-K}
    \begin{cases}
    \dd Y_t =  - C(t,X_t){{\dt}} + Z_t\cdot\dW_t\\
    Y_T=g(X_T)
\end{cases}
\end{equation}



% \subsubsection{Feynman-Kac formula for linear parabolic PDEs}
In order to see what $Z$ and $Y$ represent, we review the Feynman-Kac formula for  linear parabolic PDEs. 
    By the Markovian property of $X$, $Y_t$ can be written as a function of $(t,X_t)$
\begin{equation}
\label{eqn:F-K}
    Y_t=V(t,X_t)=\mathbb{E}\left[\int_t^T C(r,X_r){{\dr}}+g(X_T)\Big|\mathcal{F}_t\right]
\end{equation}
We assume that $V(t,x)$ is first order continuously  differentiable with respect to $t$ and second order continuously  differentiable with respect to $x$. Then, we can apply Itô's formula to write
\[
\begin{split}
    \dd Y_t = \dd V(t,X_t)  = &\Big(V_t(t,X_t)+\frac12(\sigma^{\intercal}\sigma)(t,X_t)\cdot D^2 V(t,X_t) + b(t,X_t)\cdot \nabla V(t,X_t)\Big)\dt \\
    &\hspace{2cm} +  \nabla V(t,X_t)\cdot \sigma(t,X_t)\dW_t
\end{split}
\]
We compare the above with \eqref{eqn:BSDE_F-K} to conclude that $Z_t^\top = \nabla V(t,X_t)^\top \sigma(t,X_t)$ and
\begin{equation}
\label{eqn:PDE_linear}
    \begin{cases}
    V_t(t,x)+\frac12(\sigma^{\intercal}\sigma)(t,x)\cdot D^2 V(t,x) + b(t,x)\cdot \nabla V(t,x) + C(t,x) = 0\\
    V(T,x) = g(X_T)
\end{cases}
\end{equation}

\textbf{Convention.} For aesthetic reasons, we switch $Z$ to $\sigma^{-1\top}Z$ to write the BSDE as
\begin{equation}
\label{eqn:BSDE_F-K_newZ}
    \begin{cases}
    \dd Y_t =  - C(t,X_t){{\dt}} + Z_t\cdot \sigma(t,x) \dW_t\\
    Y_T=g(X_T)
\end{cases}
\end{equation}
or equivalently
\begin{equation}
\label{eqn:BSDE_F-K_dX}
    \begin{cases}
    \dd Y_t =  - \big(C(t,X_t)+Z_t\cdot b(t,X_t)\big){{\dt}} + Z_t\cdot \dX_t\\
    Y_T=g(X_T)
\end{cases}
\end{equation}


Formula \eqref{eqn:F-K} provides a stochastic representation for the solution of \eqref{eqn:PDE_linear} known as \emph{Feynman-Kac} formula, which is used to approximate the solution of parabolic equations of the form \eqref{eqn:PDE_linear}. This approximation method is discussed in Section~\ref{sec:F-K_approx}.
If $X_t=W_t$ is a Wiener process, $b\equiv0$ and $\sigma\equiv \text{I}_d$ (the identity matrix) and we have the heat equation with a source term
\[
\begin{cases}
    V_t(t,x)+\frac12\Delta V(t,x) + C(t,x) = 0\\
    V(T,x) = g(X_T)
\end{cases}
\]
and 
\[
V(t,X_t)=\mathbb{E}\left[\int_t^T C(r,W_r){{\dr}}+g(W_T)\Big|\mathcal{F}_t\right]
\]

However, the BSDE \eqref{eqn:BSDE_F-K_newZ} provide more than an approximation for $Y_t$ or $V(t,x)$. The solution to the BSDE consists of the pair $(Y_t,Z_t)$ which yields simultaneously, 
$Y_t = V(t,X_t)$ and $Z_t= \nabla V(t,X_t)$. The BSDE approximation methods for control problems can also be applied for linear parabolic PDEs to obtain solution and first derivative at the same time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{BSDE representation for optimal control problems}
\label{sec:BSDE_approx}
In this section, we derive the BSDE for control problems in two ways. First, we use the HJB equation. Then, we derive it without using the HJB equation. 
In both cases, we only consider the case where the control variable is present in the drift of $X$, 
\[
{\dX}_t =b(t,X_t,{\color{orange}u_t}){{\dt}}+\sigma(t,X_t)\dW_t
\]
When $\sigma$ depends on the control, the BSDE is more involved and is refereed to as second order BSDE, 2BSDE. To know more about 2BSDEs, see \citet{CSTV07,STZ12}.
\subsection{BSDE via HJB}
Recall that the value function of the control problem
\[
V(t,x):=\inf_{u\in\mathcal{U}_{t,T}}\mathbb{E}\left[\int_t^T C(r,X_r,u_r){{\dr}}+g(X_T)\Big|X_t=x\right]
\]
satisfies the HJB equation
\begin{equation}\label{eqn:HJB_bsde}
    \begin{cases}
        V_t(t,x)+\frac12(\sigma^{\intercal}\sigma)(t,x)\cdot D^2 V(t,x) + H(t,x,\nabla V(t,x)) = 0\\
    V(T,x) = g(X_T)
    \end{cases}
\end{equation}
with 
\[
H(t,x,p)=\sup_{u\in U}\left\{ b(t,x,u)\cdot p + C(t,x,u)\right\}
\]
Define 
\[
\dd\tilde{X}_t = \sigma(t,\tilde{X}_t)\dW_t
\]
$\tilde{X}$ is an SDE derived from $X$ by dropping the drift $b(t,x,u)$ and it does not depend on the control anymore. 
We apply Ito's formula on $V(t,\tilde{X}_t)$ to write
\[
\begin{split}
    \dd V(t,\tilde{X}_t)  = &\Big(V_t(t,\tilde{X}_t)+\frac12(\sigma^{\intercal}\sigma)(t,\tilde{X}_t)\cdot D^2 V(t,\tilde{X}_t)\Big){{\dt}} +  \nabla V(t,\tilde{X}_t)\cdot \sigma(t,\tilde{X}_t)\dW_t
\end{split}
\]
By \eqref{eqn:HJB_bsde}, we have 
\[
\begin{split}
    \dd V(t,\tilde{X}_t)  = & -H(t,\tilde{X}_t,\nabla V(t,X_t)){{\dt}} +  \nabla V(t,\tilde{X}_t)\cdot \sigma(t,\tilde{X}_t)\dW_t\\
    = & -H(t,\tilde{X}_t,\nabla V(t,X_t)){{\dt}} +  \nabla V(t,\tilde{X}_t)\cdot \dd\tilde{X}_t
\end{split}
\]
We set $Y_t:=V(t,\tilde{X}_t)$ and $Z_t=\nabla V(t,\tilde{X}_t)$ to write
\begin{equation}
\label{eqn:BSDE_nonlinear}
    \begin{cases}
        \dd Y_t = -H\big(t,\tilde{X}_t,Z_t\big)\dt + Z_t\cdot\dd\tilde{X}_t\\
        Y_T=g(\tilde{X}_T)
    \end{cases}
\end{equation}

The above discussion leads to the following theorem.
\begin{thm}
\label{thm:BSDE}
    Let $V(t,x)$ be the classical solution (once continuously differentiable in $t$ and twice continuously differentiable in $x$) to \eqref{eqn:HJB_bsde} and $(Y_t,Z_t)$ satisfies \eqref{eqn:BSDE_nonlinear}. Then,  $V(t,\tilde{X}_t) = Y_t$ and $\nabla V(t,\tilde{X}_t) = Z_t$.
\end{thm}
\subsection{BSDE via maximum principle}
We present a formal process for writing the BSDE for a generic control problem.  We fix a control ${\color{orange}u_t}$ and denote
\[
Y^{{\color{orange}u}}_t:=\mathbb{E}\left[\int_t^T C(r,X_r,{\color{orange}u_r})\dr+g(X_T)\right]
\]
where 
\[
{\dX}_t =b(t,X_t,{\color{orange}u_t}){{\dt}}+\sigma(t,X_t)\dW_t
\]
Inspired by the martingale representation theorem, Theorem~\ref{thm:mg_rep} and \eqref{eqn:BSDE_F-K_dX},  
\[
\dd Y^{{\color{orange}u}}_t = - \big(C(t,X_t,{\color{orange}u_t}) + Z_t^{\color{orange}u}\cdot b(t,X_t,{\color{orange}u_t})\big) \dt + Z_t^{\color{orange}u}\cdot \dX_t
\]
We define
\[
H(t,x,z):=\inf_{u\in U}\left\{C(t,x,{\color{orange}u}) + z\cdot  b(t,x,{\color{orange}u})\right\}
\]
and consider the \emph{nonlinear} BSDE 
\begin{equation}
\label{eqn:BSDE_pre_formal}
    \dd Y_t =  - H(t,X_t,Z_t) \dt + Z_t\cdot \dd X_t
\end{equation}
Now, we appeal to the following maximum principle for BSDEs
\begin{thm}
    For $i=1,2$, let $(Y^i,Z^i)$ be the solution to the nonlinear BSDEs
    \[
    \dd Y^i_t =  - H_i(t,X_t,Z^i_t) \dt + Z^i_t\cdot \dd X_t,~~~Y^i_T=g(X_T)
    \]
    If $H_1(t,x,z)\le H_2(t,x,z)$, then $Y^1_t\le Y^2_t$ for all $t$.
\end{thm}
To use the above theorem, if we take $H_1(t,x,z)=C(t,x,u) + z\cdot  b(t,x,{\color{orange}u})$ and $H_2(t,x,z)$ $=H(t,x,z)$, then we argue that 
\[
Y^{{\color{orange}u}}_t\le Y_t
\]
Furthermore, the equality is attained at
\[
u^*(t,x,z):=\text{\normalfont argmin}_{u\in U}\left\{z\cdot b(t,x,u) + C(t,x,u)\right\}
\]
\begin{ex}
    Consider the stochastic linear quadratic problem below:
    \begin{equation}
        \inf_u\mathbb{E}\Big[\int_0^T \Big(aX_t^2+bX_t +Au_t^2 + Bu_t\Big){\dt}+\alpha X_T^2+\beta X_T\Big]
    \end{equation}
    with
    \begin{equation}
        {\dX}_t=(\gamma X_t+\kappa u_t){{\dt}}+\sigma \dW_t
    \end{equation}
where $a,A, \alpha>0$ and $b,B,\beta,\gamma$ and $\kappa$ are constants.
Use both methods discussed in this section to write the BSDE for this control problem.
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Beyond Markovian control}
A generic BSDE is usually independent of a stochastic control problem and is given by
\begin{equation}
    \label{eqn:BSDE}
    \begin{cases}
        \dd Y_t=-H(t,X_t,Y_t,Z_t)\dt + Z_t dW_t\\
        Y_T=\xi
    \end{cases}
\end{equation}
where $\xi$ is a random variable dependent on $\mathcal{F}_T$. If $H$ is a Hamiltonian derived from stochastic control problem, then $Y_t$ and $Z_t$ represent the value function and its gradient. 

Dependence of $H$ on $X_t$ can  be generalized to dependence of $H$ to the path of $X$ until time $t$, i.e., $H(t,X_{\cdot\wedge t},Y_t,Z_t)$ where $X_{\cdot\wedge t} = (X_s:s\le t)$. Furthermore, we can relax dependence of $H$ on a specific state variable $X$ and write $H(t,\omega,Y_t,Z_t)$, where $\omega$ is a random outcome in the sample space. for instance, $\omega$ can be the path of Brownian motion. 

In this case, we write a non-Markovian (path-dependent) BSDE by 
\begin{equation}
    \label{eqn:BSDE_nonMarkov}
    \begin{cases}
        \dd Y_t=-H(t,\omega,Y_t,Z_t)\dt + Z_t dW_t\\
        Y_T=\xi
    \end{cases}
\end{equation}

For a generic BSDE to have a solution, it is sufficient that 
\begin{enumerate}[label= \bfseries \arabic*)]
    \item $\mathbb{E}[\xi^2]<\infty$
    \item for all $y$ and $z$, $H(t,\omega,y,z)$ is progressively measurable, i.e., measurable with respect to $t$ and $\omega$ simultaneously with respect to the information until time $t$
    \item $\mathbb{E}[\int_0^T H^2(t,\omega,0,0)\dt]<\infty$
    \item $H(t,\omega,y,z)$ is Lipschitz on $y$ and $z$ uniformly in $t,\omega$, i.e.
    \[
    |H(t,\omega,y,z)-H(t,\omega,y',z')|
    \le C|y-y'|+|z-z'|)
    \]
    for some constant $C$ independent of $(t,\omega)$.
\end{enumerate}
We have the above theorem from \cite[Proposition~1.1]{pardoux98}.
\begin{thm}[Existence and uniqueness theorem for BSDEs]
Under the above conditions,  there exists a couple $(Y_t,Z_t)$ such that \eqref{eqn:BSDE_nonMarkov} holds, i.e.,
\begin{equation}
Y_t=\xi+\int_t^T \Big(H(s,\omega,Y_s,Z_s){{\ds}}-Z_s\dW_s\Big),
\end{equation}
and there exists a constant $C>0$ that only depends on the Lipschitz constant $H$ such that that
\begin{equation}
\mathbb{E}\left[\sup_{t\in[0,T]}|Y_t|^2+\int_0^TZ_s^2{{\ds}}\right]\le C\left(\mathbb{E}[\int_0^TH^2(t,\omega,0,0)\dt + \xi^2]\right)
\end{equation}
\end{thm}




\subsection{Linear BSDEs}
Linear BSDEs take the form
\begin{equation}\label{eqn:PDE_linear_BSDE}
\dd Y_t=-(\alpha_tY_t+\beta_tZ_t+L_t){{\dt}}+Z_t\dW_t,
\end{equation}
where $\alpha$, $\beta$, and $L$ are arbitrary progressively measurable processes.
One can write a closed-form solution for a linear BSDE. To do so, note that
\begin{equation}
	\dd Y_t=-(\alpha_tY_t+L_t){{\dt}}+Z_tdB^{\beta}_t,
\end{equation}
where by the Girsanov theorem, $dB^{\beta}_t:=\beta_t{{\dt}}+\dW_t$ is a Brownian motion under the probability $\mathbb{P}^{\beta}$ given in
\begin{equation}
	\frac{d\mathbb{P}^{\beta}}{d\mathbb{P}~}\Big|_{\mathcal{F}_t}:=\exp\left(
	\int_0^t\beta_s \dW_s-\frac12\int_0^t\beta^2_s{{\ds}}
	\right)
\end{equation}
If we define $\tilde{Y}_t:=e^{\int_0^t \alpha_s{{\ds}}}Y_t$, then
\begin{equation}
	d\tilde{Y}_t=e^{\int_0^t \alpha_s {{\ds}}}\left(L_t{{\dt}}+Z_tdB^{\beta}_t\right).
\end{equation}
In other words,
\begin{equation}
	\tilde{Y}_t=\tilde{Y}_T-\int_{t}^{T} e^{\int_0^s \alpha_z dz}\left(L_s{{\dt}}+Z_sdB^{\beta}_s\right),
\end{equation}
or, 
\begin{equation}
	{Y}_t=e^{\int_t^T \alpha_s{{\ds}}}\tilde{Y}_T-\int_{t}^{T} e^{\int_t^s \alpha_z dz}\left(L_s{{\dt}}+Z_sdB^{\beta}_s\right)=e^{\int_t^T \alpha_s{{\ds}}}\xi-\int_{t}^{T} e^{\int_t^s \alpha_z dz}\left(L_s{{\dt}}+Z_sdB^{\beta}_s\right).
\end{equation}
After taking conditional expectation with respect to $\mathbb{P}^{\beta}$, we obtain
\begin{equation}
	Y_t=\mathbb{E}^{\beta}\left[e^{\int_t^T \alpha_s {{\ds}}}\xi-\int_{t}^{T} e^{\int_t^s \alpha_z dz}L_s{{\dt}}
	\Big|\mathcal{F}_t\right].
\end{equation}
If we define 
\begin{equation}
	\Gamma_t=e^{\int_t^T \alpha_s {{\ds}}}\frac{d\mathbb{P}^{\beta}}{d\mathbb{P}~}\Big|_{\mathcal{F}_t}=\exp\left(
	\int_0^t\beta_s \dW_s+\int_0^t(\alpha_s-\frac12\beta^2_s){{\ds}}
	\right),
\end{equation}
by changing the measure back to $\mathbb{P}$
\begin{equation}
	Y_t=\Gamma^{-1}_t\mathbb{E}\left[\Gamma_{T} \xi-\int_{t}^{T} \Gamma_{s} L_s{{\dt}}
	\Big|\mathcal{F}_t\right]=\mathbb{E}^{\beta}\left[ e^{\int_t^T \alpha_z dz}\xi-\int_{t}^{T} e^{\int_t^s \alpha_z dz} L_s{{\dt}}
	\Big|\mathcal{F}_t\right]
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
\section{Some nonstandard control problems and their HJB equations}
In this section, we provide a review of some HJB equations that come from stochastic singular control, optimal stopping time, stochastic impulse control, and switching problems. The treatment of such problems via HJB equations is similar, however, we first need to derive HJB equations. 
\subsection{Stochastic singular control problems}
Consider the stochastic control problem below:
\begin{eg}
    \begin{equation}
        \inf_{u_t}\mathbb{E}\bigg[\int_0^T\Big(\frac12 X_t^2 - u_t\Big){\dt}+ X_T^2- X_T\bigg]
    \end{equation}
    with $u_t\ge0$ (takes nonnegative values) and 
    \begin{equation}
        {\dX}_t=(X_t+u_t){{\dt}}+\sigma \dW_t
    \end{equation}
    The HJB  is formally written as 
    \begin{equation}
        \begin{cases}
            {V}_{t} + x V_{x} + \frac{\sigma^2}{2}\partial^2_{x} V + \frac12x^2 + \inf_{u\ge0} u( V_{x}-1)=0\\
            V(T,x)=x^2-x
        \end{cases}
    \end{equation}
    Note that 
    \begin{equation}
        \inf_{u\ge0} u( V_{x}-1)=
        \begin{cases}
            -\infty &  V_{x}-1<0\\
            0 &  V_{x}-1\ge0
        \end{cases}
    \end{equation}
    Therefore, it is natural to assume that the value function satisfies $ V_{x}-1\ge0$. In fact, we expect so find the value functions such that if $ V_{x}>1$, then ${V}_{t} + x V_{x} + \frac{\sigma^2}{2}\partial^2_{x} V + \frac12x^2=0$ and when $ V_{x}=1$, the we only have ${V}_{t} + x V_{x} + \frac{\sigma^2}{2}\partial^2_{x} V + \frac12x^2\le 0$. More precisely, inequality $ V_{x}-1\ge0$ divides $[0,T]\times\mathbb{R}$ into two regions: 
    \begin{itemize}
        \item $\mathbf{N} = \{(t,x):  V_{x}(t,x)>1\}$
        \item $\mathbf{R} = \{(t,x):  V_{x}(t,x)=1\}$
    \end{itemize}
Inside $\mathbf{N}$, we expect that ${V}_{t} + x V_{x} + \frac{\sigma^2}{2}\partial^2_{x} V + \frac12x^2=0$ holds. This equations has a solution of the form $v(t,x)=a(t)x^2+b(t)x+c(t)$. 

We denote the boundary of $\mathbf{N}$ by $\partial \mathbf{N}$ and we define $R(t)$ such that $(t,R(t))\in\partial\mathbf{N}$, assuming that $R(t)$ is uniquely determined.
In addition, we expect to see that the value function and its first derivative are the same in the both sides of $\partial \mathbf{N}$, particularly, $ V_{x}(t,R(t))=1$. Therefore, if $v$ is the solution to 
    \begin{equation}\label{eqn:for v in sigular example}
        \begin{cases}
            {V}_{t} + x V_{x} + \frac{\sigma^2}{2}\partial^2_{x} v + \frac12x^2 =0\\
            v(T,x)=x^2-x
        \end{cases}
    \end{equation}
    then, we anticipate to write 
    \begin{equation}
        V(t,x)=\begin{cases}
            v(t,x)& x\ge R(t)\\
            x-R(t)+v(t,R(t))& x< R(t)
        \end{cases}
    \end{equation}
Solving \eqref{eqn:for v in sigular example}, we obtain that $v(t,x)=\frac{5e^{2(T-t)}-1}{4}x^2-e^{T-t}x-\frac{\sigma^2}{4}\Big(T-t-\frac{5e^{2(T-t)}-1}{2}\Big)$ and, therefore, $R(t)=2\frac{e^{T-t}+1}{5e^{2(T-t)}-1}$. 
\end{eg}
\begin{ex}\label{ex:DS}
    \begin{equation}
        \inf_{u_t\ge0}\mathbb{E}\Big[\int_0^\tau e^{-r t} \Big(\mu - u_t\Big){\dt}\Big]
    \end{equation}
    where $\tau=\inf\{s\ge0: X_t=0\}$ with
    \begin{equation}
        {\dX}_t=(\gamma X_t-u_t){{\dt}}+\sigma \dW_t, ~~ X_0=x\ge0
    \end{equation}
    Write the HJB and obtain the nonlinear term by evaluating the infimum. 
\end{ex}
Singular control problems are easy to detect; the running cost and drift of the SDE are both linear in the control. To see this, consider the control problem
\begin{equation}\label{eqn:dpp_singular}
        \inf_{u}\mathbb{E}\Big[\int_0^T \big(C(s,X^u_s)+au_s\big){{\ds}} + g(X_T^u)\Big]
    \end{equation}
where $u_t$ takes values in a closed convex cone $\mathcal{C}$ and 
\begin{equation}
	\begin{cases}
	    {\dX}^u_t =(\mu(t,X_t^u)+Au_t){{\dt}}+\sigma(t,X_t^u)\dW_t\\
     X^u_0=x\in\mathbb{R}^d
	\end{cases}
\end{equation}
The heuristic derivation of the HJB yields
\begin{equation}
    \begin{cases}
        0={V}_{t}(t,x)+\frac12  a(t,x,u)\cdot D^2V(t,x) +\nabla V(t,x)\cdot \mu(t,x)+C(t,x)+\inf_{u}\Big\{au +
           Au\cdot \nabla V(t,x)\Big\}\\
         V(T,x)=g(x)
    \end{cases}
\end{equation}
$a=\sigma^\intercal\sigma$. The infimum above is given by 
\begin{equation}
    \inf_{\hat{u}\in\mathcal{C},~|\hat{u}|=1,\lambda\ge0}\lambda \hat{u}\cdot \big(a +
          A\nabla V(t,x)\big)=\inf_{\lambda\ge0}\lambda \inf_{|\hat{u}|=1}\hat{u}\cdot \big(a +
          A\nabla V(t,x)\big)=\inf_{\lambda\ge0}\lambda \mathcal{H}(\nabla V(t,x))
\end{equation}
where $\mathcal{H}(\nabla V(t,x)):=\inf_{|\hat{u}|=1}\hat{u}\cdot \big(a +
          A\nabla V(t,x)\big)$. If $\mathcal{H}(\nabla V(t,x))<0$, then the infimum is $-\infty$ and the problem becomes degenerate. To avoid degeneracy, we must have $\mathcal{H}(\nabla V(t,x))\ge0$, in which case infimum is attained in $\lambda=0$ and we have 
\begin{equation}
    \begin{cases}
        0={V}_{t}(t,x)+\frac12  a(t,x,u)\cdot D^2V(t,x) +\nabla V(t,x)\cdot \mu(t,x)+C(t,x)\\
         V(T,x)=g(x)
    \end{cases}
\end{equation}
For simplicity of notation, we set
\begin{equation}\label{eqn:mathcalL}
   \mathcal{L}V(t,x):={V}_{t}(t,x)+\frac12  a(t,x,u)\cdot D^2V(t,x) +\nabla V(t,x)\cdot \mu(t,x)+C(t,x)
\end{equation}          

The interpretation of the HJB requires some \textit{variational inequalities}:\footnote{For more details on why we write the variational inequality this way, see \cite[Chapter 11]{Fleming-Soner-book-06} and the  notion of sub and super solutions.}
\begin{enumerate}
    \item for all $(t,x)$, $-\mathcal{L}V(t,x)\ge0$ and $\mathcal{H}\nabla V(t,x)\ge0$.
    \item If $\mathcal{H}(\nabla V(t,x))>0$, then $-\mathcal{L}V(t,x)=0$.
\end{enumerate}


The description of the optimal control in singular control problems need the notion of local time of SDEs. For instance, in Exercise~\ref{ex:DS}, the optimal control is the local time of the process, ${\dX}_t=\gamma X_t{{\dt}}+\sigma \dW_t$ at the point $\hat{x}$, existence of which is obtained through solving the HJB and going through verification step. 
%%%%%%%%%%%%%%%%%%
\subsection{Optimal stopping problem}
Let $X$ be given by the SDE
\begin{equation}
    {\dX}_t = \mu(t,X_t){\dt}+\sigma(t,X_t)\dW_t
\end{equation}
in a filtered probability space
and consider the following problem
\begin{equation}
    \sup_{}\mathbb{E}\bigg[\int_0^{\tau\wedge T} e^{-rt} C(t,x_t){\dt} + g(X_{\tau\wedge T})\bigg]
\end{equation}
where the supremum is over all stopping time $\tau$ adapted to the filtration. These class of problems, which are not stochastic control problems,  called \emph{optimal stopping problems}. To solve optimal stopping problems, we can write the following DPP:
\begin{equation}
    V(t,x)=\mathbb{E}_{t,x}\bigg[\int_0^{\eta}e^{-r(s-t)}C(t,X_s){\ds}+e^{-r(\eta-t)}g(X_\eta)\bigg]
\end{equation}
for any stopping time $\eta$ with values in $[t,T]$. 
The DPP above leads to the following variational HJB equation:
\begin{enumerate}
    \item $V(t,x)\ge g(x)$ and $-\mathcal{L}V(t,x)\ge0$
    \item if $V(t,x)> g(x)$, then $-\mathcal{L}V(t,x)=0$
\end{enumerate}
where $\mathcal{L}$ is given in \eqref{eqn:mathcalL}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Numerical optimal control}
In this chapter, we present some of the most recent numerical methods to solve optimal control problems. The first numerical methods presented here are based on solving HJ or HJB equations. Since HJ or HJB equations are nonlinear PDEs, are nonlinear PDE solvers can be potential solutions. However, some of the classical methods  of solving nonlinear PDE, such as finite difference, finite elements, finite volume, and the likes, are only practical in low dimensions ($d\le3$). Moreover, learning such methods heavily relies on numerical analysis techniques, which is outside the interest of these notes. Therefore, we present the most recent methods that can be applied in higher dimensions alongside some general conditions for the numerical PDE schemes to converge to the value function of the control problem. 

Before we continue to learn the numerical methods, we shall emphasize on that we are trying to find the optimal control and/or the value function for each possible value of $t$ and $x$. Therefore, we first study the approximation methods of functions. 

\section{Approximating functions}
Let's assume that we want to approximate a function $f:[a,b]^d\to\mathbb{R}$ using only information at finite number of points, i.e., $f(x_i)=y_i$.

\subsection{Regression}
Simplest of all methods is regression. In regression, we choose a basis set of functions $\{\phi_1,\cdots,\phi_K\}$ and find coefficients $a_1,...,a_K\in\mathbb{R}^d$ such that 
\[
f(x)\approx \hat{f}(x):=\sum_{k=1}^Ka_k\cdot \phi_k(x)
\]
Let $\{(x^j,y^j):i=1,...,J\}$ be such that $y^j=f(x^j)$, then we set values for $a_1,..,a_K$ such that the mean square error is minimized: 
\begin{equation}
\label{prob:inf_regresion}
\inf_{a_1,...,a_K}\sum_{j=1}^J\left(\sum_{k=1}^{K} a_k \phi_k(x^j)-y^j\right)^2
\end{equation}
Taking derivatives with respect to $a_k$, we obtain
\[
\sum_{j=1}^J\phi_{k}(x^j)\sum_{\ell=1}^{K} a_{\ell} \phi_{\ell}(x^j)-y^j=0
\]
or
\begin{equation}
    \label{eqn:PDE_linear_regression}
\sum_{\ell=1}^{K} \phi_{k\ell}a_{\ell}=b_{k},~~\text{ for all }~~k=1,...,K
\end{equation}
where
\[
\phi_{k\ell}:=\sum_{j=1}^J\phi_{k}(x^j)\phi_{\ell}(x^j)~\text{ and }~b_k:=\sum_{j=1}^J\phi_{k}(x^j)y^j
\]
Solving the linear system of equation \eqref{eqn:PDE_linear_regression} is often time-consuming. If the functions $\phi_k$ are not chosen properly, the solution may not be unique. However, if we choose an \emph{orthogonal} system of functions, then the solution to \eqref{eqn:PDE_linear_regression} is simple. Let 
\[
\int_a^b \phi_k(x)\phi_{\ell}(x)dx=
\begin{cases}
    0&k\neq\ell\\
    \neq0&k=\ell
\end{cases}
\]
and $x^j$s are i.i.d. samples, then
\[
\frac{1}{J}\phi_{k\ell}=\frac{1}{J}\sum_{j=1}^J\phi_{k}(x^j)\phi_{\ell}(x^j)\approx \int_a^b \phi_k(x)\phi_{\ell}(x)dx\begin{cases}
    =0&k\neq\ell\\
    \int_a^b \phi^2_k(x)dx&k=\ell
\end{cases}
\]
and \eqref{eqn:PDE_linear_regression} is approximated by
\[
a_k = \frac{\int_a^b \phi_k(x)f(x)dx}{\int_a^b \phi^2_k(x)(x)dx} \approx \frac{\sum_{j=1}^{J}y^j\phi_{k}(x^{j})}{\sum_{j=1}^{J}\phi^2_{k}(x^{j})}
\]
If we work with real data, we may not have the luxury to assume $x^j$s are i.i.d. In the context of optimal control problems, there is no prior real data and we do not have this issue. On a different point, the regression can be applied to approximate functions with values in $\mathbb{R}^d$. However, the complexity of orthogonal functions in higher dimensions can increase significantly. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neural networks}
A \emph{feed-forward neural network} is a function that is constructed as follows. There are $L$ layers.
In the first layer, the input $x\in\mathbb{R}^d$ is multiplied by  a $d_1\times d$ matrix and the result is added to a $d_1$ vector. 
\[
\tilde{x}_1=\Lambda_1x:=W_{d_1\times d}x+b_1
\]
Then, before the output of the first layer is fed into the next layer, a nonlinear function $\sigma_1$ is applied on $x_1$,
\[
x_1:=\Sigma_1(\tilde{x}_1)
\]
Similarly, at layer $\ell$, the output $x_{\ell-1}$ of the previous layer is transformed by an affine function and inserted into the nonlinear function $\sigma_\ell$:
\[
x_\ell=\Sigma_{\ell}(\Lambda_{d_\ell}x_{\ell-1}), ~\text{ with }~\Lambda_{d_\ell}:=W_{d_{\ell}\times_{\ell-1}}x_{\ell-1}+b_\ell
\]
In the final layer of a feedforward neural network, the dimension of the output is determined by the dimension of the output of the function that needs to be approximated.
If we like to approximate a function $f:[a,b]^d\to\mathbb{R}^m$, $d_L=m$. Then matrices $W_{d_{\ell-1}\times{d_{\ell}}}$  are called the weights and vectors $b_\ell$ are called the bias of the neural network, or simply we can call them parameters of the neural network. We denote the neural network by
\[
NN(x,\theta):=\Sigma_{\ell}\circ\Lambda_{d_\ell}\circ\cdots\circ\Sigma_1\circ L_{1}(x)
\]
where the parameters of the neural network are denoted by
\[
\theta=(W_{d_1\times d},b_1,...,W_{d_{\ell-1}\times d_\ell},b_{d_{\ell}})
\]
What makes neural network approximation efficient is the simplicity of the functions involved. The affine functions are indeed simple. The nonlinear functions $\Sigma_i$ are called activation functions. Some example of activation functions include
\begin{align}
    \Sigma(x)=(x)^+&~~~\text{ReLU}\\
    \Sigma(x)=\frac{1}{1+e^{-x}}&~~~\text{Sigmoid}\\
    \Sigma(x)=\tanh(x)&~~~\text{Tangent hyperbolic}
\end{align}
The activation functions on multi-dimensional data act elementwise. $\Sigma(x_1,...,x_d)=(\Sigma(x_1),...,\Sigma(x_d))$.
For the goal of approximating a function with a neural network, on needs to minimize 
\begin{equation}\label{prob:inf_nn}
    \inf_{\theta}\mathcal{J}(\theta),,~~~~~\mathcal{J}(\theta):=\sum_{j=1}^J\left(NN(x^j,\theta)-y^j\right)^2
\end{equation}
To evaluate the infimum above, we need to appeal to gradient descent algorithm. The most important step in the gradient descent is to evaluate the derivative of the loss function in \eqref{prob:inf_nn}. This is were simplicity of the activation functions in the neural network helps. One can evaluate $\mathcal{J}(\theta)$ once and use it via symbolic calculations to evaluate the gradient at each time the parameter is changed during gradient descent. 
\begin{algorithm}
        % Algorithm content goes here
        \caption{Gradient descent}
        \label{alg:gd_nn}
        \KwData{$\theta_0$ is randomly chosen.}
        \KwData{The gradient of $\mathcal{J}(\theta)$ is evaluated in a symbolic manner.}
        \Parameter{Learning rate $\{\alpha_n\}_n$, number of iterations $N$, and tolerance $\epsilon>0$.}
        \While{$|\nabla \mathcal{J}(\theta_n)|>\epsilon$ \& $n\le N$}{
        $\theta_{n+1}\leftarrow \theta_{n} - \alpha_n \nabla \mathcal{J}(\theta_{n})$\;
        $n \leftarrow n + 1$
        }
        \Return{$\theta_n$}
        \caption{Gradient descent algorithm for neural networks}
\end{algorithm}
Any deep learning package has implemented automatic differentiation to evaluate the loss function and to build a tree using chain rule. This tree is saved in the memory and is used at any step of the algorithm to evaluate the gradient. This process is also referred to as \emph{forward propagation}. After the tree is build and the loss function is evaluated, the algorithm goes backward in the tree to evaluates the gradient and to adjust the parameters using the chain rule. This step of the process is called \emph{back propagation}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Comparison between polynomial approximation and neural network}
While polynomial approximation has been a state-of-art for many decades, neural networks claim a place in the approximation theory due to the capability so handle nonlinear problems. Especially, orthogonal polynomials are efficient if we are using regression method. However, they lack efficiency in more complicated loss functions. The most important gain of the neural networks over polynomial approximation is where the problem is high-dimensional. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convergence of numerical scheme for PDEs}
Numerical schemes for PDEs are studied extensively. Examples of such numerical schemes are \emph{finite-difference methods}, \emph{finite-element methods}, and \emph{Monte-Carlo methods}. In either cases, there are some general conditions that guarantees the convergence of the approximate solution to the solution of the PDE. In the context of control, the notion of solution to the HJB equation is viscosity solution, which is not necessarily a smooth function. Therefore, we study a general framework that guarantees the convergence of approximation schemes to the viscosity solutions for the HJB equations. We note that there are some prerequisites to this framework. First, we need to show that the HJB has a unique viscosity solution. This is studied in several literature including but not limited to \cite{CIL92,Fleming-Soner-book-06,Touzi12}. Second, we need to show that the value function satisfies HJB equation in viscosity sense. This is also a classical result obtained in many literature, e.g., \citet{Fleming-Soner-book-06,Touzi12}. Here we assume that both of these results are established. Consider the HJB equation
\begin{align}
     &\begin{cases}
    0= V_t(t,x) + H(t,x,V_x,V_{xx})\\
    V(T,x)=g(x)
\end{cases}\label{eqn:HJB_multi}\\
&\hspace{0cm}H(t,x,p,\Gamma):=\inf_{u\in U}\left\{ C(t,x,u)  + p\cdot b(t,x,u)+\frac12 \Gamma\cdot  (\sigma^\top\sigma)(t,x,u)\right\}\label{eqn:Hamilton_multi}
\end{align}
\begin{assp}
    The HJB equation \eqref{eqn:HJB_multi} has a unique viscosity solution which is equal to the value function of the control problem 
    \[
    \begin{split}
    &\inf_{u\in\mathcal{U}}\left\{\int_0^TC(t,X_t,u_t)\dt+g(X_T)\right\}\\
        &\dX_t = b(t,X_t,u_t)\dt+\sigma(t,X_t,u_t)\dW_t
    \end{split}
    \]
    \end{assp}
    Note that if we set $\sigma\equiv0$, we have a deterministic control problem. 
    We consider a numerical method for an HJB equation as an equation of discrete points. For instance, in the finite-difference method, the evaluations are performed on a grid on the $(t,x)$ variables. 
    \begin{eg}
        In the simplest finite-difference method in dimension $1$, explicit centered finite-difference method, we work on the grid $\mathcal{G}_h=\{(t_i,x_j):t_i=ih,~x_j=j\sqrt{h}, ~i=0,...,N-1,~j=-J,...,J\}$, where $h=\frac{T}{N}$. The approximate partial derivatives are given by
        \[
        \begin{split}
        V_t(t_i,x_j)&\approx\frac{V(t_{i+1},x_j)-V(t_{i},x_j)}{h}\\
        V_x(t_i,x_j)&\approx\frac{V(t_{i},x_{j+1})-V(t_{i},x_{j-1})}{2h}\\
        V_{xx}(t_i,x_j)&\approx\frac{V(t_{i},x_{j+1})+V(t_{i},x_{j-1})-2V(t_{i},x_{j})}{h^2}\\
        \end{split}
        \]
       Then, we find $V^h(t_i,x_i)$ such that
       \[
       \begin{cases}
    0= V_t(t,x) + H(t,x,V_x,V_{xx})\\
    V(T,x)=g(x)
\end{cases}
       \]
    \end{eg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Policy gradient method for optimal control}
\emph{Policy gradient methods} (PGM) are one of the numerical methods to tackle complicated control problems. They are first introduced in the context of reinforcement learning (RL) and later were generalized to control problems, \citet{Powell19}. Some literature, \citet{NR21}, refer to PGM as \emph{iterative diffusion optimization}. In this section, we review the method and discuss the implementation details. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Loss function for control problems and functional optimization}
The goal of  approximation of $f:[a,b]^d\to\mathbb{R}^m$ is achieved by minimizing a loss function similar to \eqref{prob:inf_regresion}. 
In the context of optimal control, we like to find a function $\phi(t,x)$ to minimize
\begin{equation}\label{func:loss_generic}
{\mathcal{J}}(\phi):=\mathbb{E}\left[\int_0^T C(t,X_t,\phi(t,X_t)){{\dt}}+g(X_T)\right].
\end{equation}
where 
\[
{\dX}_t =b(t,X_t,\phi(t,X_t)){{\dt}}+\sigma(t,X_t,\phi(t,X_t))\dW_t
\]
In the above, $J(\phi)$ if the target of the minimization. The choice of the Markovian control $u_t=\phi(t,X_t)$ is allowed because of \citet{el1987compactification,haussmann1986existence}. An important consideration in the choice of $\phi$ is to obtain consistency with the space of admissible control. 
A control problem is simply an optimization over a set of functions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parametrization of functions}
In the loss function \eqref{func:loss_generic}, we replace $\phi$ with a parametrized function, such as  a neural network.
\begin{equation}\label{func:loss_parametrized}
{\mathcal{J}}(\phi(\cdot,\theta)):=\mathbb{E}\left[\int_0^T C(t,X_t,\phi(t,X_t;\theta)){{\dt}}+g(X_T)\right].
\end{equation}
where 
\[
{\dX}_t =b(t,X_t,\phi(t,X_t;\theta)){{\dt}}+\sigma(t,X_t,\phi(t,X_t;\theta))\dW_t
\]
Doing so, we restricted the set of all controls with the ones that can be represented by $\phi(t,x;\theta)$ for some value of $\theta$. After this restriction, if the optimal control fits into such parametrized function, we don't lose anything. Additionally, we gain a great deal in simplifying the problem from optimization of functions to optimization of parameter $\theta$.
\[
\inf_{\theta}\mathcal{J}(\theta)
\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discretization}
For implementation, we cannot work with a continuous time. We surely require to discretize the SDE for state variable and the integral for the running cost. The simplest of such discretizations is Euler-Maruyama discretization given by $\Delta t = \frac{T}{N}$ and
\begin{equation}\label{func:loss_discrete}
\hat{\mathcal{J}}(\phi(\cdot,\theta)):=\mathbb{E}\left[\sum_{i=0}^{N-1} C(t_{i},\hat{X}_{t_i},\phi(t_{i},\hat{X}_{t_{i}};\theta))\Delta t+g(\hat{X}_T)\right].
\end{equation}
where 
\[
\hat{X}_{t_{n+1}} = \hat{X}_{t_{n}} + b(t_{n},\hat{X}_{t_{n}},\phi(t_{n},\hat{X}_{t_{n}};\theta)) \Delta t+\sigma(t_{n},\hat{X}_{t_{n}},\phi(t_{n},\hat{X}_{t_{n}};\theta))\Delta B_{t_{n+1}}
\]
Here, $\Delta B_{t_{n+1}}= B_{t_{n+1}}- B_{t_{n+1}}\sim\mathcal{N}(0,\Delta t)$.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Control_lecture_notes/Figs/OneUnit.pdf}
    \caption{A design of a unit for solving control problems with deep learning. The neural network approximating control in evaluated in the inner rectangle with affine transformations $Wx+b$ and activation function  $\Sigma$. The output of the neural network yields the control $\phi(t,x)$. This unit has two other functions, one is updating the state variable $x$ after the control is evaluated and the other evaluates the contribution of the  control and state variable in the loss function.}
    \label{fig:pgm_unit}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Monte Carlo sampling and empirical loss function}
In practice, we cannot evaluate the expected value analytically. Therefore, we appeal to sampling from $\hat{X}$ and empirical approximation of the expected value. To achieve this goal, we require to create sample path of $\hat{X}$. We start from sampling $J$ i.i.d. samples of $\hat{X}_0$ via some arbitrary distribution $\mathcal{L}_0$, $\hat{X}_0^j$. Then, we generate i.i.d. samples of the increments of the Wiener process. 
\[
\Delta B_{t_{n+1}}^j\sim\mathcal{N}(0,\Delta t),~~\text{for }~~j=1,...,J,~n=0,...N-1
\]
\begin{equation}\label{eqn:discrete_euler}
\hat{X}^j_{t_{n+1}} = \hat{X}^j_{t_{n}} + b(t_{n},\hat{X}^j_{t_{n}},\phi(t_{n},\hat{X}^j_{t_{n}};\theta)) \Delta t+\sigma(t_{n},\hat{X}^j_{t_{n}},\phi(t_{n},\hat{X}^j_{t_{n}};\theta))\Delta B_{t_{n+1}}^j    
\end{equation}
Then, the empirical loss is given by
\begin{equation}\label{func:loss_empirical}
\overline{\mathcal{J}}(\phi(\cdot,\theta)):=\frac{1}{J}\sum_{j=1}^{J}\left[\sum_{n=0}^{N-1} C(t_{n},\hat{X}^j_{t_n},\phi(t_{n},\hat{X}^j_{t_{n}};\theta))\Delta t+g(\hat{X}^j_T)\right].
\end{equation}
The gradient of $\bar{\mathcal{J}}$ with respect to $\theta$ can be evaluated by using chain rule.
\begin{algorithm}
        % Algorithm content goes here
        \caption{Policy gradient method for control problems}
        \label{alg:pgm}
        \KwData{$i=0$, $\phi(t,x;\theta)$ is constructed and $\theta=\theta_i$ is randomly chosen.}
            \KwData{$\Delta t=\frac{T}{N}$, $\hat{X}_0^j$ are sampled from $\mathcal{L}_0$ and $\Delta B_{t_{n+1}}^j\sim\mathcal{N}(0,\Delta t)$ are sampled all independently, $n=0,...,N-1$ \& $j=1,...,J$.}
        \Parameter{Learning rate $\{\alpha_i\}_i$ and number of iterations $I$}
        \While{$i\le I$}{
        $\overline{\mathcal{J}}(\phi(\cdot;\theta_i)= 0$\;
        \For{$n=0,...,N-1$ \& $j=1,...,J$}{
        \eqref{eqn:discrete_euler} is used to create $J$ sample paths of $\hat{X}$.\;
        $\overline{\mathcal{J}}(\phi(\cdot;\theta_i)\gets\overline{\mathcal{J}}(\phi(\cdot;\theta_i)+\frac{1}{J}\sum_{j=1}^{J}C(t_{n},\hat{X}^j_{t_n},\phi(t_{n},\hat{X}^j_{t_{n}};\theta_i))\Delta t$\;
        $\overline{\mathcal{J}}(\phi(\cdot;\theta_i) \gets \overline{\mathcal{J}}(\phi(\cdot;\theta_i) + \frac{1}{J}\sum_{j=1}^{J}g(\hat{X}^j_T)$
        }
        The gradient of empirical loss, $\nabla_{\theta}\bar{\mathcal{J}}(\theta_i)$, is evaluated.\;
        $\theta_{i+1}\gets \theta_{i} - \alpha_i \nabla_\theta \overline{\mathcal{J}}(\theta_{i})$\;
        $i \gets i + 1$
        }
        \Return{$\theta_i$}
        \caption{Gradient descent algorithm for neural networks}
\end{algorithm}
\subsection{Design of PGM}
A key part of the policy gradient method is the evaluation in lines 4, 5, and 6 in Algorithm~\ref{alg:pgm}. After $\theta_i$ and consequently $\phi(t,x;\theta_i)$ are updated, in line 4, we  re-evaluate the sample paths in \eqref{eqn:discrete_euler} based on $\theta_i$. 
We call this step \emph{Update$(\phi,x)$} in Figure~\ref{fig:pgm_unit}. In line 5, we evaluate the contribution of the control $\phi(t,x;\theta_i)$ to the running cost in the loss function by evaluating
\[
C(t_{i},\hat{X}^j_{t_i},\phi(t_{i},\hat{X}^j_{t_{i}};\theta))\Delta t
\]
We call this step \emph{Loss$(\phi,x)$}, as shown in Figure~\ref{fig:pgm_unit}. In line 6, when $t_i=T$, we add the contribution is $g(\hat{X}^j_T)$ to the total loss. Next steps of the algorithm are computing $\nabla_\theta \overline{\mathcal{J}}(\theta_{i})$ and adjusting the value of $theta_i$ accordingly. 

It is important to note that deep learning packages such as \verb|torch| or \verb|tensorflow|, construct $\nabla_\theta \overline{\mathcal{J}}(\theta_{i})$ using chain rule only once and update it at each iteration of the algorithm. 

\begin{ex}
    Solve the following linear quadratic problem via PGM and compare the PGM to the closed-form solution.
    \[
    \inf_u\mathbb{E}\left[\int_0^1\left(X_t^2+X_t+u_t^2\right)\dt + X_1^2 -X_1\right]
    \]
    with
    \[
    dX_t=(X_t-u_t)\dt+\dW_t
    \]
Recall that the closed-form solution is obtained by writing the HJB equation, similar to Example~\ref{eg:LQ_drift}, and verifying that the value function takes the form 
\[
V(t,x)=f(t)x^2+h(t)x+\ell(t)
\]
In this process we find ODEs for the coefficients $f$, $h$, and $\ell$ which we need to solve.
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Feynman-Kac approximation of linear equations}
\label{sec:F-K_approx}
While not related to control, it is important to draw an analogy between the BSDE approximation of value function in a control problem and the Feynman-Kac representation of the value of linear parabolic equations. Since the value function of a control problem satisfies an HJB equation, the nonlinear version of a parabolic equation, BSDE serves as a generalization of Feynman-Kac formula to nonlinear problems.

One can use the expected value in \eqref{eqn:F-K} to approximate the solution to \eqref{eqn:PDE_linear}. We generate the samples of $X$ by the \emph{Euler-Maruyama} discretization. Set $\Delta t = T/N$ and $t_n=n\Delta t$ and generate $J$ samples of increments of Wiener process $\Delta W_{t_{n+1}}^j=W_{t_{n+1}}^j-W_{t_{n}}^j\sim N(0,\Delta t)$
\begin{equation}
\label{eqn:Euler-Maruyama}
    \hat{X}^j_{t_{n+1}} = \hat{X}^j_{t_{n}} + b(t_n,\hat{X}_{t_n}^j)\Delta t+\sigma(t_n,\hat{X}_{t_n}^j)\Delta W_{t_{n+1}}^j
\end{equation}
Then, we can approximate $V(0,x)$ by
\[
\mathbb{E}\left[\int_0^T C(r,X_r){{\dr}}+g(X_T)\right]\approx\frac{1}{J}\Big(\sum_{n=0}^{N-1} C(t_n,\hat{X}_{t_n}^j)\Delta t+g(\hat{X}_T^j)\Big)
\]
\begin{algorithm}
        % Algorithm content goes here
        \KwData{$T$, $N$, $X_0^j\sim\mathcal{L}_0$, $W_{t_{n+1}}^j\sim N(0,\Delta t)$, $\phi(\cdot,\theta)$}
        $\Delta t \gets T/N$\;
        \For{$n=0,...,N-1$, $j=0,...,J$}{
        Evaluate $\hat{X}^j_{t_{n+1}}$ using \eqref{eqn:Euler-Maruyama}\;
        }
        $\hat{Y}^j:=\sum_{n=0}^{N-1}C(t_n,\hat{X}_{t_n}^j)\Delta t+g(\hat{X}_T^j)$\;
        Mean-square minimization: $\theta^*=\text{argmin}\frac{1}{J} \Big(\phi(X_0^j;\theta)-\hat{Y}^j\Big)^2$\;
        \Return{$\phi(x;\theta^*)$}
        \caption{Feynman-Kac approximation for linear parabolic equations}
        \label{alg:F-K}
\end{algorithm}
The initial value of $X_0^j$ can be taken as a given point $x$ or be generated according to an arbitrary distribution, $\mathcal{L}_0$. 
If $X_0^j$ is generated by a distribution, in Algorithm~\ref{alg:F-K}, we need approximate $V(0,x)$ by a parametrized function $\phi(\cdot;\theta)$ by minimizing a mean-square error. This minimization is performed in Step 5 in  Algorithm~\ref{alg:F-K}.


Algorithm~\ref{alg:F-K} only approximates the value function. If we write the BSDE corresponding to \eqref{eqn:PDE_linear}, we can approximate both the value function and its gradient simultaneously. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Numerical methods based on BSDE}
\label{eqn:BSDE_numerical}
Consider the HJB
\begin{equation}
    \begin{cases}
    V_t(t,x)+\frac12(\sigma^{\intercal}\sigma)(t,x)\cdot D^2 V(t,x) + H(t,x,\nabla V(t,x)) = 0\\
    V(T,x) = g(X_T)
    \end{cases}
\end{equation}
and the corresponding BSDE
\begin{equation}
    \begin{cases}
        \dd Y_t = -H(t,{X}_t,Z_t)\dt + Z_t\cdot\sigma(t,X_t)\dW_t\\
        Y_T=g({X}_T)
    \end{cases}
\end{equation}
By Theorem~\eqref{thm:BSDE}, we know that $V(t,X_t) = Y_t$ and $\nabla V(t,X_t) = Z_t\sigma^{-1}(t,X_t)$, where $\dd X_t=\sigma(t,X_t)\dW_t$. 
Consider the discretization of $X$ given by the Euler-Maruyama scheme:
\[
\hat{X}_{t_{n+1}}=\hat{X}_{t_{n}} + \sigma(t_n,\hat{X}_{t_{n}})\Delta W_{t_{n+1}}
\]
where $\Delta W_{t_{n+1}}= W_{t_{n+1}}- W_{t_{n}}$.
We also discretize the BSDE:
\begin{equation}
\label{eqn:BSDE_discrete}
    \begin{cases}
        \hat{Y}_{t_n} = \hat{Y}_{t_{n+1}} +H(t_n,\hat{X}_{t_n},\hat{Z}_{t_n})\Delta t - \hat{Z}_{t_n}\cdot\sigma(t_n,\hat{X}_{t_n})\Delta W_{t_{n+1}}\\
        \hat{Y}_T=g(\hat{X}_T)
    \end{cases}
\end{equation}
In \eqref{eqn:BSDE_discrete}, if we take conditional expectation given $\hat{X}_{t_n}$, we obtain 
$$\mathbb{E}[\hat{Z}_{t_n}\cdot\sigma(t_n,\hat{X}_{t_n})\Delta W_{t_{n+1}}|\hat{X}_{t_n}]=\hat{Z}_{t_n}\cdot\sigma(t_n,\hat{X}_{t_n})\mathbb{E}[\Delta W_{t_{n+1}}|\hat{X}_{t_n}]=0$$
and 
\begin{equation}
    \hat{Y}_{t_n} = \mathbb{E}[\hat{Y}_{t_{n+1}}|\hat{X}_{t_n}] +H(t_n,\hat{X}_{t_n},\hat{Z}_{t_n})\Delta t
\end{equation}
Furthermore, if we multiply both sides on \eqref{eqn:BSDE_discrete} by $\Delta W_{t_{n+1}}$ and take conditional expectation given $\hat{X}_{t_n}$, we have
\begin{equation}
\begin{split}
    \hat{Y}_{t_n} \mathbb{E}[\Delta W_{t_{n+1}}|\hat{X}_{t_n}]= &\mathbb{E}[\hat{Y}_{t_{n+1}}\Delta W_{t_{n+1}}|\hat{X}_{t_n}] + H(t_n,\hat{X}_{t_n},\hat{Z}_{t_n})\mathbb{E}[\Delta W_{t_{n+1}}|\hat{X}_{t_n}]\Delta t \\
    &\hspace{1cm}- \hat{Z}_{t_n} \sigma^{-1}(t,\hat{X}_t)\mathbb{E}[\Delta W_{t_{n+1}}^\intercal\Delta W_{t_{n+1}}|\hat{X}_{t_n}]
\end{split}
\end{equation}
In the above, $\Delta W_{t_{n+1}}^\intercal\Delta W_{t_{n+1}}$ is a $d\times d$ matrix obtained from multiplying a column vector to a row vector. 
\[
\Delta W_{t_{n+1}}^\intercal\Delta W_{t_{n+1}}
=\left[
\begin{matrix}
    \Delta W_{1,t_{n+1}}^2&\Delta W_{1,t_{n+1}}\Delta W_{2,t_{n+1}}&\cdots &\Delta W_{1,t_{n+1}}\Delta W_{d,t_{n+1}}\\
    &&\vdots&\\
    \Delta W_{d,t_{n+1}}\Delta W_{1,t_{n+1}}&\Delta W_{d,t_{n+1}}\Delta W_{2,t_{n+1}}&\cdots &\Delta W_{d,t_{n+1}}^2
\end{matrix}
\right]
\]
Since $\mathbb{E}[\Delta W_{t_{n+1}}|\hat{X}_{t_n}]=0$, we have 
\begin{equation}
    0= \mathbb{E}[\hat{Y}_{t_{n+1}}\Delta W_{t_{n+1}}|\hat{X}_{t_n}] - \hat{Z}_{t_n}\sigma(t_n,\hat{X}_{t_n}) \mathbb{E}[\Delta W_{t_{n+1}}^\intercal\Delta W_{t_{n+1}}|\hat{X}_{t_n}]
\end{equation}
Note that 
\[
\mathbb{E}[\Delta W_{i,t_{n+1}}\Delta W_{j,t_{n+1}}|\hat{X}_{t_n}]=\begin{cases}
    0&i\neq j\\
    \Delta t & i=j
\end{cases}
\]
Then, 
\begin{equation}
    \hat{Z}_{t_n}= \frac{1}{\Delta t}\sigma^{-1}(t_n,\hat{X}_{t_n}) \mathbb{E}[\hat{Y}_{t_{n+1}}\Delta W_{t_{n+1}}|\hat{X}_{t_n}]   
\end{equation}
The numerical scheme for BSDE is given by 
\begin{equation}
\label{eqn:BSDE_discrete_Z}
    \begin{cases}
    \hat{X}_{t_{n+1}}=\hat{X}_{t_{n}} + \sigma(t_n,\hat{X}_{t_{n}})\Delta W_{t_{n+1}}\\
    \hat{Z}_{t_n}= \frac{1}{\Delta t}\sigma^{-1}(t_n,\hat{X}_{t_n}) \mathbb{E}[\hat{Y}_{t_{n+1}}\Delta W_{t_{n+1}}|\hat{X}_{t_n}] \\
        \hat{Y}_{t_n} = \mathbb{E}[\hat{Y}_{t_{n+1}}|\hat{X}_{t_n}] +H(t_n,\hat{X}_{t_n},\hat{Z}_{t_n})\Delta t\\
        \hat{Y}_T=g(\hat{X}_T)
    \end{cases}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deep BSDE}
One of the most recent advancement in the numerical methods based on BSDE was achieved by \citet{HJE18}, where they introduced a method based on deep neural networks to solve the BSDEs.
Consider the BSDE
\begin{equation}
    \begin{cases}
        \dd Y_t = -H(t,{X}_t,Z_t)\dt + Z_t\cdot\sigma(t,X_t)\dW_t\\
        Y_T=g({X}_T)
    \end{cases}
\end{equation}
Given two functions $\psi(t,x)$ and $phi(x)$, we can write the BSDE as a forward process
\begin{equation}
    \begin{cases}
        \dd \tilde{Y}^{\phi,\psi}_t = -H(t,{X}_t,\psi(t,X_t))\dt + \psi(t,X_t)\cdot\sigma(t,X_t)\dW_t\\
        \tilde{Y}^{\phi,\psi}_0= \phi(X_0)
    \end{cases}
\end{equation}
If $\psi(t,x)= \nabla V(t,x)$ and $Y_0=V(0,x)$, then we must have $Y=\tilde{Y}^{\phi,\psi}$ and $Z_t=\nabla V(t,X_t)=\psi(t,X_t)$ and the terminal condition is satisfied $\tilde{Y}^{\phi,\psi}_T=V(T,X_T)=g(X_T)$. Therefore, solving the BSDE is equivalent to finding the minimizer of
\[
\inf_{\phi(x),\psi(t,x)}\mathbb{E}\left[\left(\tilde{Y}^{\phi,\psi}_T-g(X_T)\right)^2\right]
\]
To find the minimizer approximately, we discretize 
\[
\hat{X}_{t_{n+1}}=\hat{X}_{t_{n}} + \sigma(t_n,\hat{X}_{t_{n}})\Delta W_{t_{n+1}}
\]
and 
\begin{equation}
    \begin{cases}
        \hat{Y}^{\phi,\psi}_{t_{n+1}} = \hat{Y}^{\phi,\psi}_{t_{n}} -H(t_n,{X}_{t_{n}},\psi(t_n,{X}_{t_{n}}))\Delta t + \psi(t_n,{X}_{t_{n}})\cdot\sigma(t_n,{X}_{t_{n}})(W_{t_{n+1}}-W_{t_{n}})\\
        \hat{Y}^{\phi,\psi}_0= \phi(X_0)
    \end{cases}
\end{equation}
and minimize
\[
\inf_{\phi(x),\psi(t,x)}\mathbb{E}\left[\left(\hat{Y}^{\phi,\psi}_T-g(\hat{X}_T)\right)^2\right]
\]
The above minimization problem is over an infinite dimensional space of functions $(\phi(x),\psi(t,x))$. To make it practical, we reduce it to an optimization problem over a parametrized set of functions:
\[
\inf_{\theta}\mathbb{E}\left[\left(\hat{Y}^{\theta}_T-g(\hat{X}_T)\right)^2\right]
\]
where
\begin{equation}
    \begin{cases}
        \hat{Y}^{\theta}_{t_{n+1}} = \hat{Y}^{\theta}_{t_{n}} -H(t_n,{X}_{t_{n+1}},\psi(t_n,{X}_{t_{n+1}};\theta))\Delta t + \psi(t_n,{X}_{t_{n+1}}\theta)\cdot\sigma(t_n,{X}_{t_{n+1}})(W_{t_{n+1}}-W_{t_{n}})\\
        \hat{Y}^{\theta}_0= \phi(X_0;\theta)
    \end{cases}
\end{equation}
Furthermore, we evaluate it empirically by generating $J$ sample paths of $\hat{X}$:
\[
\inf_{\theta}\sum_{i=1}^{J}\left[\left(\hat{Y}^{j,\theta}_T-g(\hat{X}^{j}_T)\right)^2\right]
\]
where
\begin{equation}
    \begin{cases}
        \hat{Y}^{j,\theta}_{t_{n+1}} = \hat{Y}^{j,\theta}_{t_{n}} -H(t_n,{X}_{t_{n+1}},\psi(t_n,{X}^{j}_{t_{n+1}};\theta))\Delta t + \psi(t_n,{X}^{j}_{t_{n+1}}\theta)\cdot\sigma(t_n,{X}^{j}_{t_{n+1}})(W^{j}_{t_{n+1}}-W^{j}_{t_{n}})\\
        \hat{Y}^{j,\theta}_0= \phi(X^{j}_0;\theta)
    \end{cases}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Old old stuff Stochastic Control}
% We explain the components of a stochastic control problem.

% \begin{ex}[Merton problem]
%     In the same setting as \ref{eg:admissibility_paradox}, consider
%     \begin{equation}\label{prob:merton}
%         V(x) = \sup_{u}\mathbb{E}[U(X_T^u)|X^u_0=x]
%     \end{equation}
%     where $U:\mathbb{R}\to\mathbb{R}$ is a differentiable strictly concave strictly increasing function such that $U^\prime(\infty)=0$, e.g., $U(x)=1-e^{-\alpha x}$, $U(x) = \frac{\ln \alpha x}{\alpha}$, or $U(x) = \frac{x^{1-\alpha}}{1-\alpha}$ for $\alpha\in(0,1)$.
%     Does the same paradox as in St. Petersburg occurs here?
%     Does the set of admissible controls $\mathcal{A}_C$ solves the issue?
% \end{ex}






%%%%%%%%%%%%%%%
% \section{Discrete-time dynamic programming principle (DPP)}
% Recall from Corollary~\ref{coro:Markov_value_function} that the value function satisfies
% \begin{equation}\label{eqn:value_function}
%     V(t,x) =\inf_{u\in\mathcal{A}_{t,T}}\mathbb{E}\left[\int_{t}^{T}C(s,X_s^u,u_s){{\ds}}+g(X_T^u)\Big|X^u_t=x\right]
% \end{equation}
% Dynamic programming principle can be better understood in discrete-time setting. So, here we spend some time to explain a stochastic control problem in discrete time. It is natural to expect that, under suitable conditions, any continuous-time problem can be approximated by a discrete-time problem. For instance, \eqref{problem:generic} can be approximated by 
% \begin{equation}
%     \inf_{u} \mathbb{E}\bigg[\sum_{t=0}^{T-1}C(t,\hat{X}^u_t,u_t)\Delta t+g(\hat{X}^u_T)\bigg]
% \end{equation}
% where 
% \begin{equation}
%     \hat{X}^u_{t+1} =\hat{X}^u_t+\mu(t,\hat{X}^u_t,u_t)\Delta t+\sigma(t,\hat{X}^u_t,u_t)\Delta B_{t+1}
% \end{equation}
% where $\Delta t=T/N$ and $\Delta B_{t+1}=B_{t+\Delta t}-B_{t}$.

% Without loss of generality, we can drop $\Delta t$ and replace $\Delta B_{t+1}$ by a standard i.i.d random variable.
% \begin{equation}\label{prob:discrete_optimal_control}
%     \inf_{u} \mathbb{E}\bigg[\sum_{t=0}^{T-1}C(t,X^u_t,u_t)+g(X^u_T)\bigg]
% \end{equation}
% where the infimum is over all stochastic process $u:[0,T]\times\Omega\to\mathbb{R}^m$ and 
% \begin{equation}
%     \label{eqn:discrete_state}
%     X^u_{t+1} =X^u_t+\mu(t,X^u_t,u_t)+\sigma(t,X^u_t,u_t)\xi_{t+1}
% \end{equation}
% and $\{\xi_t\}_{t=1}^T$ is a sequence of i.i.d. random variables with mean $0$ and variance $1$.

% Note that we can think about the process as a sequence of random variables $u_0,...,u_{T-1}$, which can be chosen based on the discretion of the controller up to the adaptedness condition. This allows us to write the control problem as
% \begin{equation}
%     \inf_{u_0}\cdots\inf_{u_{T-1}} \mathbb{E}\bigg[\sum_{t=0}^{T-1}C(t,X^u_t,u_t)+g(X^u_T)\bigg]
% \end{equation}
% The value function for this problem is written as 
% \begin{equation}
%     V(t,x)= \inf_{u_t}\cdots\inf_{u_{T-1}} \mathbb{E}\bigg[\sum_{s=t}^{T-1}C(s,X^u_s,u_s)+g(X^u_T)\Big| X^u_t=x\bigg]
% \end{equation}
% Since $X^u_t=x$, $u_t$ is chosen over all real numbers and, therefore, $C(t,X^u_t,u_t)$ is deterministic. One can write
% \begin{equation}
%     V(t,x)= \inf_{u_t}
%     C(t,x,u_t) +\inf_{u_{t+1}}\cdots\inf_{u_{T-1}} \mathbb{E}\bigg[\sum_{s=t+1}^{T-1}C(s,X^u_s,u_s)+g(X^u_T)\Big| X^u_t=x\bigg]
% \end{equation}
% Tower property of conditional expectation implies that 
% \begin{equation}
%     \mathbb{E} \bigg[\sum_{s=t+1}^{T-1}C(s,X^u_s,u_s)+g(X^u_T)\Big| X^u_t=x\bigg]= \mathbb{E}\bigg[\mathbb{E}\Big[\sum_{s=t+1}^{T-1}C(s,X^u_s,u_s)+g(X^u_T)\Big|X^u_{t+1}\Big]\Big| X^u_t=x\bigg]
% \end{equation}
% \begin{ex}
%     Show that one can take the infimum inside the first expectation, i.e.,
%     \begin{equation}
%     \begin{split}
%     \inf_{u_{t+1},...,u_{T-1}} &\mathbb{E} \bigg[\mathbb{E}\Big[\sum_{s=t+1}^{T-1}C(s,X^u_s,u_s)+g(X^u_T)\Big|X^u_{t+1}\Big]\Big| X^u_t=x\bigg]\\
%     &= \mathbb{E} \bigg[\inf_{u_{t+1},...,u_{T-1}} \mathbb{E}\Big[\sum_{s=t+1}^{T-1} C(s,X^u_s,u_s)+g(X^u_T)\Big|X^u_{t+1}\Big]\Big| X^u_t=x\bigg]
%     \end{split}
% \end{equation}
% \end{ex}
% By definition of value function, 
% \begin{equation}
%     \inf_{u_{t+1},...,u_{T-1}} \mathbb{E}\Big[\sum_{s=t+1}^{T-1} C(s,X^u_s,u_s)+g(X^u_T)\Big|X^u_{t+1}\Big]=V(t+1,X^u_{t+1})
% \end{equation}
% Therefore,
% \begin{equation}\label{one-step_DPP}
%     V(t,x)= \inf_{u_t}
%     C(t,x,u_t) + \mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]
% \end{equation}
% This provides us with a recursive formula to solve a discrete-time control problem.
% \begin{equation}\label{eqn:dpp_discrete}
%     \begin{cases}
%         V(t,x)= \inf_{u_t}
%     C(t,x,u_t) + \mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]\\
%     V(T,x)=g(x)
%     \end{cases}
% \end{equation}




%%%%%%%%%%%%%%%
% \section{Hamilton-Jacobi-Bellman equation}
% In Theorem~\ref{thm:stoch_dpp_no_stopping}, for fixed $h,\epsilon>0$ take $\tau=\tau^h=\inf\{s>0, |X_s^u-x|\ge \epsilon\}\wedge (t+h)$. Assume that the value function is $C^{1,2}$, once continuously differentiable on $t$ and twice continuously differentiable on $x$.By applying It\^o's formula on $V({\tau^h},X_{\tau^h}^u)$, we obtain
% \begin{equation}
% \begin{split}
%     V({\tau^h},X_{\tau^h}^u)=&V(t,x)\\
%     &+\int_t^{\tau^h} \Big({V}_{t}(s,X_s^u)+\frac12 D^2V(s,X_s^u) \cdot a(s,X_s^u,u_s) + \nabla V(s,X_s^u)\cdot \mu(s,X_s^u,u_s)\Big){{\ds}}\\
%     &+\int_t^{\tau^h} \sigma(s,X_s^u,u_s)\dW_s
% \end{split}
% \end{equation}
% where $a=\sigma^\intercal\sigma$. Recall  for two matrices $A\cdot B=\textrm{Tr}[A^\intercal B]$.  Assuming $\mathbb{E}_{t,x}\left[\int_0^{\tau^h} \sigma(s,X_s^u,u_s)\dW_s\right]=0$\footnote{One can always choose $\epsilon$ in the definition of ${\tau^h}$ such that the stochastic integral has zero expected value.}, \eqref{eqn:dpp} can be written as
% \begin{equation}
%     \begin{split}
%         0 =&\inf_{u}\mathbb{E}_{t,x}\Big[\int_t^{\tau^h} \Big( C(s,X^u_s,u_s){{\ds}} \\
%         & {V}_{t}(s,X_s^u)+\frac12 D^2V(s,X_s^u) \cdot a(s,X_s^u,u_s) + \nabla V(s,X_s^u)\cdot \mu(s,X_s^u,u_s)\Big){{\ds}}\Big]
%     \end{split}
% \end{equation}
% Note that ${\tau^h}=O(h)$. Therefore, we divide by $h$ and send $h\to0$:
% \begin{equation}
%     \begin{split}
%         0 =&\lim_{h\to0}\inf_{u}\mathbb{E}_{t,x}\Big[\frac1h\int_t^{\tau^h} \Big( C(s,X^u_s,u_s){{\ds}} \\
%         & {V}_{t}(s,X_s^u)+\frac12 D^2V(s,X_s^u) \cdot a(s,X_s^u,u_s) + \nabla V(s,X_s^u)\cdot \mu(s,X_s^u,u_s)\Big){{\ds}}\Big]\\
%         =&\inf_{u}\lim_{h\to0} \mathbb{E}_{t,x}\Big[\frac1h\int_t^{\tau^h} \Big( C(s,X^u_s,u_s){{\ds}} \\
%         & {V}_{t}(s,X_s^u)+\frac12 D^2V(s,X_s^u) \cdot a(s,X_s^u,u_s) + \nabla V(s,X_s^u)\cdot \mu(s,X_s^u,u_s)\Big){{\ds}}\Big]\\
%         =&\inf_{u} \mathbb{E}_{t,x}\Big[\lim_{h\to0}\frac1h\int_t^{\tau^h} \Big( C(s,X^u_s,u_s){{\ds}} \\
%         & {V}_{t}(s,X_s^u)+\frac12 D^2V(s,X_s^u) \cdot a(s,X_s^u,u_s) + \nabla V(s,X_s^u)\cdot \mu(s,X_s^u,u_s)\Big){{\ds}}\Big]\\
%         =&{V}_{t}(t,x)+\inf_{u}\Big\{C(t,x,u) +
%          \frac12 D^2V(t,x) \cdot a(t,x,u) + \nabla V(t,x)\cdot \mu(t,x,u)\Big\}
%     \end{split}
% \end{equation}
% The HJB is given by the following PDE:
% \begin{equation}
%     \begin{cases}
%         0={V}_{t}(t,x)+\inf_{u}\Big\{C(t,x,u) +
%          \frac12 D^2V(t,x) \cdot a(t,x,u) + \nabla V(t,x)\cdot \mu(t,x,u)\Big\}\\
%          V(T,x)=g(x)
%     \end{cases}
% \end{equation}
% \begin{rem}
%     Rigorously, the proof is more complected. First, we need to justify $\lim_{h\to0}\inf_{u}=\inf_{u}\lim_{h\to0}$. To show this, we have to show that the expected value as a function of $h$ is continuous uniformly on $u$. This can be obtained by continuity of $C$, $V$, and first and second derivatives of $V$. Second, justification of $\lim_{h\to0}\mathbb{E}=\mathbb{E}\lim_{h\to0}$ requires dominated convergence theorem, which allows to change the order of expected value and limit. This also requires use of mean value theorem. For more information, see \cite{Touzi12}.
% \end{rem}

\begin{ex}\label{ex:LQ_diffusion}
Solve modified version of Exercise~\ref{eg:LQ_drift} with modification 
\begin{equation}
        {\dX}_t=(cX_t+du_t){{\dt}}+(eX_t+fu_t) \dW_t
    \end{equation}
\end{ex}
\begin{ex}
    Extend the stochastic linear quadratic problem in Exercise~\ref{ex:LQ_diffusion} to higher dimension and write the HJB.
\end{ex}
\begin{eg}
    [Merton optimal investment problem]{}
    Remember a self-financing portfolio with a 
    Black-Scholes risky asset, 
    ${{\dd}}S_t=S_t(\mu {{\dt}}+\sigma \dW_t)$, 
    under zero interest rate  satisfies
    \begin{equation}
        {\dX}_t=\theta_t(\mu {{\dt}}+ \sigma \dW_t)
    \end{equation}
    where $\theta$ is the amount of money invested in the risky asset.
    Merton problem is to maximize the expected utility of wealth at a time horizon $T$:
    \begin{equation}
        \sup_{\theta}\mathbb{E}[U(X_T^\theta)]
    \end{equation}
    The HJB for this problem is given by 
    \begin{equation}
        \begin{cases}
            0={V}_{t} + \sup_{\theta}\Big\{
            \frac{\sigma^2}{2}V_{xx} +\mu  V_{x}\Big\}\\
            V(T,x)=U(x)
        \end{cases}
    \end{equation}
    After simplification, we have 
        \begin{equation}
        \begin{cases}
            0={V}_{t} -\frac{\mu^2 ( V_{x})^2}{2\sigma^2V_{xx}}\\
            V(T,x)=U(x)
        \end{cases}
    \end{equation}
    We can find a closed form solution for the following cases of utility by using separation of variables, $V(t,x)=f(t)U(x)$:
    \begin{enumerate}
        \item Exponential utility $U(x)=1-e^{-\lambda x}$. The separation of variables is $V(t,x)=-f(t)e^{-\lambda x}$.
        \item HARA utility $U(x)=\frac{x^{1-\lambda}}{1-\lambda}$, with $\lambda\in(0,1)$.
    \end{enumerate}
\end{eg}

\begin{ex}
    [Merton optimal consumption problem]{}
    It is similar to the Merton optimal investment problem except, the investor is consuming from the account and what matters is the utility of consumption.
    \begin{equation}
        {\dX}_t=\theta_t(\mu {{\dt}}+ \sigma \dW_t)-c_t{{\dt}}
    \end{equation}
    where $\theta$ is the amount of money invested in the risky asset and the consumption $c$ satisfies $c_t\ge0$.
    Merton problem is 
    \begin{equation}
        \sup_{c\ge0,~\theta}\mathbb{E}\Big[\int_0^\infty e^{-\gamma t}U(c_t){\dt}\Big], ~~\gamma>0
    \end{equation}
    Write the HJB for this problem.
\end{ex}
\begin{rem}
    $\gamma>0$ in the above example represents preference of current consumption over future consumption.
\end{rem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Verification}\label{sec:verification}
% Instead of going through viscosity solutions, we consider the cases where the HJB equation has a $C^{1,2}$ solution, which is a candidate for the value function. Then, we introduce a \emph{verification theorem}, which approves that the solution is indeed the value function.
% \begin{thm}[Verification]\label{thm:verification_old}
%     Let HJB equation, \eqref{eqn:HJB} has a $C^{1,2}$ solution, $v(t,x)$. Then, $v(t,x)\ge V(t,x)$.\\
% In addition, assume that there exists $u^*(t,x)$ such that 
% \begin{equation}
%         u^*(t,x)\in\textrm{argmin}_{u}\Big\{C(t,x,u) +
%          \frac12 D^2v(t,x) \cdot a(t,x,u) + \nabla v(t,x)\cdot \mu(t,x,u)\Big\}
% \end{equation}
% and $u^*(t,x)$ is an admissible Markovian (feedback) control, i.e., \eqref{eqn:state_Markov_control} has a (strong) solution, $X^*_t$, and $u^*_t=u^*(t, X^*_t)$ is admissible, $u^*\in\mathcal{A}$. Then, $v(t,x)=V(t,x)$.
% \end{thm}
% \begin{proof}
% Since $v$ satisfies \eqref{eqn:HJB} in classical sense, we conclude that for any value of $u\in U$, we have 
% \begin{equation}\label{eqn:subsol_thm_verif}
% 0\ge\partial_{t}v(t,x)+C(t,x,u)+\mu(t,x,u)\nabla v(t,x)+\frac12{a(t,x,u)}\cdot D^2v(t,x) 
% \end{equation}
% Since $v(t,x)\in\mathrm{C}^{1,2}$, it follows from the It\^o formula that for any $u\in\mathcal{A}_{t,T}$, 
% \begin{equation}
% \begin{split}
% v&(T,X_T^{t,x,u})=v(t,x)\\
% &+\int_0^T\Big(\partial_{t}v(s,X_s^{t,x,u})+\mu(s,X_s^{t,x,u},u_s)\cdot\nabla v(s,X_s^{t,x,u})+\frac12{a(s,X_s^{t,x,u},u_s)}\cdot D^2v(s,X_s^{t,x,u})\Big){{\ds}}\\
% &+\int_t^T\sigma(s,X_s^{t,x,u},u_s)\dW_s
% \end{split}
% \end{equation}
% By \eqref{eqn:subsol_thm_verif} and terminal condition $v(T,x)=g(x)$, we obtain
% \begin{equation}\label{eqn:911}
% g(X_T^u)\ge{v}(t,x)-\int_0^TC(s,X_s^{t,x,u},u_s){{\ds}}+\int_t^T\sigma(s,X_s^{t,x,u},u_s)\dW_s
% \end{equation}
% If we take conditional expectation, the martingale property of the stochastic integral implies that
% \begin{equation}
% \mathbb{E}_{t,x}\bigg[g(X_T^u)+\int_0^TC(s,X_s^{t,x,u},u_s){{\ds}}\bigg]\ge{v}(t,x)
% \end{equation}
% Now, by taking supremum over $u\in\mathcal{A}_{t,T}$, we obtain
% \begin{equation}
% V(t,x)=\sup_{u\in\mathcal{A}_{t,T}}\mathbb{E}_{t,x}\bigg[g(X_T^u)+\int_0^TC(s,X_s^{t,x,u},u_s){{\ds}}\bigg]\ge{v}(t,x).
% \end{equation}\label{eqn:913}
% Now, assume that  for some $u^*(t,X^*_t)\in\mathcal{A}$, $Y^{u^*}_t=v(t,X_t^{u^*})+\int_0^t C(s,X_s^{u^*},u^*_s) {{\ds}}$ is a martingale. By applying the It\^o formula, we have
% \begin{equation}
% \begin{split}
% Y^{u^*}_t=&v(t,x)+\int_t^T\sigma(s,X_s^{t,x,u^*},u_s)\dW_s+\int_0^T\Big(\partial_{t}v(s,X_s^{t,x,u^*})+\mu(s,X_s^{t,x,u^*},u^*_s)\cdot \nabla v(s,X_s^{t,x,u^*})\\
% &\hspace*{4cm}+\frac12{a(s,X_s^{t,x,u^*},u^*_s)}\cdot D^2v(s,X_s^{t,x,u^*})+C(s,X_s^{u^*},u^*_s)\Big){{\ds}}\\
% =&v(t,x)+\int_t^T\sigma(s,X_s^{t,x,u^*},u_s)\dW_s
% \end{split}
% \end{equation}
% Therefore, the martingale property of $Y^{u^*}$, implies that
% \begin{equation}
% \begin{split}
% C(s,X_s^{u^*},u^*_s)+&\partial_{t}v(s,X_s^{t,x,u^*})+\mu(s,X_s^{t,x,u^*},u^*_s)\cdot\nabla v(s,X_s^{t,x,u^*})\\
% +&\frac12{a(s,X_s^{t,x,u^*},u^*_s)}\cdot D^2v(s,X_s^{t,x,u^*})=0,~~~\textrm{a.s.}
% \end{split}
% \end{equation}
% Therefore, for $u^*$, all the inequalities in \eqref{eqn:911}-\eqref{eqn:913} holds as equality and we obtain $v=V$.
% \end{proof}
% We apply Theorem~\eqref{thm:verification} to the following examples.
% \begin{ex}
%     [Merton optimal investment problem]{}
%     Remember a self-financing portfolio with a 
%     Black-Scholes risky asset, 
%     ${{\dd}}S_t=S_t(\mu {{\dt}}+\sigma \dW_t)$, 
%     under zero interest rate  satisfies
%     \begin{equation}
%         {\dX}_t=\theta_t(\mu {{\dt}}+ \sigma \dW_t)
%     \end{equation}
%     where $\theta$ is the amount of money invested in the risky asset.
%     Merton problem is 
%     \begin{equation}
%         \sup_{\theta}\mathbb{E}[U(X_T^\theta)]
%     \end{equation}
%      The HJB is given by
%      \begin{equation}
%          \begin{cases}
%              0={V}_{t}+ \sup_{\theta}\bigg\{\frac{\theta^2\sigma^2}{2}V_{xx}+\theta\mu  V_{x}\bigg\}\\
%              v(T,x)=U(x)
%          \end{cases}
%      \end{equation}
%      This simplifies to 
%           \begin{equation}\label{eqn:HJB_merton_investment}
%          \begin{cases}
%              0={V}_{t}-\frac{(\mu  V_{x})^2}{\sigma^2 V_{xx}}\\
%              v(T,x)=U(x)
%          \end{cases}
%      \end{equation}
%      For $U(x)=-e^{-\lambda x}$, with $\lambda>0$, use separation of variables $u(t,x)=-f(t)e^{-\lambda x}$ to find a closed form solution for the HJB.
% \end{ex}
% \begin{ex}
%     For $U(x)=x^\lambda$, use separation of variables $u(t,x)=f(t)x^\lambda$ to find a closed form solution for the HJB. Is there any restriction on the value of $\lambda$?
% \end{ex}
% \begin{ex}
%     For $U(x)=\ln x$, can you suggest a separation of variables?
% \end{ex}

% \begin{ex}
%     [Merton optimal consumption problem]{}
%     It is similar to the Merton optimal investment problem except, the investor is consuming from the account and what matters is the utility of consumption.
%     \begin{equation}
%         {\dX}_t=\theta_t(\mu {{\dt}}+ \sigma \dW_t)-c_t{{\dt}}
%     \end{equation}
%     where $\theta$ is the amount of money invested in the risky asset and $c$ is the consumption with $c_t\ge0$.
%     Merton problem is 
%     \begin{equation}
%         \sup_{\theta,c}\mathbb{E}\Big[\int_0^\infty e^{-\gamma t}U(c_t){\dt}\Big], ~\gamma>0
%     \end{equation}
%     Show that the HJB is given by
%     \begin{equation}
%         0= \sup_{\theta}\bigg\{\frac{\theta^2\sigma^2}{2}v^{\prime\prime}+\theta\mu v^\prime\bigg\}+\sup_{c\ge0}\Big\{U(c)-cv^\prime\Big\}-\gamma v
%     \end{equation}
%     or 
%     \begin{equation}\label{eqn:HJB_consumption}
%         0=\frac{(\mu v^\prime)^2}{\sigma^2 v^{\prime\prime}}+\sup_{c\ge0}\Big\{U(c)-cv^\prime\Big\}-\gamma v
%     \end{equation}
% For $U(c)=\frac{x^{1-\lambda}}{1-\lambda}$, with $\lambda\in(0,1)$, we have
% \begin{equation}
%     \sup_{c\ge0}\Big\{U(c)-cv^\prime\Big\}=U(c^*)-c^*v^\prime
% \end{equation}
% with $U^\prime(c^*)=v^\prime$ or $c^*$
% \end{ex}

%%%%%%%%%%%%%%%
% \subsection{Martingale approach}\label{sec:martingale}
% The martingale principle for optimal control (\cite{DV73}) is another verification result which does not require the differentiability of the value function.
% \begin{thm}\label{thm:martingale_verification}
% 	Assume that there exists a function $v(t,x)$ such that for all $u\in\mathcal{A}$ the process $\{Y^u_t\}_{t\ge0}$
% 	\begin{equation}
% 		Y^u_t:=v(t,X_t^u)+\int_0^tC(s,X_s^u,u_s){{\ds}}
% 	\end{equation}
% 	is a super martingale and that for some $u^*\in\mathcal{A}$, $\{Y^{u^*}_t\}_{t\ge0}$ is a martingale. Then, $u^*$ is an optimal control and the value function is equal to $v(t,x)$.
% \end{thm}
% \begin{proof}
% By the supermartingale property, we have
% \begin{equation}
% Y^u_t\le\mathbb{E}[Y^u_T|\mathcal{F}_t]=\mathbb{E}\Big[g(X_T^u)+\int_t^TC(s,X_s^u,u_s){{\ds}}\Big|\mathcal{F}_t]\Big]+\int_0^tC(s,X_s^u,u_s){{\ds}}
% \end{equation}
% Thus, 
% \begin{equation}
% v(t,x)=Y^u_t-\int_0^tC(s,X_s^u,u_s){{\ds}}\le\mathbb{E}[Y^u_T|\mathcal{F}_t]=\mathbb{E}\Big[g(X_T^u)+\int_t^TC(s,X_s^u,u_s){{\ds}}\Big|\mathcal{F}_t]\Big],
% \end{equation}
% for all $u\in\mathcal{A}_{t,T}$. Given $X_t^u=x$, we obtain
% \begin{equation}
% v(t,x)\le\sup_{u\in\mathcal{A}_{t,T}}\mathbb{E}\Big[g(X_T^u)+\int_t^TC(s,X_s^u,u_s){{\ds}}\Big|X_t^u=x]\Big]=V(t,x).
% \end{equation}
% If for some $u^*$, $Y^{u^*}$ is a martingale, then in all the above, the inequality turns into equality and we have $v(t,x)=V(t,x)$.
% %Since $V(t,x)\in\mathrm{C}^{1,2}$, we can apply It\^o formula to write 
% %\begin{equation}
% %\begin{split}
% %Y^u_t=&\int_0^t\Big(C(s,X_s^u,u_s)+\partial_{t}V(s,X_s^u)+\mu(s,X_s^u,u_s)\partial_{x}V(s,X_s^u)+\frac{\sigma^2}{2}(s,X_s^u,u_s)\partial_{xx}V(s,X_s^u) \Big){{\ds}}\\
% %&+\int_{t}^{\tau}\sigma(s,X_s^u,u_s)\partial_{x}V(s,X_s^u)\dW_t
% %\end{split}
% %\end{equation}
% \end{proof}
% The above theorem can also be regarded as a verification that a possible candidate $V(t,x)$ is a value function of the optimal control problem. The following example shows the use of this theorem.
% \begin{eg}\label{eg:martingle_consumption}
% In Example~\ref{eg:admissibility_paradox}, let $U(c)=\frac{c^{1-\alpha}}{1-\alpha}$ when $c\ge0$ and $-\infty$ otherwise. Consider the function $V(t,x)=e^{-\gamma (T-t)}(T-t+e^{A/\alpha t})^{\alpha}e^{-AT}U(x)$ with $A=r(1-\alpha)-\gamma+\frac{(\mu-r)(1-\alpha)}{2\sigma^2\alpha}$.
% 	\begin{equation}
% 		Y^u_t=V(t,X_t^u)+\int_0^te^{-\gamma s}U(c_s){{\ds}}.
% 	\end{equation}
% 	By It\^o formula, we have 
% 	\begin{equation}
% 	\begin{split}
% 				\dd Y_t^u=&\Big({V}_{t}+ \big(\theta_t(\mu-r)+rX^u_t-c_t\big) V_{x}+ 
% \frac12\sigma^2\theta_t^2\partial_{xx} V+e^{-\gamma t}U(c_t)\Big){{\dt}}\\
% 		&+\sigma\theta_t V_{x} \dW_t. 
% 	\end{split}
% 	\end{equation}
% 	By direct calculation, one can see that 
% 	\begin{equation}
% 		{V}_{t}+ \big(\theta_t(\mu-r)+rX^u_t-c_t\big) V_{x}
% 		+\frac12\sigma^2\theta_t^2\partial_{xx} V+e^{-\gamma t}U(c_t)\le0,~~\mathbb{P}\textrm{-a.s.}
% 	\end{equation}
% 	for all values of  $\theta$ and $c$. In addition, for $c^*_t(x)=( V_{x}(t,x))^{-1/\alpha}$ and $\theta^*_t(x)=-\frac{(\mu-r)\partial_xV(t,x)}{\sigma^2\partial_{xx}V(t,x)}$, 
% 	\begin{equation}
% 		{V}_{t}+ \big(\theta_t(\mu-r)+rX^u_t-c_t\big) V_{x}
% 		+\frac12\sigma^2\theta_t^2\partial_{xx} V+e^{-\gamma t}U(c_t)=0,~~\mathbb{P}\textrm{-a.s.}
% 	\end{equation}
% It remains to show that $c^*$ and $\theta^*$ are admissible Markov controls, i.e., to show that 
% 	\begin{equation}
% 		dX^*_t=\Big(\theta_t(X^*_t)\big((\mu-r) {{\dt}}+\sigma \dW_t\big)-rX^*_t{{\dt}}\Big)-c^*_t(X^*_t){{\dt}},
% 	\end{equation}
% 	has a strong solution. We leave the details as an exercise. 
% \end{eg}
\begin{rem}\label{rem:inf_submartingale}
If the control problem is with infimum instead of supremum,
\begin{equation}
V(t,x)=\inf_{u\in\mathcal{A}_{t,T}}\mathbb{E}\left[\int_{t}^{T}C(s,X_s^{t,x,u},u_s){{\dt}}+g(X_T^{t,x,u})\right],
\end{equation}
Theorem~\ref{thm:martingale_verification} is should be modified. More precisely, $Y_t^u$ is a submartingale. 

For deterministic cases, supermartingale (resp. submartingale) means nonincreasing (resp. nondecreasing).
\end{rem}
Finding a candidate for a value function and an optimal control is the subject of the future sections.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Backward stochastic differential equations}
% \label{sec:BSDE}
% The backward stochastic differential equations (BSDE) is an alternative way to tackle the optimal control problems. One advantage of the BSDEs is that they can  cover the nonMarkovian optimal control problems. The derivation of the BSDEs from an optimal control problem follows from the It\^o martingale representation theorem. 
% \begin{thm}
% In a probability space which hosts a Brownian motion $B$, let $\mathbb{F}^B:=\{\mathcal{F}^B_t\}_t$ be the right-continuous augmented filtration generated by a Brownian motion $B$. 
% Let $X$ be a $\mathcal{F}^B_T$-measurable random variable with finite expectation. Then, there exists a $\mathbb{F}^B$-progressively measurable process $Z_t(\omega)$ such that 
% \begin{equation}
% X=\mathbb{E}[X]+\int_0^TZ_t\dW_t
% \end{equation}

% In addition, if $M$ is a continuous martingale with respect to $\mathbb{F}$, then, there exists a $\mathbb{F}^B$-progressively measurable process $\phi_(\omega)$ such that 
% \begin{equation}
% M_t=M_0+\int_0^tZ_s\dW_s,~~~\textrm{for }~~~ t\ge0.
% \end{equation}
% \end{thm}
% In the above theorem, $Z$ can interpreted as the sensitivity of the martingale $M$ with respect to the Browanian noise $B$.

% Consider for a given stochastic process $C_t(\omega)$ 
% \begin{equation}
% Y_t:=\mathbb{E}\left[\int_t^T   L_s(\omega){{\ds}} + \xi \Big|\mathcal{F}^B_t\right].
% \end{equation}
% We simply assume that $r_s(\omega)$ and $L_s(\omega)$ are $\mathbb{F}^B$-progressively measurable processes and $\xi$ is a $\mathcal{F}_T^X$-measurable and integrable random variable. For the martingale define by 
% \begin{equation}
% M_t:=\mathbb{E}\left[\int_0^T  L_s(\omega){{\ds}}+\xi\Big|\mathcal{F}^B_t\right],
% \end{equation}
% and by the It\^o martingale representation theorem, we have
% \begin{equation}
% M_T=\int_0^T   L_s(\omega){{\ds}}+\xi=M_t+\int_t^TZ_s\dW_s,
% \end{equation}
% for some $\mathbb{F}^B$-progressively measurable process $Z_t(\omega)$.
% Since $Y_T=\xi$, we have 
% \begin{equation}
% M_T=\int_0^T   L_s(\omega){{\ds}}+Y_T=M_t+\int_t^TZ_s\dW_s,
% \end{equation}
% On the other hand,
% since $\int_0^t   L_s(\omega){{\ds}}$ is $\mathcal{F}^B_t$-measurable, we can write
% \begin{equation}
% M_t=\int_0^t   L_s(\omega){{\ds}}+\mathbb{E}\left[\int_t^T   L_s(\omega){{\ds}}+\xi\Big|\mathcal{F}^B_t\right]=\int_0^t   L_s(\omega){{\ds}}+Y_t.
% \end{equation}
% Therefore,
% \begin{equation}
% \int_0^T   L_s(\omega){{\ds}}+Y_T=\int_0^t   L_s(\omega){{\ds}}+Y_t+\int_t^TZ_s\dW_s,
% \end{equation}
% or,
% \begin{equation}\label{eqn:bsde_mg}
% Y_t=Y_T+\int_t^T   L_s(\omega){{\ds}}-\int_t^TZ_s\dW_s.
% \end{equation}
% The BSDE \eqref{eqn:bsde_mg} can be written formally by 
% \begin{equation}
% \begin{cases}
% \dd Y_t=- L(t,\omega){{\dt}}+Z_t\dW_t\\
% Y_T=g(X_T)
% \end{cases}
% \end{equation}
% Note that we could have written the above forwardly, i.e.,
% $Y_t=Y_0+\int_0^t   L_s(\omega){{\ds}}-\int_0^tZ_s\dW_s$. However, this is not very useful, because $Y_0$ is not known. See example below.
% \begin{eg}
% Recall that
% the solution to the linear equation 
% \begin{equation}
% \begin{cases}
% 		0 =\partial_{t}v(t,x)+C(t,x)+[\mu \cdot\nabla v](t,x)+\frac12[\sigma^\intercal\sigma \cdot D^2v](t,x)\\
% v(T,x)=g(x)
% \end{cases}		
% \end{equation}
% is given by the Feynman-Kac formula:
% \begin{equation}
% V(t,x)=\mathbb{E}\left[\int_t^T C(s,X_s){{\ds}}+g(X_T)\Big| X_t=x\right],
% \end{equation}
% where
% \begin{equation}
% {\dX}_t=\mu(t,X_t){{\dt}}+\sigma(t,X_t)\dW_t.
% \end{equation}
% If we define $Y_t=V(t,X_t)$, then \eqref{eqn:bsde_mg} is given by 
% \begin{equation}
% Y_t=g(X_T)+\int_t^T C(s,X_s){{\ds}}-\int_t^TZ_s\dW_s.
% \end{equation}
% In the above, $g$ and $L$ are known. Given that we can find $Z$, $Y_t$ is known. However, if we write the equation forward
% \begin{equation}
% Y_t=Y_0+\int_0^t C(s,X_s){{\ds}}-\int_0^tZ_s\dW_s.
% \end{equation}
% Here, $Y_0=V(0,X_0)$ is not known. 
% \end{eg}


% The following theorem shows that a BSDE can generalize the Feynman-Kac formula to nonlinear equations.
% \begin{thm}\label{thm:Markov_BSDE}
% Assume that the following \emph{semilinear}\footnote{The an equation with linear second order term and possibly nonlinear first order term is called semilinear.} PDE has a solution $V(t,x)\in\mathrm{C}^{1,2}$.
% \begin{equation}\label{eqn:semilinear_PDE}
% \begin{cases}
% 0=\partial_{t}v(t,x)+[\mu\cdot{\nabla}v](t,x)+\frac12[{\sigma^\intercal\sigma\cdot}D^2V](t,x)+C(t,x,v(t,x),\nabla v(t,x))\\
% V(T,x)=g(x).
% \end{cases}		
% \end{equation}
% Then, $Y_t=V(t,X_t)$ and  $Z_t=\sigma(t,X_t)\nabla V(t,X_t)$ satisfy the BSDE
% \begin{equation}\label{eqn:Markov_BSDE}
% \begin{cases}
% \dd Y_t=- C(t,X_t,Y_t,\sigma^{-1}(t,X_t)Z_t){{\dt}}+Z_t\dW_t\\
% Y_T=g(X_T)
% \end{cases}
% \end{equation}
% where 
% \begin{equation}
% {\dX}_t=\mu(t,X_t){{\dt}}+\sigma(t,X_t)\dW_t.
% \end{equation}
% \end{thm}
% \begin{proof}
% By applying It\^o lemma on $Y_t=V(t,X_t)$, we obtain
% \begin{equation}
% \begin{split}
% \dd Y_t=&\big(\partial_{t}V(t,X_t)+\mu(t,X_t)\nabla V(t,X_t)+\frac12{\sigma^2(t,X_t)}\partial_{xx}V(t,X_t)\big){{\dt}}+\sigma(t,X_t)\nabla V(t,X_t)\dW_t\\
% &=-C(t,X_t,V(t,X_t),\nabla V(t,X_t)){{\dt}}+\sigma(t,X_t)\nabla V(t,X_t)\dW_t\\
% &=- C(t,X_t,Y_t,\sigma^{-1}(t,X_t)Z_t){{\dt}}+Z_t\dW_t
% \end{split}
% \end{equation}
% \end{proof}

% In the above, the second equality is obtained from the PDE and the third equality is from the definition of $Y$ and $Z$. In addition, by the terminal condition we have, $Y_T=V(T,X_T)=g(X_T)$.
% In general, the use of BSDEs over the HJB is  preferable when the regularity of the function $V(t,x)$ is not established. Because by the existence theorem for the BSDEs, we already know that  $Z_t$ exists as stochastic process even if $\sigma(t,X_t)\nabla V(t,X_t)$ does not make sense in cases when $\nabla V$ does not exist as specific points.

% Motivated by Theorem~\ref{thm:Markov_BSDE}, we can define a general BSDE as 
% \begin{equation}\label{eqn:BSDE_old}
% 	\begin{cases}
% 		\dd Y_t=-C(t,Y_t,Z_t,\omega){{\dt}}+Z_s\dW_s\\
% 		Y_T=\xi
% 	\end{cases},
% \end{equation}
% The argument $\omega$ inside $L$ represents a general dependence on the randomness. In particular, it can represent solution $X_t$ of a possible path-dependent SDE
% \begin{equation}
% 	{\dX}_t=\mu(t,X_\cdot){{\dt}}+\sigma(t,X_\cdot)\dW_t,
% \end{equation}   
% a possible control process $u_t$, or both $X_t$ and $u_t$. We discuss such dependencies in further details in Section~\ref{sec:max_principle}.
% \begin{rem}
% 	In \eqref{eqn:BSDE}, we blended $\sigma^{-1}$ inside the Lagrangian $L(s,X_s,Y_s,\sigma^{-1}Z_s,\omega)$  into it and simply write $L(s,X_s,Y_s,Z_s,\omega)$.
% \end{rem}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Comparison principle}\label{sec:comp_principle}
% Consider 
% \begin{equation}\label{eqn:comp_principle}
% 	\begin{cases}
% 		dY^{(i)}_t=-L^{(i)}(t,Y^{(i)}_t,Z^{(i)}_t,\omega){{\dt}}+Z^{(i)}_s\dW_s\\
% 		Y^{(i)}_T=\xi^{(i)}
% 	\end{cases},
% \end{equation}
% for $i=1,~2$. The following theorem provides a sufficient condition for comparing $Y^{(1)}$ and $Y^{(2)}$, and hence, it is called comparison principle.
% \begin{thm}\label{thm:comp_principle}
% Assume that for $i=1,~2$, $(Y^{(i)},Z^{(i)})$ is the solution for \eqref{eqn:comp_principle} and $L(t,y,z,\omega)$ is Lipschitz in $(y,z)$ uniformly in $\omega$ and $t$.
% 	Further assume that $\xi^{(1)}\ge\xi^{(2)}$ and, for each value $(t,y,z)$, $L^{(1)}(t,y,z,\omega)\le L^{(2)}(t,y,z,\omega)$, a.s.
% 	Then, $Y^{(1)}\ge Y^{(2)}$, a.s.
% \end{thm}
% \begin{proof}
	
% \end{proof}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Maximum principle}\label{sec:max_principle}
% For the purpose of control theory, we assume that $L$ takes the form $L(t,x,\varrho,\Pi,u_t)$, where $u_t$ is a progressively measurable process that represents a control at time $t$.
% In this case, we denote the solution to the BSDE by $(Y^u,Z^u)$, where  
% \begin{equation}
% \begin{cases}
% Y^u_t=\xi+\int_t^TF(s,X_s,Y^u_s,Z^u_s,u_s){{\ds}}-\int_t^TZ^u_s\dW_s\\
% %dY^u_t=-F(t,X_t, Y^u_t, Z^u_t, U_t){{\dt}}+Z^u_t \dW_t\\
% Y_T=\xi\in\mathcal{F}^{X}_t\\
% {\dX}_t=\mu(t,X_t){{\dt}}+\sigma(t,X_t)\dW_t
% \end{cases}
% \end{equation}
% and $F(s,X^u_s,Y^u_s,Z^u_s,u_s)=L(s,X_s,Y_s,\sigma^{-1}(s,X^u_s,u_s)Z_s,u_s)$.
% Then, a control problem can be written as 
% \begin{equation}
% Y_t=\mathrm{esssup}_{u\in\mathcal{A}_{t,T}}Y_t^u.
% \end{equation}
% Note that the set of admissible controls are defined specific to a particular problem. In addition, if the terminal condition $\xi=g(X_t^u)$, the problem is a Markovian control problem that was studied in previous sections. Otherwise, it is not Markovian, and therefore, the value  cannot be written as a function $V(t,x)$. In fact, value function takes a more general form of 
% \begin{equation}
% Y_t=\mathrm{esssup}_{u\in\mathcal{A}_{t,T}}\mathbb{E}\left[\int_t^TF(s,X^u_s,Y^u_s,Z^u_s,u_s){{\ds}}+\xi\Big|\mathcal{F}^X_t\right].
% \end{equation}
% In the above, the Lagrangian $F$ not only depends on the state of the system $X_s^u$ and control $u_s$, but also depends on the history of the value $Y^u_s$ and the sensitivity of the value with respect to the Brownian noise $Z^u_t$.


% To solve such optimal control problems, we require the following comparison principle.

% \begin{thm}[Maximum principle for BSDEs]
% The process 
% \begin{equation}
% Y_t=\mathrm{esssup}_{u\in\mathcal{A}_{t,T}}\mathbb{E}\left[\int_t^TF(s,X^u_s,Y^u_s,Z^u_s,u_s){{\ds}}+\xi\Big|\mathcal{F}^X_t\right]=\mathrm{esssup}_{u\in\mathcal{A}_{t,T}}Y_t^u
% \end{equation}
% there exists a $Z$ such that $(Y,Z)$
% satisfies the BSDE
% \begin{equation}
% \begin{cases}
% Y_t=\xi+\int_t^TF^*(s,X_s,Y_s,Z_s){{\ds}}-\int_t^TZ_s\dW_s\\
% Y_T=\xi\in\mathcal{F}^{X}_T\\
% {\dX}_t=\mu(t,X_t){{\dt}}+\sigma(t,X_t)\dW_t
% \end{cases}
% \end{equation}
% where
% \begin{equation}
% F^*(s,x,y,z):=\sup_{u\in U}F(t,x,y,z,u),~~~\textrm{and}~~~u^*(t,x,y,z):=\mathrm{argmax}_{u\in U}F(t,x,y,z,u)
% \end{equation}
% \end{thm}
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
% \chapter{Numerical evaluation of stochastic control problems}



%%%%%%%%%%%%%%%%
% \section{DPP based approximation}
% Recall from \eqref{eqn:dpp_discrete}, the value function and the optimal control in discrete stochastic control problem \eqref{prob:discrete_optimal_control} can be evaluated through.
% \begin{equation}\label{eqn:dpp_discrete_numerical}
%     \begin{cases}
%         V(t,x)= \inf_{u}
%     C(t,x,u) + \mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]\\
%     V(T,x)=g(x)\\
%     u^*_t(x)\in A(x):=\textrm{argmax}_{u}
%     C(t,x,u) + \mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]
%     \end{cases}
% \end{equation}
% where
% \begin{equation}
%     X^u_{t+1}=x+\mu(t,x,u)+\sigma(t,x,u)\xi_{t+1}
% \end{equation}
% and $\{\xi_t\}_{t=1}^T$ is a sequence of i.i.d. random variables with mean $0$ and variance $1$. In the above, we need to evaluate (1) conditional expectation $\mathbb{E}[\cdot|X_t=x]$ and (2) the infimum over control $u$. Given $V(t+1,\cdot)$, these calculations can be done separately. However, the separate evaluation creates inefficiency in the calculations. Therefore, we propose the one-shot approximation of $\inf_{u}C(t,x,u) + \mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]$ through the following methods. 

% \subsection{Evaluation of infimum}
% First note that 
% \begin{equation}
% \begin{split}
%     C(t,x,u(x))+&\mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]
%     \\
%     &=\mathbb{E}[C(t,X,u(X))+ V(t+1,X+\mu(t,X,u(X))+\sigma(t,X,u(X))\xi_{t+1})|X=x]
% \end{split}
% \end{equation}
% Denoting the right-hand side above by $\Phi(x,u)$, then we seek a function $\hat{u}^*(x)$ such that 
% \begin{equation}\label{eqn:hatu^*}
%     \hat{u}^*(\cdot)\in A(X):=\textrm{argmin}_{u(\cdot)}\mathbb{E}[\Phi(X,u(X))]
% \end{equation}
% The following Lemma guarantees that $\hat{u}^*$ solves \eqref{eqn:dpp_discrete_numerical}.
% \begin{lem}\label{lem:expectation_minimization}
%     Assume that $\hat{u}^*\in A(X)$ defined above. Then, $\hat{u}^*(x)\in A(x)$ for all $x$ in the set of values of $X$.
% \end{lem}
% \begin{proof}
%      For a complete proof, we need a measurable selection theorem and some other conditions. For simplicity, we only provide the sketch of the proof. Assume that $u^*(x)\not\in A(x)$ with a positive probability on the set of values of $X$. We denote the set by $B$. Then, $x\in B$, we define $\tilde{u}^*(x)$ such that
%      \[
%      \tilde{u}^*(x)\in\textrm{argmax}_{u}
%     C(t,x,u) + \mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]
%      \]
%      and $\tilde{u}^*(x)=\hat{u}^*(x)$. 
%      Therefore, 
%      \[
%      \mathbb{E}[\Phi(X,\tilde{u}^*(X))]<\mathbb{E}[\Phi(X,\hat{u}^*(X))]
%      \]
%      which contradicts the definition of $\hat{u}^*$.
% \end{proof}

% From the nonparametric point of view, we can approximate $\hat{u}^*(\cdot)$ in \eqref{eqn:hatu^*} by a nonparametric model $u(\cdot;\theta)$ via the following optimization
% \begin{equation}
%     \theta^*\in \textrm{argmin}_{\theta}\mathbb{E}[\Phi(X,u(X;\theta))]
% \end{equation}
% For instance, $u(\cdot;\theta)$ can be a neural network with parameter $\theta$.

% \section{Multi-step evaluation}
% While \eqref{eqn:dpp_discrete_numerical} is a classic way to evaluate optimal control problems, it is not very efficient. There are two reasons for the lack of efficiency. First, we have to run a loop over the number of time steps. Second, after evaluation of each nonparametric, we need to evaluate the value function. The total number of parameters is the number of parameters in each step times the number of steps, which for nonparametric models is massive and  potentially needs a lot of memory. Therefore, we propose the following optimization in place of \eqref{eqn:dpp_discrete_numerical}.
% \begin{equation}
%      \inf_{u(\cdot,\cdot)} \mathbb{E}\bigg[\sum_{t=0}^{T-1}C(t,X^u_t,u(t,X^u_t))+g(X^u_T)\bigg]
% \end{equation}
% where
% \begin{equation}
% \begin{cases}
%         X^u_{t+1} =X^u_{t}+\mu(t,X^u_{t},u(t,X^u_{t}))+\sigma(t,X^u_{t},u(t,X^u_{t}))\xi_{t+1}\\
%         X_0\textrm{ is a random variable.}
% \end{cases}
% \end{equation}
% The justification for the above problem is the same as Lemma~\ref{lem:expectation_minimization} for one-step DPP method. 
% Note that we can use a nonparametric model, $u(t,x;\theta)$,  to approximate the minimization problem by
% \begin{equation}
% \theta^*\in \textrm{argmin}_{\theta}\mathbb{E}\bigg[\sum_{t=0}^{T-1}C(t,X^u_t,u(t,X^u_t;\theta))+g(X^u_T)\bigg]
% \end{equation}
% with 
% \begin{equation}
% \begin{cases}
%         X^u_{t+1} =X^u_{t}+\mu(t,X^u_{t},u(t,X^u_{t};\theta))+\sigma(t,X^u_{t},u(t,X^u_{t};\theta))\xi_{t+1}\\
%         X_0\textrm{ is a random variable.}
% \end{cases}
% \end{equation}
% When an approximate optimal strategy, $u(t,x;\theta^*)$, is found, then one can find the value function through evaluating 
% \begin{equation}
%     \mathbb{E}_{t,x}\bigg[\sum_{s=t}^{T-1}C(s,X^u_s,u(t,X^u_s;\theta^*))+g(X^u_T)\bigg]
% \end{equation}
% with
% \begin{equation}
% \begin{cases}
%         X^u_{s+1} =X^u_{s}+\mu(s,X^u_{s},u(s,X^u_{s};\theta^*))+\sigma(s,X^u_{s},u(s,X^u_{s};\theta^*))\xi_{s+1}\\
%         X_t=x
% \end{cases}
% \end{equation}
% Of course, the evaluation of the conditional expectation above is a different problem.



% {\color{red}
% After finding or approximating }$\hat\phi(t,x,u)\approx$, we need to evaluate 
% \begin{equation}
%     u^*(x):=\inf_{u(x)} C(t,x,u(x)) + \hat\phi(t,x,u(x))
% \end{equation}
% To find such a function, one can solve the following risk minimization:
% \begin{equation}
%     \inf_{u(t,x)}\mathbb{E}[C(t,X,u(X)) + \hat\phi(t,X,u(X))]
% \end{equation}
% or empirically:
% \begin{equation}
%     \hat{u}(t,x)\in\argmin_{u(x;\theta)}J^{-1}\sum_{j\in[J]}C(t,X^j,u(X^j;\theta)) + \hat\phi(t,X^j,u(X^j;\theta))]
% \end{equation}
% Then, 
% \begin{equation}
%     \hat{V}(t,x)= 
%     C(t,x,\hat{u}(t,x)) + \hat\phi(t,x,\hat{u}(t,x)]
% \end{equation}
% One-step dynamic programming principle is laid out in Figure~\ref{fig:one-step}
% \begin{figure}
%     \centering
%     \includegraphics{Figs/one_step_DPP_chart.pdf}
%     % \includegraphics{Figs/one_step_DPP_chart_2.pdf}
%     \caption{One-step dynamic programming principle}
%     \label{fig:one-step}
% \end{figure}


% \begin{rem}
%     The challenges of evaluation of conditional expectation and evaluation of value function, makes implementation of the one-step DPP impractical. We are better off if we only use DPP for theoretical implications and do numerical implementations in different ways. However, in most methods we require the evaluation of conditional expectation.
% \end{rem} 


% %%%%%%%%
% \subsection{Evaluation of conditional expectation}
% We require to evaluate $\mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]$  before the infimum in \eqref{eqn:dpp_discrete}. We first write $\phi(t,x,u):=\mathbb{E}[V(t+1,X^u_{t+1})|X_t=x]$, where $\phi(t,X^u_t,u)=\mathbb{E}[V(t+1,X^u_{t+1})|X_t]$. By the definition of conditional expectation,
% \begin{equation}
%     \phi(t,X_t,u)=\argmin_{\psi(x)}\mathbb{E}[(V(t+1,X^u_{t+1})-\psi(X_t))^2].
% \end{equation}
% Choosing a parametric or non-parametric model for $\psi$ leads to approximation:
% \begin{equation}
%     \hat\phi(t,X_t,u)=\argmin_{\theta}\mathbb{E}[(V(t+1,X^u_{t+1})-\psi(X_t;\theta))^2]
% \end{equation}
% Note that we can choose to generate samples for $X_t$ according to an arbitrary distribution and use \eqref{eqn:discrete_state} to simulate joint samples of $(X_t,X^u_{t+1})$. Then, the above approximation of conditional expectation will be performed empirically by 
% \begin{equation}
%     \hat\phi(t,X_t,u) \approx\argmin_{\theta}J^{-1}\sum_{j\in[J]}[(V(t+1,X^{u,j}_{t+1})-\psi(X^{j}_t;\theta))^2
% \end{equation}

% \section{Numerical methods based on BSDEs}
% Recall from Section~\ref{sec:BSDE} that the solution $V(t,x)$ to the semilinear equation 
% \begin{equation}
% \begin{cases}
% 0=\partial_{t}v(t,x)+[\mu\cdot{\nabla}v](t,x)+\frac12[{\sigma^\intercal\sigma\cdot}D^2V](t,x)+C(t,x,v(t,x),\nabla v(t,x))\\
% V(T,x)=g(x).
% \end{cases}		
% \end{equation}
% is related to the BSDE
% \begin{equation}
% \begin{cases}
% \dd Y_t=- C(t,X_t,Y_t,\sigma^{-1}(t,X_t)Z_t){{\dt}}+Z_t\dW_t\\
% Y_T=g(X_T)
% \end{cases}
% \end{equation}
% by 
% \begin{equation}
%     Y_t=V(t,X_t) \textrm{ and } Z_t=[\sigma\nabla V](t,X_t)
% \end{equation}
% where 
% \begin{equation}\label{eqn:sde_YZ}
% {\dX}_t=\mu(t,X_t){{\dt}}+\sigma(t,X_t)\dW_t.
% \end{equation}
% One way to interpret the solution to the PDE is to find  functions $\mathbf{Y}(t,x)$ and $\mathbf{Z}(t,x)$ such that 
% \begin{equation}\label{eqn:BSDE_YZ}
% \mathbf{Y}(t,X_t)=g(X_T) -\int_t^T \Big(C(s,X_s,\mathbf{Y}(s,X_s),\mathbf{Z}(s,X_s)){{\ds}}+\mathbf{Z}(s,X_s)\dW_s\Big)
% \end{equation}
% Functions $\mathbf{Y}$ and $\mathbf{Z}$ can be approximated by neural networks $\hat{\mathbf{Y}}(t,x,\alpha)$ and $\hat{\mathbf{Z}}(t,x;\beta)$ and the equations \eqref{eqn:sde_YZ} and \eqref{eqn:BSDE_YZ} can be approximated discretely by 
% \begin{equation}
% \begin{cases}
% \hat{\mathbf{Y}}(t_n,\hat{X}_{t_n})
% =\hat{\mathbf{Y}} (t_{n+1},\hat{X}_{t_{n+1}}) 
% - C(t_n,\hat{X}_{t_n},\hat{\mathbf{Y}}(t_n,\hat{X}_{t_n}),\hat{\mathbf{Z}}(t_n,\hat{X}_{t_n})) \Delta t
% +\hat{\mathbf{Z}}(t_n,\hat{X}_{t_n})\Delta B_{t_{n+1}}\\
% \hat{X}_{t_{n+1}}=\hat{X}_{t_n}+\mu(t_n,\hat{X}_{t_n})\Delta t+\sigma(t_n,\hat{X}_{t_n})\Delta B_{t_{n+1}}.
% \end{cases}
% \end{equation}
% Specifically, we shall determine $\hat{\mathbf{Y}}$ and $\hat{\mathbf{Z}}$ such that $g(\hat{X}_T)$ is as close to $\hat{\mathbf{Y}}(T,\hat{X}_T)$ as possible. In other words,  we can find the approximate solution $\hat{\mathbf{Y}}(t,x,\alpha^*)$ and $\hat{\mathbf{Z}}(t,x;\beta^*)$ with 
% \begin{equation}
%     (\alpha^*,\beta^*)\in\mathop\textrm{argmin}\limits_{\alpha,\beta}\mathbb{E}\Big[\big(\hat{\mathbf{Y}}(T,\hat{X}_T)-g(\hat{X}_T)\big)^2\Big]
% \end{equation}

\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results from optimization}
In this sections, we quickly review two of the most influential results from optimization, namely the Lagrange multiplier and gradient descent, which later be used in the context of control problems. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lagrange multiplier}
If we add a constraint to a simple minimization problem such as $\min_x f(x)$, the Lagrange multiplier method is the way to proceed. In a nutshell, the Lagrange multiplier method turns a constrained optimization problem into a saddle point problem without constraints by adding more variables. 

Consider  the constrained problem below:
\begin{equation}        
\label{constraint_optimization1}
    \inf_x f(x) ~~~ \textrm{subject to}~~~g(x)=0
\end{equation}
Define the Lagrangian:
\begin{equation}\label{lagrangian}
    L(x,\lambda):= f(x) - \lambda \cdot g(x)
\end{equation}
Then, under proper conditions, the following saddle point problem yields the solution to \eqref{constraint_optimization1}.
\begin{equation}
\sup_{\lambda}\inf_{x}L(x,\lambda)
\end{equation}
The function $H(\lambda)=\inf_{x}L(x,\lambda)$ is called Hamiltonian and the Lagrange multiplier $\lambda$ is called the dual variable. The unconstrained problem
\begin{equation}\label{prob:dual}
    \sup_{\lambda}H(\lambda)
\end{equation}
is called the \emph{dual problem} for the \emph{primal problem} \eqref{constraint_optimization1}.

The key to the success of the Lagrange multiplier method is the \emph{strong duality}.
\begin{equation}
    \sup_{\lambda}\inf_{x} L(x,\lambda)=\inf_{x}\sup_{\lambda}L(x,\lambda)
\end{equation}
With strong duality, if for some $x$, $g(x)\neq(0)$, then 
\begin{equation}
    \sup_{\lambda}L(x,\lambda) = \sup_{\lambda}\{f(x) - \lambda \cdot g(x)\} = \infty
\end{equation}
Therefore, $\inf_{x}\sup_{\lambda}L(x,\lambda)$ is restricted to $x$ with $g(x)=0$ and 
\begin{equation}
    \inf_{x:g(x)=0} \sup_{\lambda}L(x,\lambda) = \inf_{x:g(x)=0} f(x)
\end{equation}

It is not always easy to check if strong duality holds. 
However, \emph{Karush-Kuhn-Tucker} conditions (KKT) provide some necessary and sufficient conditions for strong duality.
\begin{thm}
Assume the differentiability of $f$ and $g$.
    If $x^*$ solves \eqref{constraint_optimization1}  and $\lambda^*$ solves \eqref{prob:dual} such that 
    \begin{equation} \label{cond:KKT}
    \begin{cases} \triangledown f(x^*) - \lambda^* \cdot \triangledown g(x^*) = 0\\
            g (x^*)=0
        \end{cases}
    \end{equation}
    then, strong duality holds and $(x^*,\lambda^*)$ is a saddle point for $\sup_{\lambda}\inf_{x}L(x,\lambda)$. 

    Conversely, if strong duality holds, then any saddle point $(x^*,\lambda^*)$ satisfies \eqref{cond:KKT}. 
    In particular, $x^*$ solve \eqref{constraint_optimization1}.
\end{thm}
A geometric interpretation of $\lambda^*$ in KKT conditions is explained in See Figure~\ref{fig:KKT_1}.
\begin{figure}
    \centering
\includesvg[scale=0.39]{LagrangeMultipliers2D.svg}
    \caption{KKT conditions for $g(x)=0$. Source \char57391\sc{ikipedi}A}
    \label{fig:KKT_1}
\end{figure}
             




When the constraint is given by some inequalities and qualities, i.e.,
\begin{equation}
    \label{constraint_optimization2}
    \inf_x f(x) ~~~ \textrm{subject to}~~~g(x)=0,~~~h(x)\ge 0
\end{equation}
the range of dual variable in Hamiltonian changes:
\begin{equation}\label{lagrangian2}
    L(x,\lambda,\mu):= f(x) - \lambda \cdot g(x) - \mu \cdot h(x)
\end{equation}
\begin{equation}
    \sup_{{\color{blue}\mu\ge0}}\sup_{\lambda}\inf_{x}L(x,\lambda,\mu)
\end{equation}
The reason for such modification can formally be seen after switching $\inf_x$ and $\sup_{\mu\ge0}$ in the above
\begin{equation}
    \sup_{\mu\ge0}\inf_{x}L(x,\lambda,\mu) =\inf_{x}\sup_{\mu\ge0}L(x,\lambda,\mu)
\end{equation}
If $x_1$ is such that $h(x_1)<0$, then 
$$\sup_{\mu\ge0}L(x_1,\lambda,\mu)= f(x_1) - \lambda g(x_1) - h(x_1) \sup_{\mu\ge0} \mu =\infty$$
Therefore, $\inf_{x}\sup_{\mu\ge0}L(x,\lambda,\mu)$ is not attained at $x_1$. Thus, any saddle point $(x^*,\lambda^*,\mu^*)$ with $\mu^*>0$ must satisfy $h(x^*)\ge0$. More general KKT conditions guarantee the strong duality in this case:
    \begin{equation} \label{cond:KKT_general}
    \begin{cases} \triangledown f (x^*) - \lambda^* \triangledown g (x^*) -\mu^* \triangledown 
 h(x^*)= 0\\
            g(x^*)=0\\
            h(x^*)\ge0\\
            \mu^*\ge0\\
            \mu^*\cdot h(x^*)=0
        \end{cases}
    \end{equation}
The last equality emphasizes that either $h(x^*)>0$ holds, in which case $\mu^*=0$, or $h^*(x^*)=0$, in which case $\mu^*$ is irrelevant. See Figure~\ref{fig:KKT_2}.
\begin{figure}
    \centering
\includesvg[scale=2]{Inequality_constraint_diagram.svg}
    \caption{KKT conditions for $h(x)\ge0$. The figure uses $g$ for $h$. Source \char57391\sc{ikipedi}A}
    \label{fig:KKT_2}
\end{figure}
\subsubsection{Lagrange multiplier and constrained dynamic optimization}
Let's first put the Lagrange multiplier in the context of an optimization problem:
    \begin{equation}
        \inf\bigg\{\int_0^T\Big(x_t^2 - \alpha_t x_t\Big) {{\dt}}\bigg\}
    \end{equation}
subject to $x_T=0$. The Lagrangian is
\begin{equation}
    \int_0^T\Big(x_t^2 - \alpha_t x_t\Big) {{\dt}} + \lambda x_T
\end{equation}
and KKT condition suggests that $x^*_t=2\alpha_t$ for $t<T$ and $x^*_T=0$ solves the problem. However, this is not an interesting problem. One can simply argue that changing $x$ at $T$ does not change the value of the integral, and therefore it is not really a constraint. Even if we impose a more restricted constraint such as $x_t\ge0$ on all $t$, the myopic solution is simply $x^*_t=\max\{\alpha_t,0\}/2$.

However, for the constraint $\int_0^T x_t{{\dt}} = 0$, the optimization problem becomes more interesting.
\begin{equation}
        \sup_{\lambda}\inf_{x}\int_0^T\Big(x_t^2 - (\alpha_t  + \lambda )x_t\Big) {{\dt}} 
\end{equation}
KKT condition becomes $x^*_t =   (\alpha_t + \lambda^* )/2$ and $\int_0^Tx^*_t=0$. This implies that $\lambda^* = -\frac{1}{T}\int_0^T\alpha_t{{\dt}}$, and therefore $x^*_t =   (\alpha_t - \frac{1}{T}\int_0^T\alpha_t{{\dt}} )/2$. 

Adding restrictions to a control problem is slightly more subtle due to the dynamics of $x$. We postpone the study of such problems to the future endeavors. However, a simple control problem can be described as a constrained optimization and can be solved via the Lagrange multiplier. We discuss this approach in the next section.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient descent}
Let $f:\mathbb{R}^d\to\mathbb{R}$ be a differentiable function. We know that if $x_*$ is a local minimum of $f$, then $\nabla f(x_*)=0$. Therefore, solving $\nabla f(x)=0$ yields critical points of the function $f$ including the local and global minimums, if there is any. In very high-dimensional problems such as those in deep learning, $f$ is a highly nonlinear function. The set of critical points of $f$ can be very irregular and hard to find via solving $\nabla f(x)=0$, \citet{PRV21}. In such situation, finding a global minimum requires some luck. However, a local minimum can be found via the method of \emph{gradient descent}, GD henceforth. 
GD is based on a simple fact. At any point $x$, direction of maximum descent (ascent) of $f$ is parallel to $-\nabla f(x)$ ($\nabla f(x)$). Therefore, if we move from point $x$ toward $-\nabla f(x)$, the value of $f$ decreases and we are closer to a local minimum than we were at $x$. In other words, for some small $\alpha>0$ $x-\alpha\nabla(x)$ is closer to a local minimum than $x$ was. 
\begin{algorithm}
        % Algorithm content goes here
        \caption{Gradient descent}
        \label{alg:gd}
        \KwData{$x$ randomly chosen from the domain.}
        \Parameter{Learning rate $\{\alpha_n\}_n$, number of iterations $N$, and tolerance $\epsilon>0$.}
        \While{$|\nabla f(x_n)|>\epsilon$ \& $n\le N$}{
        $x \leftarrow x - \alpha_n \nabla f(x)$\;
        $n \leftarrow n + 1$
        }
        \Return{$x$}
        \caption{Gradient descent algorithm}
\end{algorithm}
The most important parameter in GD is $\alpha$, \emph{learning rate}. Ideally, when we are further from a local minimum, we like to move faster, therefore, we choose a larger learning rate. When we are very close to the minimum, we like the learning rate to be smaller. A general rule of thumb suggests $\alpha_n$ should be such that $\alpha_n\to0$ and $\sum_n\alpha_n=\infty$. Decrease of $\alpha_n$ to 0, allows DG to slow down when it gets closer to the minimum. If $\alpha_n$ is constant, it can potentially jump over the minimum. The jutification for $\sum_n\alpha_n=\infty$ can be rigorously explained. The following theorem shows why this condition holds under some strong assumption. The theorem shows the continuous gradient descent to the global minimum of a convex function. In practice, GD is a discrete algorithm. 
\begin{thm}
    Let $\int_0^\infty\alpha_t\dt=\infty$ and $f$ be a convex function with a global minimum at $x_*$ such that for all $x$
    \[
    (x-x_*)\cdot \nabla f(x) \ge \lambda |x-x_*|^2
    \]
    Then, the solution of the ODE given by $\dd x_t = -\alpha_t \nabla f(x_t) \dt$ converges to $x_*$. 
\end{thm}
\begin{proof}
    We use chain rule to evaluate 
    \[
    \dd |x_t-x_*|^2 = 2(x_t-x_*)\cdot \dx_t = -2 \alpha (x_t-x_*)\cdot\nabla f(x_t) \dt
    \]
    By the assumption of the theorem, we have
    \[
    \dd |x_t-x_*|^2 = -2 \alpha_t (x_t-x_*)\cdot\nabla f(x_t) \dt\le -2\lambda\alpha_t |x_t-x_*|^2\dt
    \]
    or
    \[
    \dd |x_t-x_*|^2 + 2\lambda\alpha_t |x_t-x_*|^2\dt \le 0
    \]
    By multiplying the above by $\exp(2\lambda\int_0^t\alpha_s\ds)$ and integrating, we obtain
    \[
    \exp(2\lambda\int_0^t\alpha_s\ds)|x_t-x_*|^2\le |x_0-x_*|^2
    \]
    or
    \[
    |x_t-x_*|^2\le |x_0-x_*|^2\exp(-2\lambda\int_0^t\alpha_s\ds)
    \]
    The right-hand above goes to zero as $t\to\infty$.
\end{proof}

Plain GD is not working for many applications and several versions of GD are introduced overcome the issues arising in applications. For example, if dimension is very large, $d>1e6$, evaluation of gradient of a function is very time consuming. In such cases, \emph{stochastic gradient descent}, SGD henceforth, in used. SGD randomly chooses small number of directions from $\{1,...,d\}$ e.g. $10$ out of $1e^6$ and gradient is only evaluated in those directions. To stabilize SGD, the introduced \emph{momentum} into gradient descent. A popular SGD algorithm is \emph{ADAMS} algorithm. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\begin{comment}
\newpage
\chapter{Outdated material}
%%%%%%%%%%%%%%%%%
\section{Dynamic programming principle (DPP)}
Let $t, h>0$ such that $t+h<T$.
Assume that $V(t+h,x)$ is known for all $x\in\mathbb{R}^d$ and we want to find $V(t,x)$ for all $x\in\mathbb{R}^d$.
Recall from \eqref{eqn:value_function} that 
\begin{equation}
\begin{split}	
V(t,x)&=\sup_{u\in\mathcal{A}_{t,T}}\mathbb{E}\left[\int_{t}^{t+h}C(s,X_s^{t,x,u},u_s){{\dt}}+\int_{t+h}^{T}C(s,X_s^{t,x,u},u_s){{\ds}}+g(X_T^{t,x,u})\right]\\
	&=\sup_{u\in\mathcal{A}_{t,T}}\mathbb{E}\left[\int_{t}^{t+h}C(s,X_s^{t,x,u},u_s){{\ds}}+J(t+h,X^{t,x,u}_{t+h},u)\right]
\end{split}
\end{equation}
Now consider an arbitrary $u\in\mathcal{A}_{t,T}$. It is easy to verify that the restriction of $u$ to $[t,t+h]$ and to $[t+h,T]$ creates members of $\mathcal{A}_{t,t+h}$ and $\mathcal{A}_{t+h,T}$, respectively. $u:[t,t+h]\times\Omega\to U$ is  progressively measurable and satisfies other properties of $\mathcal{A}$.
However, the concatenation of two arbitrary members of $\mathcal{A}_{t,t+h}$ and $\mathcal{A}_{t+h,T}$ does not necessarily creates a member of $\mathcal{A}_{t,T}$. If we define
\begin{equation}
	u_s:=\begin{cases}
		\hat{u}_s&s\in[t,t+h]\\
		\tilde{u}_s&s\in[t+h,T]
	\end{cases}
\end{equation}
where $\hat{u}\in\mathcal{A}_{t,t+h}$ and $\tilde{u}\in\mathcal{A}_{t+h,T}$, then $u$ may not even be progressively measurable. Fortunately, measurable selection theorem provide us with a result to overcome this difficulty. See \cite{ST02} for more rigorous treatment of dynamic programming principle via measurable selection theorem. We formally write
\begin{equation}
\begin{split}	
V(t,x)&=\sup_{\hat{u}\in\mathcal{A}_{t,t+h}}\sup_{\tilde{u}\in\mathcal{A}_{t+h,T}}\mathbb{E}\left[\int_{t}^{t+h}L(s,X_s^{t,x,\hat{u}},\hat{u}_s){{\ds}}+J(t+h,X^{t,x,\hat{u}}_{t+h};\tilde{u})\right]\\
&=\sup_{\hat{u}\in\mathcal{A}_{t,t+h}}\mathbb{E}\left[\int_{t}^{t+h}L(s,X_s^{t,x,\hat{u}},\hat{u}_s){{\ds}}+\sup_{\tilde{u}\in\mathcal{A}_{t+h,T}}J(t+h,X^{t,x,\hat{u}}_{t+h};\tilde{u})\right]\\
&=\sup_{\hat{u}\in\mathcal{A}_{t,t+h}}\mathbb{E}\left[\int_{t}^{t+h}L(s,X_s^{t,x,\hat{u}},\hat{u}_s){{\ds}}+V(t+h,X^{t,x,\hat{u}}_{t+h})\right]
\end{split}
\end{equation}
Note that in the above, the second supremum can be move inside the expectation, because the $L$-term is independent of the choice of $\hat{u}$ and the $J$-term can be maximized over $\tilde{u}$. This is analogous to $\sup_u\int_0^1f(x,u)dx=\int_0^1 \sup_uf(x,u)dx=\int_0^1 f(x,u^*(x))dx$, where $u^*(x)$ satisfies $\partial_xf(x,u^*(x))=0$. 
The DPP can be expended to stopping times. Let $\tau$ be a stopping time with values in $[t,T]$. Then we have 
\begin{equation}\label{eqn:DPP_cnt}
	V(t,x)=\sup_{{u}\in\mathcal{A}_{t,\tau}}\mathbb{E}\left[\int_{t}^{\tau}L(s,X_s^{t,x,{u}},{u}_s){{\ds}}+V(\tau,X^{t,x,{u}}_{\tau})\right]
\end{equation}
The intuition behind DPP is that if the value function at a future stopping time $\tau$ (or $t+h$) is known, one can solve a control problem from $t$ to $\tau$ (or $t+h$) to find $V(t,x)$.


\begin{eg}
We can write a DPP	for the optimal consumption problem in \eqref{eg:admissibility_paradox}. Note that the discount factor in the DPP can create two forms for of DPP based on how we define the value function. One way to define the value function is to consider \eqref{eqn:value_function}.
\begin{equation}
	V(t,x)=\sup_{u\in\mathcal{A}_{t,T}}\mathbb{E}\left[\int_{t}^{T}e^{-\gamma s}U(c_s){{\ds}}+g(X_T^{t,x,u})\right]
\end{equation}
Then, we obtain 
\begin{equation}
	V(t,x)=\sup_{{u}={(\theta,c)}\in\mathcal{A}_{t,\tau}}\mathbb{E}\left[\int_{t}^{\tau}e^{-\gamma s}U(c_s){{\ds}}+V(\tau,X^{t,x,{u}}_{\tau})\right]
\end{equation}
The second approach in writing the DPP, which is unique to exponential discounting $e^{-\gamma s}$, is to write 
\begin{equation}
	e^{-\gamma t}V(t,x)=\sup_{{u}={(\theta,c)}\in\mathcal{A}_{t,\tau}}\mathbb{E}\left[\int_{t}^{\tau}e^{-\gamma s}U(c_s){{\ds}}+e^{-\gamma \tau}V(\tau,X^{t,x,{u}}_{\tau})\right].
\end{equation}
In other words,
\begin{equation}\label{eqn:DPP_exponential}
	V(t,x)=\sup_{{u}={(\theta,c)}\in\mathcal{A}_{t,\tau}}\mathbb{E}\left[\int_{t}^{\tau}e^{-\gamma (s-t)}U(c_s){{\ds}}+e^{-\gamma (\tau-t)}V(\tau,X^{t,x,{u}}_{\tau})\right].
\end{equation}
The two definitions of value function are equivalent. $V(t,x)$ in the former definition is the same as $e^{-\gamma t}V(t,x)$ in the latter. However, the later leads to a more familiar form of Hamilton-Jacobi-Bellman equation and is now standard form for exponential discounting.
\end{eg}


\begin{ex}
	Control problems can have infinite horizon, provided that we use a proper discounting. For instance, 
	\begin{equation}
		V=\sup_{u\in\mathcal{A}}\mathbb{E}\int_0^\infty e^{-\gamma t}L(t,X_t^u){{\dt}}
	\end{equation}
	where ${\dX}_t^u$ is given by \eqref{eqn:state_process}.
	Here, the Lipschitz continuity of $L$, $\mu$, and $\sigma$ guarantees the finiteness of $V$.
	
	Write a value function for the above problem and derive DPP for that value function.
\end{ex}




%%%%%%%%%%%%%%
\section{Hamilton-Jacobi-Bellman (HJB) equation}
In this section, we assume that the value function $V(t,x)$ defined in \eqref{eqn:value_function} is $\textrm{\normalfont C}^{1,2}$. Thus, we can apply the It\^o formula on $V(\tau,X^{t,x,\hat{u}}_{\tau})$ to write
\begin{equation}
	V(\tau,X^{t,x,\hat{u}}_{\tau})=V(t,x)+\int_{t}^{\tau}\left(\partial_{t}V+\mu\partial_{x}V+\frac{\sigma^2}{2}\partial_{xx}V \right){{\ds}}+\int_{t}^{\tau}\sigma\partial_{x}V\dW_s.
\end{equation}
In the above the partial derivatives of $V$ depend on $(s,X_s^{t,x,u})$ and $\mu$ and $\sigma$ depend on $(s,X_s^{t,x,u},u_s)$.
By plugging in $V(\tau,X^{t,x,\hat{u}}_{\tau})$ from the above in DPP \eqref{eqn:DPP_cnt}, we obtain 
\begin{equation}
	0=\sup_{{u}\in\mathcal{A}_{t,\tau}}\mathbb{E}\left[\int_{t}^{\tau}\left(L+\partial_{t}V+\mu\partial_{x}V+\frac{\sigma^2}{2}\partial_{xx}V \right){{\ds}}+\int_{t}^{\tau}\sigma\partial_{x}V\dW_s\right]
\end{equation}
The stochastic integral $\int_{t}^{\tau}\sigma\partial_{x}V\dW_s$ should vanish under expectation, if $\tau$ satisfies the assumptions of the optional sampling theorem. Generally, one can choose $\tau$  such that $\sigma\partial_{x}V$ remains bounded on $[t,\tau]$ and make the stochastic integral vanish inside expectation. 
\begin{equation}
	0=\sup_{{u}\in\mathcal{A}_{t,\tau}}\mathbb{E}\left[\int_{t}^{\tau}\left(L+\partial_{t}V+\mu\partial_{x}V+\frac{\sigma^2}{2}\partial_{xx}V \right){{\ds}}\right]
\end{equation}
In addition, we can replace $\tau$ by $\tau\wedge h$ for  small $h$.
\begin{equation}
	0=\sup_{{u}\in\mathcal{A}_{t,\tau\wedge h}}\mathbb{E}\left[\int_{t}^{\tau\wedge h}\left(L+\partial_{t}V+\mu\partial_{x}V+\frac{\sigma^2}{2}\partial_{xx}V \right){{\ds}}\right]
\end{equation}
By dividing both sides by $h$ and sending $h\to0$, we obtain
\begin{equation}\label{eqn:HJB1}
\begin{cases}
		0=\partial_{t}V(t,x)+\sup_{u\in \mathbf{U}}\left\{C(t,x,u)+\mu(t,x,u)\partial_{x}V(t,x)+\frac12{\sigma^2(t,x,u)}\partial_{xx}V(t,x) \right\}\\
		V(T,x)=g(x)
\end{cases}
\end{equation}
The terminal condition $V(T,x)=g(x)$ represents the terminal reward.
The above partial differential equation is called Hamilton-Jacobi-Bellman equation (HJB). By solving the above HJB, one expects to obtain the value function $V(t,x)$ and the optimal Markovian control 
\begin{equation}
	u^*(t,x)\in\textrm{argmax}_{u\in\mathbf{U}}\left\{C(t,x,u)+\mu(t,x,u)\partial_{x}V(t,x)+\frac12{\sigma^2(t,x,u)}\partial_{xx}V(t,x)\right\}
\end{equation}
\begin{eg}
Recall from \eqref{eqn:DPP_exponential} that 
\begin{equation}
	V(t,x)=\sup_{{u}={(\theta,c)}\in\mathcal{A}_{t,\tau}}\mathbb{E}\left[\int_{t}^{\tau}e^{-\gamma (s-t)}U(c_s){{\ds}}+e^{-\gamma (\tau-t)}V(\tau,X^{t,x,{u}}_{\tau})\right].
\end{equation}
By  applying It\^o formula on $e^{-\gamma (\tau-t)}V(\tau,X^{t,x,{u}}_{\tau})$, we obtain
\begin{equation}
\begin{split}
	e^{-\gamma (\tau-t)}V(\tau,X^{t,x,\hat{u}}_{\tau})=&V(t,x)\\
	&+\int_{t}^{\tau}e^{-\gamma (s-t)}\Big(\partial_{t}V-\gamma V+(\theta_s(\mu-r)+rX_s-c_s)\partial_{x}V
	+\frac{\sigma^2}{2}\theta^2_s\partial_{xx}V \Big){{\ds}}\\
	&+\sigma\int_{t}^{\tau}e^{-\gamma (s-t)}\theta_s\partial_{x}V\dW_t
	\end{split}
\end{equation}
By plugging the above into DPP programing and using $\tau\wedge h$ for the stopping time as explained in this section, we obtain the following HJB
\begin{equation}
\begin{cases}
	0=\partial_{t}V-\gamma V+rx\partial_{x}V+\sup_{\theta\in\mathbb{R},c\ge0}
	\left\{ U(c)+
	(\theta(\mu-r)-c)\partial_{x}V+
	\frac{\sigma^2}{2}\theta^2\partial_{xx}V
	\right\}\\
	V(T,x)=U(x)
	\end{cases}
\end{equation}
The HJB can be further simplified.
\begin{equation}\label{eqn:HJB_consumption}
\begin{cases}
	0=\partial_{t}V-\gamma V+rx\partial_{x}V+\sup_{c\ge0}
	\left\{ U(c)-c\partial_{x}V\right\}
	+\sup_{\theta\in\mathbb{R}}\left\{\theta(\mu-r)\partial_{x}V+
	\frac{\sigma^2}{2}\theta^2\partial_{xx}V
	\right\}\\
	V(T,x)=U(x)
	\end{cases}
\end{equation}
In the above, we separated the supremum over two controls into two separate supremum, because $c$ and $\theta$ can be controlled sufficiently independent in a short time interval. They do have some dependency when they both show up in the definition of admissible control; specifically,  $X_t^{u}\ge-C$. However, the HJB is the limit of DPP in the short time interval near time $t$.

Solving \eqref{eqn:HJB_consumption} is not easy. In practice, we can only rely on the numerical implementation of HJB equations. However, for some simple choices of the utility function $U$, a closed-form solution exists. For example, if we choose $U(c)=\frac{c^{1-\alpha}}{1-\alpha}$ and we look for a solution of the form $V(t,x)=f(t)\frac{x^{1-\alpha}}{1-\alpha}$, we obtain
\begin{equation}
	V(t,x)=(T-t+e^{A/\alpha t})^{\alpha}e^{-AT}U(x)
\end{equation} 
 with $A=r(1-\alpha)-\gamma+\frac{(\mu-r)(1-\alpha)}{2\sigma^2\alpha}$. See Example~\ref{eg:martingle_consumption}.
\end{eg}


\begin{ex}
Show that the value function $V(t,x)=(T-t+e^{A/\alpha t})^{\alpha}e^{-AT}U(x)$ with $U(x)=\frac{c^{1-\alpha}}{1-\alpha}$ meets the verification theorem criteria for the value function of the HJB \eqref{eqn:HJB_consumption}.
\begin{equation}
\begin{cases}
	0=\partial_{t}V-\gamma V+rx\partial_{x}V+\sup_{c\ge0}
	\left\{ U(c)-c\partial_{x}V\right\}
	+\sup_{\theta\in\mathbb{R}}\left\{\theta(\mu-r)\partial_{x}V+
	\frac{\sigma^2}{2}\theta^2\partial_{xx}V
	\right\}\\
	V(T,x)=U(x)
	\end{cases}
\end{equation}

To make the job easier, we provide some simplifications to the PDE.
The nonlinearities can be explicitly calculated.
\begin{equation}
	\sup_{c\ge0}\left\{ U(c)-c\partial_{x}V\right\}=\begin{cases}
	\frac{(\partial_{x}V)^{-\beta}}{\beta}&~~~y\ge0\\
	\infty&~~y<0
\end{cases},
\end{equation} 
where $\beta=\frac{1-\alpha}{\alpha}$,
and
\begin{equation}
\sup_{\theta\in\mathbb{R}}\left\{\theta(\mu-r)\partial_{x}V+
	\frac{\sigma^2}{2}\theta^2\partial_{xx}V
	\right\}=-\frac{(\mu-r)^2(\partial_{x}V)^2}{2\sigma^2\partial_{xx}V}.
\end{equation} 
In other words, the HJB is 
\begin{equation}
\begin{cases}
	0=\partial_{t}V-\gamma V+rx\partial_{x}V+\frac{1}{\beta}(\partial_{x}V)^{-\beta}
	-\frac{(\mu-r)^2}{2\sigma^2}\frac{(\partial_{x}V)^2}{\partial_{xx}V}\\
	V(T,x)=U(x)
	\end{cases}
\end{equation}
Fully nonlinear PDEs, such as the one above, are not easy to handle. Luckily, we do have a closed-form solution for the above PDE. This helps us to find the candidates for the optimal solution. The martingale property for verification, should be checked by using the following Markov controls and the candidates for the optimal control.
\begin{equation}
c^*(t,x)=\textrm{argmax}_{c\ge0}\left\{ U(c)-c\partial_{x}V\right\}	= (\partial_{x}V(t,x))^{-1/\alpha}.
\end{equation}
and
\begin{equation}
\theta^*(t,x)=\textrm{argmax}_{\theta\in\mathbb{R}}\left\{\theta(\mu-r)\partial_{x}V+
	\frac{\sigma^2}{2}\theta^2\partial_{xx}V
	\right\}	= -\frac{(\mu-r)\partial_{x}V(t,x)}{\sigma^2\partial_{xx}V(t,x)}.
\end{equation}
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Verification of a classical solution for an HJB equation}
There is a gap between the solution of HJB and the value function that satisfies the DPP. We shall show that there are some issues that can make a reasonably looking solution of the HJB far from the value function of the control problem. These issue is overcome in the theory of viscosity solutions for the nonlinear partial differential equation \footnote{Pierre-Louis Lions was granted a Fields medal for the theory of viscosity solution in 1994. The application of this theory is not just limited to optimal control}. We clarify this issue in the following deterministic control example, which is a simplification of \cite[Examples~2.1 and 2.2]{Fleming-Soner-book-06}.
\begin{eg}[Exit-time problems]\label{ex:eikonal}
	We want to minimize 
\begin{equation}
	J(x;u)={\tau}
\end{equation}
where $\tau$ is the time of exit of $(t,X_t^u)$ from $[0,1]\times[-1,1]$, ${\dX}_t^u=u_t{{\dt}}$,  and $u$ takes values in $[-1,1]$.
Before writing the DPP and the HJB for this problem, we need to explain a few differences form the standard optimal control problems that we so far studied.

This type of problems is the optimal control until the exit time. It is defined by making proper adjustments to the standard optimal control problems, such as replacement of the horizon $T$ with the exit time of $\tau$. The value function is defined by
\begin{equation}\label{eqn:value_function_exit_time}
	V(t,x)=\sup_{u\in\mathcal{A}_{t,T}}\mathbb{E}\left[\int_{t}^{\tau}C(s,X_s^{t,x,u},u_s){{\dt}}+g(\tau,X_{\tau}^{t,x,u})\right].
\end{equation}
Note that here $\tau$ is the time of exit of $(t,X_t^u)$ from $[0,T]\times O$, where $O$ is a prespecified open domain, and it also depends on $(t,x,u)$, which we drop for simplicity. We should read it as $\tau^{t,x,u}$. As soon as $X_\tau^{t,x,u}$ hits the boundary of $O$ at a point $\tilde{x}$, the reward/cost of $g(\tau,\tilde{x})$ is incurred. The term $\int_{t}^{\tau}C(s,X_s^{t,x,u},u_s){{\dt}}$ is the running reward/cost until exist.

The HJB for exit-time problems is similar to a standard HJB, \eqref{eqn:HJB}, with an extra boundary condition on the \textit{parabolic} boundary of $(\{T\}\times \bar{O}) \cup ([0,T)\times \partial O)$. 
\begin{equation}\label{eqn:HJB_exit_time}
\begin{cases}
		0=\partial_{t}V(t,x)+\sup_{u\in \mathbf{U}}\left\{C(t,x,u)+\mu(t,x,u)\partial_{x}V(t,x)+\frac12{\sigma^2(t,x,u)}\partial_{xx}V(t,x) \right\}\\
		V(t,x)=g(t,x)~~~~~~\textrm{for}~~t\in[0,T)~\textrm{and}~~x\in\partial O\\
		V(T,x)=g(T,x)~~~~~~\textrm{for}~~x\in \bar{O}
\end{cases}
\end{equation}

We go back to our example with $L\equiv 1$, $g\equiv0$, $O=(-1,1)$, and $T=\infty$. The rule of thumbs suggest that to minimize $\tau$, we should exit $O$ as fast as possible by choosing the closest boundary point between $\{-1,1\}$ and choose the highest speed. In other word, if $x\in(-1,0)$ (resp. $x\in(0,1)$), we choose $u=-1$ (resp. $u=1$) and the the shorted exit time, value function, is $V(x)=1-|x|$\footnote{$V$ does not depend on $t$.}. At point $x=0$, we both choices are optimal. Therefore, the value function is $V(x)=1-|x|$ for $x\in(-1,1)$. 

The DPP for this problem is
\begin{equation}
	V(x)=\inf_{u:[0,h]\to[-1,1]}\left\{h+V\Big(x+\int_0^h u_s{{\ds}}\Big)\right\}.
\end{equation}
By using the chain rule\footnote{Old-fashion name for It\^o formula}, we obtain $V\big(x+\int_0^h u_s{{\ds}}\big)=V(x)+\int_0^h V^{\prime}(X_s^u)u_s{{\ds}}$. Therefore, 
\begin{equation}
	0=\inf_{u:[0,h]\to[-1,1]}\left\{h+\int_0^h V^{\prime}(X_s^u)u_s{{\ds}}\right\},
\end{equation}
which yields the HJB
\begin{equation}\label{eqn:Eikonal_perturbed}
\begin{cases}
		0=1-|V^{\prime}(x)|\\
		V(\pm1)=0
\end{cases}
\end{equation}
The value function $V(x)=1-|x|$ is not differentiable at $x=0$, but in the other points, satisfies the HJB \eqref{eqn:Eikonal_perturbed}. However, there are other solutions to \eqref{eqn:Eikonal_perturbed} which are differentiable except in finite number of points, but they are not obviously solutions to the control problem. For example,
\begin{equation}\label{eqn:nonsolution}
	v(x)=\begin{cases}
		1-|x|&~~\frac12\le |x|\le 1\\
		|x|&~~0\le |x|\le \frac12
	\end{cases}
\end{equation}
This example suggests that we need something more than the verification step in Theorem~\ref{thm:martingale_verification} and Remark~\ref{rem:inf_submartingale} of Section~\ref{sec:martingale}, when we obtain a reasonable solution to the HJB. For this deterministic example,  the martingale approach in Theorem~\ref{thm:martingale_verification} and Remark~\ref{rem:inf_submartingale} of Section~\ref{sec:martingale} for value function $V=1-|x|$ yields that
 $Y_t^u=V(t,X_t^u)+\int_0^t C(s,X_s^u,u_s){{\ds}}$ should be nondecreasing in $t$\footnote{The submartingale (martingale) property for the deterministic case reduces to nondecreasing (constant) in $t$.}. More precisely
\begin{equation}
	Y_t^u=V(X_t^u)+t=1-\Big|x+\int_0^tu_s{{\ds}}\Big|+t,
\end{equation}
which is nondecreasing for $|u_t|\le 1$. For, $u^*_t=\mathrm{sgn}(x)$\footnote{For $x=0$, choose arbitrarily between $\pm1$.}, we obtain $Y_t^{u^*}=1-|x|$, which is is constant in $t$. 
However, for the function $\tilde{V}$, although $\tilde{V}(X_t^u)+t$ is also nondecreasing, we cannot find the optimal control that makes $\tilde{V}(X_t^u)+t$ constant in $t$. In a more complicated example, when an optimal control is not priorly known, we won't be able to use verification in Theorem~\ref{thm:martingale_verification} to tell apart $\tilde{V}$ and $V$. 
\end{eg}

The above example outlines a potential issue when we cannot simply verify that a candidate value function is indeed a value function. The verification Theorem~\ref{thm:martingale_verification} can be used via HJB equation in a more detailed form. 
% \begin{thm}[Verification]\label{thm:verification_old}
% Assume that the HJB \eqref{eqn:HJB} has a classical solution
% $\tilde{V}(t,x)\in\mathrm{C}^{1,2}$. Then, $\tilde{V}(t,x)\le V(t,x)$, where $V(t,x)$ is the value function of the control problem \ref{eqn:value_function}. 

% In addition, if there exists a Markovian control $u^*(t,X^*_t)\in\mathcal{A}$ such that $Y^{u^*}$ in Theorem~\ref{thm:martingale_verification} is a martingale, then $\tilde{V}(t,x)= V(t,x)$ and $u^*(t,X^*_t)$ is an optimal control.
% \end{thm}

\begin{rem}\label{rem:weaker_verification}
In the proof of Theorem~\ref{thm:verification}, we shall see that the $\textrm{\normalfont C}^{1,2}$ can be replaced with a weaker condition as long as we can write It\^o formula. For instance, it suffice that $\tilde{V}$ is $\textrm{\normalfont C}^{1,1}$ at all points and $\textrm{\normalfont C}^{1,2}$ except in finite number of points where the left and right second derivatives exist.

For the deterministic problems, the classical solution for the HJB is $\textrm{\normalfont C}^{1,1}$. However, in the verification theorem, we can use a significantly weaker condition. Namely, we need to have weak derivatives in any sense that the following holds
\begin{equation}
\tilde{V}(T,X_T^{u})=\tilde{V}(t,x)+\int_t^T\big(\partial_{t}\tilde{V}(s,X_s^{u})+\mu(s,X_s^{u},u_s)\partial_{x}\tilde{V}(s,X_s^{u})\big){{\ds}}
\end{equation} 
\end{rem}
\begin{eg}
In Example~\ref{ex:eikonal}, the function $V(x)=1-|x|$ satisfies the weaker condition for Theorem~{thm:verification} that is specified in Remark~\ref{rem:weaker_verification}. In addition, it also satisfies the second part of the verification theorem about the exsistence of an optimal control. However, function $\tilde{V}$ defined in \eqref{eqn:nonsolution} only satisfies the first part of the verification theorem and we can only conclude that $\tilde{V}\le V$. In fact, for $x\in(-\frac12,\frac12)$, $\tilde{V}(x)<V(x)$. There is a step that we should not miss: the control $u^*=\mathrm{sgn}(X_t^{u^*})$ is an admissible control. For that, we shall show that ${\dX}_t=\mathrm{sgn}(X_t){{\dt}}$ has a solution, which it obviously does.
\end{eg}

\begin{ex}
In Section~\ref{sec:SLQR}, show that \eqref{eqn:optimal_control_SLQR} is an admissible control and therefore, the verification shows that the solution of the HJB $\frac12x\cdot P_t x+ Q_t$ is indeed the value function.
\end{ex}

%If the problem is a deterministic problem, the regularity requirement reduces to $\tilde{V}(t,x)\in\mathrm{C}^{1,1}$. 




\ssection{A standard example:  stochastic linear-quadratic regulator}\label{sec:SLQR}
The linear-quadratic regulator is a benchmark problem that can be used to test methods of solution. In our case, we follow \cite[Theorem III.8.1]{Fleming-Soner-book-06}. 
In this standard problem, $\mathbf{U}=\mathbb{R}^m$ and the state process an $\mathbb{R}^d$-values process that satisfies
\begin{equation}
	{\dX}^u_t=(A_tX_t^u+B_tu_t){{\dt}}+\sigma_t dW_t.
\end{equation}
$A_t$ and $B_t$ takes values in the set of $d$-by-$d$ and $d$-by-$m$ matrices, respectively. We switch the notation of the Brownian motion to $W_t$ not to be mistaken by the matrix $B_t$. The goal is to minimize
\begin{equation}
	J(t,x;u)=\mathbb{E}\left[\frac12\int_t^T \left(X_s^u\cdot M_s X_s^u+u_s\cdot N_s u_s\right){{\ds}}+x_T^u\cdot D X_T^u\right],
\end{equation}
where $M_t$, $N_t$, and $D$ are nonnegative definite matrices.
The HJB for this problem is given by
\begin{equation}
\begin{cases}
	0=\partial_{t}V+\frac12x\cdot M_tx+\nabla V^{\top} \cdot A_tx+\frac12\textrm{Tr}[{\sigma^{\top}}{D^2V}{\sigma}]+\sup_{u\in\mathbb{R}^m}
	\left\{\frac12u\cdot N_tu+\nabla V\cdot B_tu
	\right\}\\
	V(T,x)=x\cdot Dx
	\end{cases}
\end{equation}
The nonlinear term in the above HJB can be explicitly evaluated. To avoid unnecessary complication, we assume that $M_t$ is positive definite and therefore invertible\footnote{We can work with a pseudoinverse if it is not positive definite.}.
\begin{equation}
	\min_{u\in\mathbb{R}^m}
	\left\{\frac12u\cdot N_tu+ \nabla V \cdot B_tu
	\right\}=-\frac12\nabla V^{\top}B_t N_t^{-1}B_t^{\top}\nabla V
\end{equation}
We anticipate to guess that the solution of the problem is of the quadratic form $V(t,x)=\frac12x\cdot P_t x+ Q_t$. After plugging $V$ into the HJB, we obtain
\begin{equation}
\begin{cases}	
	0=\frac12x\cdot \big(P_t^{\prime}+M_t+A_t^{\top} P_t-P_t^{\top} K_tP_t\big)x+ Q_t^{\prime}+\frac12\textrm{Tr}[{\sigma^{\top}} P_t{\sigma}]\\
	V(T,x)=x\cdot Dx
\end{cases}
\end{equation}
Here $K_t=B_tN_t^{-1}B_t^{\top}$ and we used
$\nabla V(t,x)=P_t x$ and $D^2V(t,x)=P_t$.
The above leads to the following system of ODEs
\begin{align}
		&0=P_t^{\prime}+M_t+A_t^{\top} P_t-P_t^{\top} K_tP_t\label{eqn:ODE_LQR1}\\
	&0=Q_t^{\prime}+\frac12\textrm{Tr}[{\sigma^{\top}} P_t{\sigma}]\cdot P_t\label{eqn:ODE_LQR2}\\
	&P_T=D\label{eqn:Terminal_LQR1}\\
	&Q_T=0\label{eqn:Terminal_LQR2}
\end{align}
The ODE~\eqref{eqn:ODE_LQR1} is a Riccati equation. By the change of variable $K_tP_t=R_t^{\prime}R_t^{-1}$, the Ricatti equation turns into a nonhomogeneous linear second order equation on $R_t$. The terminal condition \eqref{eqn:Terminal_LQR1} helps us to find $P_t$. Then, 
$Q_t=\frac12\sigma^{\top}\sigma\cdot\int_t^TP_s{{\ds}}$. The optimal control is given by
\begin{equation}\label{eqn:optimal_control_SLQR}
	u^*(t,x)=\textrm{argmin}_{u\in\mathbb{R}^m}
	\left\{\frac12u\cdot N_tu+B_tu\cdot \nabla V
	\right\}=- N_t^{-1}B_t^{\top}P_tx
\end{equation}
As you can see, the solution to \eqref{eqn:ODE_LQR1}-\eqref{eqn:Terminal_LQR2} becomes significantly simpler if $M$, $N$, $A$, and $B$ are constant matrices. 


















\end{comment}










%BIB
\newpage
\bibliographystyle{plainnat}
\bibliography{optimal_control}
\end{document}





















